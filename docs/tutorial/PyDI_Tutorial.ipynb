{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDI Data Integration Tutorial\n",
    "\n",
    "This tutorial demonstrates comprehensive data integration using PyDI. We'll work with movie datasets to showcase the data integration pipeline from entity matching to Data Fusion.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Data Loading & Profiling**: Load and analyze movie datasets with provenance tracking\n",
    "2. **Entity Matching**: \n",
    "   - Blocking strategies (Standard, Sorted Neighbourhood, Token-based, Embedding-based)\n",
    "   - Multi-attribute similarity matching with custom comparators\n",
    "   - Machine learning-based entity matching\n",
    "3. **Data Fusion**: \n",
    "   - Conflict resolution with custom fusion rules\n",
    "   - Quality assessment against test set\n",
    "   - Provenance-based conflict resolution\n",
    "\n",
    "### Datasets\n",
    "\n",
    "We'll use three movie datasets:\n",
    "- **Academy Awards**: Movies with Oscar information (4,592 records)\n",
    "- **Actors**: Movies with actor details (149 records) \n",
    "- **Golden Globes**: Movies with Golden Globe awards (2,286 records)\n",
    "\n",
    "These datasets contain overlapping movie information but with different attributes, data quality issues, and conflicting values - perfect for demonstrating real-world data integration challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "def get_repo_root():\n",
    "    \"\"\"Get repository root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'pyproject.toml').exists():\n",
    "            return current\n",
    "        current = current.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = get_repo_root()\n",
    "OUTPUT_DIR = ROOT / \"PyDI\" / \"tutorial\" / \"output\" / \"movies\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PyDI Tutorial\")\n",
    "print(f\"Repository root: {ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"All systems ready! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Profiling\n",
    "\n",
    "PyDI provides provenance-aware data loading that automatically tracks dataset metadata and optionally adds unique identifiers to each record. Let's load our movie datasets and understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.io import load_xml\n",
    "\n",
    "# Define dataset paths\n",
    "DATA_DIR = ROOT / \"docs\" / \"tutorial\" / \"input\" / \"movies\"\n",
    "\n",
    "# Load Academy Awards dataset\n",
    "academy_awards = load_xml(\n",
    "    DATA_DIR / \"data\" / \"academy_awards.xml\",\n",
    "    name=\"academy_awards\",\n",
    "    nested_handling=\"aggregate\"\n",
    ")\n",
    "\n",
    "# Load Actors dataset  \n",
    "actors = load_xml(\n",
    "    DATA_DIR / \"data\" / \"actors.xml\",\n",
    "    name=\"actors\", \n",
    "    nested_handling=\"aggregate\"\n",
    ")\n",
    "\n",
    "# Load Golden Globes dataset\n",
    "golden_globes = load_xml(\n",
    "    DATA_DIR / \"data\" / \"golden_globes.xml\",\n",
    "    name=\"golden_globes\",\n",
    "    nested_handling=\"aggregate\"\n",
    ")\n",
    "\n",
    "# Display basic information\n",
    "datasets = [academy_awards, actors, golden_globes]\n",
    "names = [\"Academy Awards\", \"Actors\", \"Golden Globes\"]\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Records: {len(df):,}\")\n",
    "    print(f\"  Attributes: {len(df.columns)}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Dataset name: {df.attrs.get('dataset_name', 'unknown')}\")\n",
    "    print()\n",
    "\n",
    "total_records = sum(len(df) for df in datasets)\n",
    "print(f\"Total records across all datasets: {total_records:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data structure\n",
    "\n",
    "print(\"\\nüìΩÔ∏è Academy Awards Dataset:\")\n",
    "display(academy_awards.head(3))\n",
    "\n",
    "print(\"\\nüé≠ Actors Dataset:\")\n",
    "display(actors.head(3))\n",
    "\n",
    "print(\"\\nüèÜ Golden Globes Dataset:\")\n",
    "display(golden_globes.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Analysis\n",
    "\n",
    "Let's use PyDI's profiling capabilities to understand our data quality and identify the best attributes for matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Dataset Summary\n",
    "\n",
    "First, let's use the DataProfiler's `summary()` method to get basic statistics for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.profiling import DataProfiler\n",
    "\n",
    "# Initialize the DataProfiler\n",
    "profiler = DataProfiler()\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    profile = profiler.summary(df) # automatically prints some statistics and returns object containing stats\n",
    "\n",
    "display(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Coverage Analysis\n",
    "\n",
    "Next, let's use the `analyze_coverage()` method to understand how attributes overlap across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = profiler.analyze_coverage(\n",
    "    datasets=datasets,\n",
    "    include_samples=True,\n",
    "    sample_count=3  # Show 3 sample values per attribute\n",
    ")\n",
    "\n",
    "print(\"üìä Attribute coverage across datasets:\")\n",
    "display(coverage)\n",
    "\n",
    "# Identify attributes suitable for entity matching\n",
    "print(\"\\nüîó Attributes suitable for entity matching:\")\n",
    "matching_attrs = coverage[coverage['datasets_with_attribute'] >= 2]['attribute'].tolist()\n",
    "print(f\"Attributes available in 2+ datasets: {matching_attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Data Profiling\n",
    "\n",
    "Now let's generate comprehensive HTML profiles for each dataset using the `profile()` method. These reports provide in-depth statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed HTML profiles for each dataset\n",
    "\n",
    "profile_dir = OUTPUT_DIR / \"dataset-profiles\"\n",
    "profile_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "profile_paths = []\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    print(f\"üìä Profiling {name}...\")\n",
    "    \n",
    "    profile_path = profiler.profile(df, str(profile_dir))\n",
    "    profile_paths.append(profile_path)\n",
    "    print(f\"  ‚úÖ Profile saved: {profile_path}\")\n",
    "\n",
    "print(f\"\\nüéØ Generated {len(profile_paths)} detailed HTML reports\")\n",
    "print(f\"üìÅ Location: {profile_dir}\")\n",
    "print(\"\\nüí° Open these HTML files in your browser for interactive exploration:\")\n",
    "for path in profile_paths:\n",
    "    print(f\"  ‚Ä¢ {Path(path).name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Entity Matching\n",
    "\n",
    "Entity Matching is the process of identifying records that refer to the same real-world entity. PyDI implements different blocking and matching methods.\n",
    "\n",
    "### Step 1: Blocking\n",
    "\n",
    "Blocking reduces the number of comparisons from O(n¬≤) to a manageable subset. Let's explore different blocking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup logging first\n",
    "import logging\n",
    "\n",
    "import os\n",
    "os.makedirs('output/logs', exist_ok=True)\n",
    "\n",
    "# choose either default logging or debug logging\n",
    "\n",
    "# # Configure logging for INFO level\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='[%(levelname)-5s] %(name)s - %(message)s',\n",
    "#     handlers=[\n",
    "#           logging.FileHandler('output/logs/pydi.log'),  # Save to file\n",
    "#           logging.StreamHandler()                      # Display on console\n",
    "#       ],\n",
    "#     force=True\n",
    "# )\n",
    "\n",
    "# Configure logging for DEBUG level\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='[%(levelname)-5s] %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "          logging.FileHandler('output/logs/pydi.log'),  # Save to file\n",
    "          logging.StreamHandler()                      # Display on console\n",
    "      ],\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.entitymatching import NoBlocker, StandardBlocker, SortedNeighbourhoodBlocker, TokenBlocker, EmbeddingBlocker\n",
    "\n",
    "# We'll focus on Actors and Golden Globes for showcasing blocking strategies\n",
    "\n",
    "max_pairs = len(actors) * len(golden_globes)\n",
    "print(f\"Without blocking: {max_pairs:,} comparisons required\")\n",
    "print(\"\\nüéØ Goal: Reduce comparisons while maintaining high recall\\n\")\n",
    "\n",
    "# No Blocking - compare all possible pairs\n",
    "print(\"\\n No Blocking\")\n",
    "\n",
    "no_blocker = NoBlocker(\n",
    "    actors, golden_globes,\n",
    "    batch_size=1000,\n",
    "    id_column='id'  # specify the ID column for both datasets\n",
    ")\n",
    "\n",
    "# in an actual large-scale application, we do not build a list of all pairs but stream over them like this\n",
    "for batch in no_blocker:\n",
    "    # do something with the pairs\n",
    "    continue\n",
    "\n",
    "# but we can also generate the full set of pairs for smaller datasets\n",
    "no_candidates = no_blocker.materialize()\n",
    "\n",
    "print(f\"  Generated: {len(no_candidates):,} candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use an actual blocker. Note that when instantiating the blocker, it also writes out a corresponding debug file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard Blocking - First 3 characters of title\n",
    "print(\"\\n1Ô∏è‚É£ Standard Blocking (Concatenation of first 2 characters of each of the first three tokens of title)\")\n",
    "\n",
    "# Add title_prefix directly to the original dataframes\n",
    "actors['title_prefix'] = actors['title'].astype(str).apply(lambda x: ''.join([word[:2].upper() for word in x.split()[:3]]))\n",
    "golden_globes['title_prefix'] = golden_globes['title'].astype(str).apply(lambda x: ''.join([word[:2].upper() for word in x.split()[:3]]))\n",
    "\n",
    "standard_blocker_a2g = StandardBlocker(\n",
    "    actors, golden_globes,\n",
    "    on=['title_prefix'],\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "\n",
    "standard_candidates_a2g = standard_blocker_a2g.materialize()\n",
    "\n",
    "print()\n",
    "print(f\"  Generated: {len(standard_candidates_a2g):,} candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sorted Neighbourhood - Sequential similarity\n",
    "print(\"\\n2Ô∏è‚É£ Sorted Neighbourhood Blocking (Title-based, Window=5)\")\n",
    "\n",
    "sn_blocker_a2g = SortedNeighbourhoodBlocker(\n",
    "    actors, golden_globes,\n",
    "    key='title',  # Sort by title\n",
    "    window=20,     # Compare with 20 neighbors\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "\n",
    "sn_candidates_a2g = sn_blocker_a2g.materialize()\n",
    "\n",
    "print()\n",
    "print(f\"  Generated: {len(sn_candidates_a2g):,} candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Token Blocking - Token-based similarity\n",
    "print(\"\\n3Ô∏è‚É£ Token Blocking (Title Tokens, Min Length=5)\")\n",
    "\n",
    "token_blocker_a2g = TokenBlocker(\n",
    "    actors, golden_globes,\n",
    "    column='title',      # Tokenize titles\n",
    "    min_token_len=3,     # Ignore very short tokens\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "\n",
    "token_candidates_a2g = token_blocker_a2g.materialize()\n",
    "\n",
    "print()\n",
    "print(f\"  Generated: {len(token_candidates_a2g):,} candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Embedding Blocking - Semantic similarity\n",
    "print(\"\\n4Ô∏è‚É£ Embedding Blocking (Semantic Similarity)\")\n",
    "\n",
    "embedding_blocker_a2g = EmbeddingBlocker(\n",
    "    actors, golden_globes,\n",
    "    text_cols=['title'],\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    index_backend=\"sklearn\",\n",
    "    top_k=20,          # Top 20 most similar\n",
    "    batch_size=500,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "    \n",
    "embedding_candidates_a2g = embedding_blocker_a2g.materialize()\n",
    "\n",
    "print()\n",
    "print(f\"  Generated: {len(embedding_candidates_a2g):,} candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Evaluation Against Ground Truth\n",
    "\n",
    "PyDI provides evaluation methods for blocking with pair completeness, pair quality, and reduction ratio:\n",
    "- **`evaluate_blocking()`**: Evaluates blocking given an already materialized set of pairs.\n",
    "- **`evaluate_blocking_batched()`**: Evaluates blocking by iterating over batches and storing results. Useful for very large datasets \n",
    "\n",
    "Let's first evaluate materialized blocking results against a set of provided ground truth correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PyDI.io import load_csv\n",
    "from PyDI.entitymatching import EntityMatchingEvaluator\n",
    "# Showcase EntityMatchingEvaluator.evaluate_blocking utility\n",
    "\n",
    "# Load test set with proper column names\n",
    "test_gt = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"actors_2_golden_globes_test.csv\",\n",
    "    name=\"test_set\", header=None, names=['id1', 'id2', 'label'], add_index=False\n",
    ")\n",
    "\n",
    "# Use EntityMatchingEvaluator.evaluate_blocking on Standard Blocking\n",
    "results = EntityMatchingEvaluator.evaluate_blocking(\n",
    "    candidate_pairs=standard_candidates_a2g,\n",
    "    blocker=standard_blocker_a2g,\n",
    "    test_pairs=test_gt,\n",
    "    out_dir=OUTPUT_DIR / \"blocking-evaluation\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüí° Evaluating pair quality only makes sense if the test set contains all possible pairs, which is not the case in this example!\")\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When datasets are huge, it is necessary to use the evaluate_blocking_batched() function to avoid materializing the full set of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = EntityMatchingEvaluator.evaluate_blocking_batched(\n",
    "    blocker=standard_blocker_a2g,\n",
    "    test_pairs=test_gt,\n",
    "    out_dir=OUTPUT_DIR / \"blocking-evaluation\"\n",
    ")\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same kind of blocking for the dataset combination Academy Awards <-> Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add title_prefix directly to the original dataframes\n",
    "academy_awards['title_prefix'] = academy_awards['title'].astype(str).apply(lambda x: ''.join([word[:2].upper() for word in x.split()[:3]]))\n",
    "\n",
    "standard_blocker_aa2a = StandardBlocker(\n",
    "    academy_awards, actors,\n",
    "    on=['title_prefix'],  # Block on first 3 characters of title\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "standard_candidates_aa2a = standard_blocker_aa2a.materialize()\n",
    "\n",
    "sn_blocker_aa2a = SortedNeighbourhoodBlocker(\n",
    "    academy_awards, actors,\n",
    "    key='title',  # Sort by title\n",
    "    window=20,     # Compare with 20 neighbors\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "sn_candidates_aa2a = sn_blocker_aa2a.materialize()\n",
    "\n",
    "token_blocker_aa2a = TokenBlocker(\n",
    "    academy_awards, actors,\n",
    "    column='title',      # Tokenize titles\n",
    "    min_token_len=3,     # Ignore very short tokens\n",
    "    batch_size=1000,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "token_candidates_aa2a = token_blocker_aa2a.materialize()\n",
    "\n",
    "embedding_blocker_aa2a = EmbeddingBlocker(\n",
    "    academy_awards, actors,\n",
    "    text_cols=['title'],\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    index_backend=\"sklearn\",\n",
    "    top_k=20,          # Top 20 most similar\n",
    "    batch_size=500,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "embedding_candidates_aa2a = embedding_blocker_aa2a.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate which blocking method we want to use for each dataset combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all blocking methods for both dataset combinations\n",
    "\n",
    "evaluator = EntityMatchingEvaluator()\n",
    "\n",
    "# Create dictionaries of candidates for both dataset combinations\n",
    "a2g_blocking_candidates = {\n",
    "    'StandardBlocking': [standard_candidates_a2g, standard_blocker_a2g],\n",
    "    'SortedNeighbourhoodBlocker': [sn_candidates_a2g, sn_blocker_a2g],\n",
    "    'TokenBlocking': [token_candidates_a2g,token_blocker_a2g],\n",
    "    'EmbeddingBlocking': [embedding_candidates_a2g,embedding_blocker_a2g]\n",
    "}\n",
    "\n",
    "aa2a_blocking_candidates = {\n",
    "    'StandardBlocking': [standard_candidates_aa2a,standard_blocker_aa2a],\n",
    "    'SortedNeighbourhood': [sn_candidates_aa2a, sn_blocker_aa2a],\n",
    "    'TokenBlocking': [token_candidates_aa2a,token_blocker_aa2a],\n",
    "    'EmbeddingBlocking': [embedding_candidates_aa2a,embedding_blocker_aa2a]\n",
    "}\n",
    "\n",
    "# Load correspondences for evaluation\n",
    "a2g_correspondences = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"actors_2_golden_globes_test.csv\",\n",
    "    name=\"a2g_test\", header=None, names=['id1', 'id2', 'label'], add_index=False\n",
    ")\n",
    "\n",
    "aa2a_correspondences = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"academy_awards_2_actors_test.csv\",\n",
    "    name=\"aa2a_test\", header=None, names=['id1', 'id2', 'label'], add_index=False\n",
    ")\n",
    "\n",
    "# Evaluate blocking for a2g datasets\n",
    "a2g_results = []\n",
    "for method_name, candidates in a2g_blocking_candidates.items():\n",
    "    result = evaluator.evaluate_blocking(candidates[0], a2g_correspondences,candidates[1], out_dir=OUTPUT_DIR / \"blocking-evaluation\")\n",
    "    result['method'] = method_name\n",
    "    result['dataset'] = 'a2g'\n",
    "    a2g_results.append(result)\n",
    "\n",
    "# Evaluate blocking for aa2a datasets  \n",
    "aa2a_results = []\n",
    "for method_name, candidates in aa2a_blocking_candidates.items():\n",
    "    result = evaluator.evaluate_blocking(candidates[0], aa2a_correspondences,candidates[1], out_dir=OUTPUT_DIR / \"blocking-evaluation\")\n",
    "    result['method'] = method_name\n",
    "    result['dataset'] = 'aa2a'\n",
    "    aa2a_results.append(result)\n",
    "\n",
    "# Select best method for each dataset (highest pair_completeness, then highest reduction_ratio)\n",
    "a2g_best = max(a2g_results, key=lambda x: (x['pair_completeness'], x['reduction_ratio']))\n",
    "aa2a_best = max(aa2a_results, key=lambda x: (x['pair_completeness'], x['reduction_ratio']))\n",
    "\n",
    "print(f\"Best blocking for a2g: {a2g_best['method']} (PC: {a2g_best['pair_completeness']:.3f}, RR: {a2g_best['reduction_ratio']:.3f})\")\n",
    "print(f\"Best blocking for aa2a: {aa2a_best['method']} (PC: {aa2a_best['pair_completeness']:.3f}, RR: {aa2a_best['reduction_ratio']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Entity Matching with Comparators\n",
    "\n",
    "Now we'll use PyDI's linear matching rule capabilities to find duplicate movies using multiple attribute comparisons.\n",
    "\n",
    "First, we define some comparators for attributes relevant to matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.entitymatching import StringComparator, DateComparator, NumericComparator\n",
    "\n",
    "# Create comparators for different attributes\n",
    "comparators = [\n",
    "    # Title similarity - most important for movies\n",
    "    StringComparator(\n",
    "        column='title',\n",
    "        similarity_function='jaccard',  # Good for movie titles\n",
    "        preprocess=str.lower  # Case normalization\n",
    "    ),\n",
    "    \n",
    "    # Date proximity - movies from same year likely same film\n",
    "    DateComparator(\n",
    "        column='date', \n",
    "        max_days_difference=365  # Allow 1 year difference\n",
    "    ),\n",
    "    \n",
    "    # Actor name similarity - supporting evidence\n",
    "    StringComparator(\n",
    "        column='actors_actor_name',\n",
    "        similarity_function='jaccard',  # Good for names\n",
    "        preprocess=str.lower,\n",
    "        list_strategy='concatenate' # Handle list attribute by concatenation\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we setup the matcher and run the matching with our chosen best blocking method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.entitymatching import RuleBasedMatcher\n",
    "\n",
    "# Initialize the blocker\n",
    "embedding_blocker_a2g = EmbeddingBlocker(\n",
    "    actors, golden_globes,\n",
    "    text_cols=['title'],\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    index_backend=\"sklearn\",\n",
    "    top_k=20,          # Top 20 most similar\n",
    "    batch_size=500,\n",
    "    output_dir=OUTPUT_DIR / \"blocking-evaluation\",\n",
    "    id_column='id'\n",
    ")\n",
    "\n",
    "# Initialize Rule-Based Matcher\n",
    "matcher = RuleBasedMatcher()\n",
    "\n",
    "correspondences_a2g = matcher.match(\n",
    "    df_left=actors,\n",
    "    df_right=golden_globes, \n",
    "    candidates=embedding_blocker_a2g, # pass the blocker, which will internally generate candidate pairs using batching\n",
    "    comparators=comparators,\n",
    "    weights=[0.7, 0.2, 0.1],  # Title most important, then date, then actor,\n",
    "    threshold=0.7, # set a similarity threshold for a match\n",
    "    id_column='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation Against Ground Truth\n",
    "\n",
    "We can evaluate the result of our entity matching with this method of the EntityMatchingEvaluator:\n",
    "- **`evaluate_matching()`**: Evaluates matching given a test set and the predicted correspondences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_test = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"actors_2_golden_globes_test.csv\", \n",
    "    name=\"test_entity_matching\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'label'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "debug_output_dir = OUTPUT_DIR / \"debug_results_entity_matching\"\n",
    "debug_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "    correspondences=correspondences_a2g,\n",
    "    test_pairs=gt_test,\n",
    "    out_dir=debug_output_dir\n",
    ")\n",
    "\n",
    "display(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need more detailed debugging results, we can set the debug flag during matching and pass the resulting info object to the evaluate_matching function to write detailed debug logs to a directory of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the matcher with debug mode enabled to get detailed debug data\n",
    "print(\"üîç Re-running matcher with debug mode to capture detailed results:\")\n",
    "\n",
    "correspondences_a2g, debug_info = matcher.match(\n",
    "    df_left=actors,\n",
    "    df_right=golden_globes, \n",
    "    candidates=embedding_blocker_a2g, # pass the blocker, which will internally generate candidate pairs using batching\n",
    "    comparators=comparators,\n",
    "    weights=[0.7, 0.2, 0.1],  # Title most important, then date, then actor,\n",
    "    threshold=0.7, # set a similarity threshold for a match\n",
    "    id_column='id',\n",
    "    debug=True  # This enables debug output capture\n",
    ")\n",
    "\n",
    "eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "    correspondences=correspondences_a2g,\n",
    "    test_pairs=gt_test,\n",
    "    out_dir=debug_output_dir,\n",
    "    debug_info=debug_info, # add debug info\n",
    "    matcher_instance=matcher # add matcher instance for context for debug files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helpful tool for investigating the goodness of the matching is to create the cluster size distribution that shows how many clusters (records referencing same entity) after matching exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing cluster size distribution in our entity matching results...\")\n",
    "\n",
    "# Create cluster size distribution from our matches\n",
    "cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "    correspondences=correspondences_a2g,\n",
    "    out_dir=str(OUTPUT_DIR / \"cluster_analysis\")\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cluster Size Distribution Results:\")\n",
    "display(cluster_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see strange distribution of clusters, we can further investigate specific clusters by writing out detailed cluster information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out detailed cluster information with all entity records for debugging purposes\n",
    "\n",
    "# Use the matches we found earlier to demonstrate cluster details\n",
    "cluster_details_path = OUTPUT_DIR / \"cluster_analysis\" / \"detailed_cluster_info.json\"\n",
    "\n",
    "# Call write_cluster_details with our entity matches\n",
    "output_path = EntityMatchingEvaluator.write_cluster_details(\n",
    "    correspondences=correspondences_a2g,\n",
    "    out_path=cluster_details_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Machine Learning-based Matching Rules\n",
    "\n",
    "Instead of using manually configured matching rules, we can also learn the weights and best comparators using machine learning if we have a labeled training set available.\n",
    "\n",
    "Let's do this for the dataset combination Academy Awards <-> Actors.\n",
    "\n",
    "First, we need to create the features for machine learning using PyDIs FeatureExtractor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.entitymatching import FeatureExtractor\n",
    "\n",
    "# Load ground truth correspondences\n",
    "aa2a_train = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"academy_awards_2_actors_training.csv\",\n",
    "    name=\"ground_truth_train\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'label'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "aa2a_test = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"academy_awards_2_actors_test.csv\",\n",
    "    name=\"ground_truth_test\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'label'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "similarity_comparators = [\n",
    "    # Title similarity features - most important for movie matching\n",
    "    StringComparator(\"title\", similarity_function=\"jaro_winkler\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"levenshtein\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"cosine\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"jaccard\", preprocess=str.lower),\n",
    "    \n",
    "    # Date proximity features\n",
    "    DateComparator(\"date\", max_days_difference=365),  # 1 years tolerance\n",
    "    \n",
    "    # Actor name similarity\n",
    "    StringComparator(\"actors_actor_name\", similarity_function=\"jaccard\", preprocess=str.lower, list_strategy='concatenate'),\n",
    "    StringComparator(\"actors_actor_name\", similarity_function=\"jaccard\", preprocess=str.lower, list_strategy='best_match'),\n",
    "]\n",
    "\n",
    "feature_extractor = FeatureExtractor(similarity_comparators)\n",
    "\n",
    "# Extract features using FeatureExtractor\n",
    "train_features = feature_extractor.create_features(\n",
    "    academy_awards, actors, aa2a_train[['id1', 'id2']], labels=aa2a_train['label'], id_column='id'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training features extracted!\")\n",
    "print(f\"Feature columns: {[col for col in train_features.columns if col not in ['id1', 'id2', 'label']]}\")\n",
    "\n",
    "# Prepare data for ML training\n",
    "feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "\n",
    "X_train = train_features[feature_columns]\n",
    "y_train = train_features['label']\n",
    "\n",
    "print(f\"Training data: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Class distribution: {y_train.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Scikit-learn integration\n",
    "\n",
    "From here on out, the full scikit-learn library can be used with the features extracted from PyDIs feature extractor without any wrapping as everything in PyDI is based on pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV with multiple models and hyperparameters\n",
    "print(f\"\\nüîç Setting up GridSearchCV...\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define models and parameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l2'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.1, 0.2],\n",
    "            'max_depth': [3, 5],\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42, probability=True),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use F1 score as the scoring metric (good for imbalanced data)\n",
    "scorer = make_scorer(f1_score)\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"GridSearch setup: {len(param_grids)} models, F1 scoring, 5-fold CV\")\n",
    "\n",
    "# Train models using GridSearchCV\n",
    "print(f\"\\nüöÄ Training Models with GridSearchCV...\")\n",
    "\n",
    "grid_search_results = {}\n",
    "best_overall_score = -1\n",
    "best_overall_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for model_name, config in param_grids.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "\n",
    "    # Create GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        scoring=scorer,\n",
    "        cv=cv_folds,\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    grid_search_results[model_name] = {\n",
    "        'grid_search': grid_search,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_estimator': grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ {model_name}: Best CV F1 = {grid_search.best_score_:.4f}\")\n",
    "    print(f\"     Best params: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Track overall best model\n",
    "    if grid_search.best_score_ > best_overall_score:\n",
    "        best_overall_score = grid_search.best_score_\n",
    "        best_overall_model = grid_search.best_estimator_\n",
    "        best_model_name = model_name\n",
    "            \n",
    "print(f\"\\nüèÜ Best Overall Model: {best_model_name} (CV F1: {best_overall_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can directly use the trained model with PyDIs MLBasedMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.entitymatching import MLBasedMatcher\n",
    "\n",
    "# Create MLBasedMatcher and apply trained model\n",
    "ml_matcher = MLBasedMatcher(feature_extractor)\n",
    "\n",
    "correspondences_aa2a = ml_matcher.match(\n",
    "    academy_awards, actors, candidates=embedding_blocker_aa2a, id_column='id', trained_classifier=best_overall_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importance if available\n",
    "if hasattr(best_overall_model, 'feature_importances_'):\n",
    "    print(f\"\\nüîç Top Feature Importances:\")\n",
    "    importance_df = ml_matcher.get_feature_importance(best_overall_model, feature_columns)\n",
    "    display(importance_df.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the ML-based matching with the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "    correspondences=correspondences_aa2a,\n",
    "    test_pairs=aa2a_test,\n",
    "    out_dir=debug_output_dir\n",
    ")\n",
    "\n",
    "display(eval_results)\n",
    "\n",
    "# Create cluster size distribution from our matches\n",
    "cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "    correspondences=correspondences_aa2a,\n",
    "    out_dir=OUTPUT_DIR / \"cluster_analysis\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cluster Size Distribution Results:\")\n",
    "display(cluster_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to similarity metrics for each attribute, PyDIs VectorFeatureExtractor can be used to create embeddings using SentenceTransformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorFeatureExtractor Examples\n",
    "\n",
    "from PyDI.entitymatching import VectorFeatureExtractor\n",
    "\n",
    "# SentenceTransformers embeddings using VectorFeatureExtractor\n",
    "st_extractor = VectorFeatureExtractor(\n",
    "    embedding_model='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    columns=['title', 'actors_actor_name', 'date'],\n",
    "    distance_metrics=['cosine'],\n",
    "    pooling_strategy='concatenate',\n",
    "    list_strategies={'actors_actor_name': 'concatenate'}\n",
    ")\n",
    "\n",
    "# Extract features using VectorFeatureExtractor\n",
    "train_features = st_extractor.create_features(\n",
    "    academy_awards, actors, aa2a_train[['id1', 'id2']], labels=aa2a_train['label'], id_column='id'\n",
    ")\n",
    "\n",
    "# ready to train ML models with scikit-learn as before\n",
    "# matching workflow is analogous to previous example with FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "academy_awards[\"academy_awards_id\"] = academy_awards[\"id\"]\n",
    "\n",
    "academy_awards.attrs[\"trust_score\"] = 3\n",
    "actors.attrs[\"trust_score\"] = 2\n",
    "golden_globes.attrs[\"trust_score\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correspondences = pd.concat([correspondences_a2g, correspondences_aa2a], ignore_index=True)\n",
    "print(f'Total correspondences: {len(all_correspondences):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fusion Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.fusion import DataFusionStrategy, longest_string, union, prefer_higher_trust\n",
    "\n",
    "strategy = DataFusionStrategy('movie_fusion_strategy')\n",
    "\n",
    "strategy.add_attribute_fuser('title', longest_string)\n",
    "strategy.add_attribute_fuser('director_name', longest_string)\n",
    "strategy.add_attribute_fuser('date', prefer_higher_trust, trust_key=\"trust_score\")\n",
    "\n",
    "strategy.add_attribute_fuser('actors_actor_name', union)\n",
    "\n",
    "print('Strategy ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fusion\n",
    "We build connected components from the converted correspondences and fuse per attribute using the rules above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.fusion import DataFusionEngine\n",
    "\n",
    "engine = DataFusionEngine(strategy, debug=True, debug_format='json')\n",
    "\n",
    "fused = engine.run(\n",
    "    datasets=[academy_awards, actors, golden_globes],\n",
    "    correspondences=all_correspondences,\n",
    "    id_column=\"id\",\n",
    "    include_singletons=False,\n",
    ")\n",
    "print(f'Fused rows: {len(fused):,}')\n",
    "display(fused.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Gold Standard\n",
    "We load the gold standard and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.fusion import tokenized_match, year_only_match, boolean_match\n",
    "\n",
    "strategy.add_evaluation_function(\"title\", tokenized_match)\n",
    "strategy.add_evaluation_function(\"director_name\", tokenized_match)\n",
    "strategy.add_evaluation_function(\"actors_actor_name\", tokenized_match)\n",
    "strategy.add_evaluation_function(\"date\", year_only_match)\n",
    "strategy.add_evaluation_function(\"oscar\", boolean_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDI.fusion import DataFusionEvaluator\n",
    "\n",
    "fusion_test_set = load_xml(DATA_DIR / 'fusion' / 'test_set.xml', name='fusion_test_set', nested_handling='aggregate')\n",
    "\n",
    "# Keep core evaluation columns if present in fused output\n",
    "eval_cols = ['academy_awards_id','title','director_name','actors_actor_name','date','oscar']\n",
    "fused_eval = fused[eval_cols].copy()\n",
    "\n",
    "# Create evaluator with our fusion strategy\n",
    "evaluator = DataFusionEvaluator(strategy)\n",
    "\n",
    "# Evaluate the fused results against the gold standard\n",
    "print(\"Evaluating fusion results against gold standard...\")\n",
    "evaluation_results = evaluator.evaluate(\n",
    "    fused_df=fused_eval,\n",
    "    fused_id_column='academy_awards_id',\n",
    "    gold_df=fusion_test_set,\n",
    "    gold_id_column='id',\n",
    ")\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"\\nFusion Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in evaluation_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "        \n",
    "print(f\"\\nOverall Accuracy: {evaluation_results.get('overall_accuracy', 0):.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydi-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
