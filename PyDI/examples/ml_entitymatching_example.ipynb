{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-md",
   "metadata": {},
   "source": [
    "# PyDI Machine Learning Entity Matching Example\n",
    "\n",
    "This notebook demonstrates comprehensive machine learning-based entity matching in PyDI using the MLBasedMatcher with scikit-learn integration.\n",
    "\n",
    "**What this shows:**\n",
    "- Load datasets with provenance tracking\n",
    "- **Traditional feature extraction**: Convert entity pairs to similarity features using comparators\n",
    "- **Vector-based features**: Use embeddings and distance metrics for deep feature representation\n",
    "- **ML model training**: Train various scikit-learn classifiers with proper validation\n",
    "- **MLBasedMatcher usage**: Apply trained models to find entity correspondences\n",
    "- **Model evaluation**: Compare performance across different ML approaches\n",
    "- **Feature importance**: Understand which features contribute most to matching decisions\n",
    "- **End-to-end ML pipeline**: Complete workflow from data to deployment\n",
    "\n",
    "Run cells below in order. Adjust paths if running outside the repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyDI imports\n",
    "from PyDI.io import load_xml\n",
    "from PyDI.entitymatching import (\n",
    "    MLBasedMatcher,\n",
    "    FeatureExtractor,\n",
    "    VectorFeatureExtractor,\n",
    "    StringComparator,\n",
    "    NumericComparator,\n",
    "    DateComparator,\n",
    "    EntityMatchingEvaluator,\n",
    "    ensure_record_ids\n",
    ")\n",
    "\n",
    "# ML and data processing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def repo_root():\n",
    "    \"\"\"Return the repository root directory.\"\"\"\n",
    "    # For notebooks in PyDI/examples/, go up 2 levels to reach repo root\n",
    "    if '__file__' in globals():\n",
    "        return Path(__file__).parent.parent.parent\n",
    "    else:\n",
    "        # In Jupyter, find the pyproject.toml to locate repo root\n",
    "        current = Path.cwd()\n",
    "        while current != current.parent:\n",
    "            if (current / 'pyproject.toml').exists():\n",
    "                return current\n",
    "            current = current.parent\n",
    "        return Path.cwd()  # fallback\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Repository root: {repo_root()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-md",
   "metadata": {},
   "source": [
    "## Step 1: Load Datasets and Ground Truth\n",
    "\n",
    "We'll use the movie datasets with ground truth labels for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = repo_root()\n",
    "academy_path = root / \"input\" / \"movies\" / \"entitymatching\" / \"data\" / \"academy_awards.xml\"\n",
    "actors_path = root / \"input\" / \"movies\" / \"entitymatching\" / \"data\" / \"actors.xml\"\n",
    "\n",
    "print(f\"Academy awards data: {academy_path}\")\n",
    "print(f\"Actors data: {actors_path}\")\n",
    "\n",
    "# Load datasets using PyDI's provenance-aware XML loader\n",
    "academy_df = load_xml(academy_path, name=\"academy_awards\")\n",
    "actors_df = load_xml(actors_path, name=\"actors\")\n",
    "\n",
    "print(f\"\\nAcademy Awards shape: {academy_df.shape}\")\n",
    "print(f\"Academy Awards columns: {list(academy_df.columns)}\")\n",
    "\n",
    "print(f\"\\nActors shape: {actors_df.shape}\")\n",
    "print(f\"Actors columns: {list(actors_df.columns)}\")\n",
    "\n",
    "# Ensure record IDs\n",
    "academy_df = ensure_record_ids(academy_df)\n",
    "actors_df = ensure_record_ids(actors_df)\n",
    "\n",
    "print(f\"\\n✓ Datasets loaded with record IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ground-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth correspondences\n",
    "train_path = root / \"input\" / \"movies\" / \"entitymatching\" / \"splits\" / \"gs_academy_awards_2_actors_training.csv\"\n",
    "test_path = root / \"input\" / \"movies\" / \"entitymatching\" / \"splits\" / \"gs_academy_awards_2_actors_test.csv\"\n",
    "\n",
    "def load_correspondences(file_path):\n",
    "    \"\"\"Load correspondence file and convert to PyDI ID format.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load raw correspondences\n",
    "    corr = pd.read_csv(file_path, names=['id1', 'id2', 'label'])\n",
    "    \n",
    "    # Convert boolean labels to numeric\n",
    "    corr['label'] = corr['label'].map({True: 1, 'TRUE': 1, False: 0, 'FALSE': 0})\n",
    "    \n",
    "    # Convert original XML IDs to PyDI format\n",
    "    def convert_id(original_id):\n",
    "        if pd.isna(original_id):\n",
    "            return original_id\n",
    "        \n",
    "        id_str = str(original_id)\n",
    "        if 'academy_awards_' in id_str:\n",
    "            # Extract number and reformat\n",
    "            try:\n",
    "                num = int(id_str.split('_')[-1]) - 1  # Convert to 0-based index\n",
    "                return f\"academy_awards_{num:06d}\"\n",
    "            except:\n",
    "                return id_str\n",
    "        elif 'actors_' in id_str:\n",
    "            # Extract number and reformat\n",
    "            try:\n",
    "                num = int(id_str.split('_')[-1]) - 1  # Convert to 0-based index\n",
    "                return f\"actors_{num:06d}\"\n",
    "            except:\n",
    "                return id_str\n",
    "        \n",
    "        return id_str\n",
    "    \n",
    "    corr['id1'] = corr['id1'].apply(convert_id)\n",
    "    corr['id2'] = corr['id2'].apply(convert_id)\n",
    "    \n",
    "    return corr\n",
    "\n",
    "# Load training and test correspondences\n",
    "train_corr = load_correspondences(train_path)\n",
    "test_corr = load_correspondences(test_path)\n",
    "\n",
    "print(f\"Training correspondences: {len(train_corr)} pairs\")\n",
    "if len(train_corr) > 0:\n",
    "    print(f\"Training label distribution:\")\n",
    "    print(train_corr['label'].value_counts())\n",
    "\n",
    "print(f\"\\nTest correspondences: {len(test_corr)} pairs\")\n",
    "if len(test_corr) > 0:\n",
    "    print(f\"Test label distribution:\")\n",
    "    print(test_corr['label'].value_counts())\n",
    "    \n",
    "print(f\"\\n✓ Ground truth loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-2-md",
   "metadata": {},
   "source": [
    "## Step 2: Traditional Feature Extraction\n",
    "\n",
    "We'll create similarity-based features using various comparators and convert labeled pairs into training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-features",
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive feature extractor with multiple comparators\ntraditional_comparators = [\n    StringComparator(\"title\", similarity_function=\"jaro_winkler\", preprocess=str.lower),\n    StringComparator(\"title\", similarity_function=\"levenshtein\", preprocess=str.lower), \n    StringComparator(\"title\", similarity_function=\"cosine\", preprocess=str.lower),\n    DateComparator(\"date\", max_days_difference=730),  # 2 years tolerance\n    StringComparator(\"actor_name\", similarity_function=\"jaro_winkler\", preprocess=str.lower),\n    # Custom features\n    {\n        \"function\": lambda r1, r2: 1.0 if str(r1.get('title', '')).lower() == str(r2.get('title', '')).lower() else 0.0,\n        \"name\": \"exact_title_match\"\n    },\n    {\n        \"function\": lambda r1, r2: len(set(str(r1.get('title', '')).lower().split()) & set(str(r2.get('title', '')).lower().split())),\n        \"name\": \"common_title_words\"\n    },\n]\n\n# Create feature extractor\ntraditional_extractor = FeatureExtractor(traditional_comparators)\n\nprint(f\"Traditional feature extractor created with {len(traditional_comparators)} features\")\nprint(f\"Feature names: {traditional_extractor.get_feature_names()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-training-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training data\n",
    "if len(train_corr) > 0:\n",
    "    print(f\"Extracting features for {len(train_corr)} training pairs...\")\n",
    "    \n",
    "    # Filter out pairs where records might be missing\n",
    "    valid_pairs = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for idx, row in train_corr.iterrows():\n",
    "        id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "        \n",
    "        # Check if both records exist\n",
    "        if (id1 in academy_df['_id'].values and \n",
    "            id2 in actors_df['_id'].values):\n",
    "            valid_pairs.append({'id1': id1, 'id2': id2})\n",
    "            valid_labels.append(label)\n",
    "    \n",
    "    valid_pairs_df = pd.DataFrame(valid_pairs)\n",
    "    valid_labels = pd.Series(valid_labels)\n",
    "    \n",
    "    print(f\"Valid training pairs: {len(valid_pairs_df)} out of {len(train_corr)}\")\n",
    "    \n",
    "    if len(valid_pairs_df) > 0:\n",
    "        # Extract features\n",
    "        train_features = traditional_extractor.create_features(\n",
    "            academy_df, actors_df, valid_pairs_df, labels=valid_labels\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Training features extracted: {train_features.shape}\")\n",
    "        print(f\"Feature columns: {[col for col in train_features.columns if col not in ['id1', 'id2', 'label']]}\")\n",
    "        \n",
    "        # Show sample features\n",
    "        print(f\"\\nSample training features:\")\n",
    "        display(train_features.head(3))\n",
    "        \n",
    "        # Feature statistics\n",
    "        feature_cols = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "        print(f\"\\nFeature statistics:\")\n",
    "        display(train_features[feature_cols].describe())\n",
    "    else:\n",
    "        print(\"No valid training pairs found - cannot extract features\")\n",
    "        train_features = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No training correspondences available\")\n",
    "    train_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-3-md",
   "metadata": {},
   "source": [
    "## Step 3: Vector-Based Feature Extraction (Optional)\n",
    "\n",
    "If sentence-transformers is available, we'll demonstrate vector-based features using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if sentence-transformers is available\n",
    "vector_features_available = False\n",
    "vector_extractor = None\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(\"✓ sentence-transformers available - will demonstrate vector features\")\n",
    "    \n",
    "    # Create vector feature extractor\n",
    "    vector_extractor = VectorFeatureExtractor(\n",
    "        embedding_model='all-MiniLM-L6-v2',  # Lightweight model\n",
    "        columns=['title', 'actor_name'],  # Use title and actor name\n",
    "        distance_metrics=['cosine', 'euclidean'],\n",
    "        pooling_strategy='concatenate'\n",
    "    )\n",
    "    \n",
    "    print(f\"Vector feature extractor created with model: all-MiniLM-L6-v2\")\n",
    "    print(f\"Vector feature names: {vector_extractor.get_feature_names()}\")\n",
    "    vector_features_available = True\n",
    "    \n",
    "    # Extract vector features for a subset of training data (for performance)\n",
    "    if len(valid_pairs_df) > 0:\n",
    "        # Use first 50 pairs for vector demo\n",
    "        sample_pairs = valid_pairs_df.head(50).copy()\n",
    "        sample_labels = valid_labels.head(50).copy()\n",
    "        \n",
    "        print(f\"\\nExtracting vector features for {len(sample_pairs)} sample pairs...\")\n",
    "        \n",
    "        vector_train_features = vector_extractor.create_features(\n",
    "            academy_df, actors_df, sample_pairs, labels=sample_labels\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Vector training features: {vector_train_features.shape}\")\n",
    "        display(vector_train_features.head(3))\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠ sentence-transformers not available - skipping vector features\")\n",
    "    print(\"To install: pip install sentence-transformers\")\n",
    "    vector_train_features = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error initializing vector features: {e}\")\n",
    "    vector_train_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-4-md",
   "metadata": {},
   "source": [
    "## Step 4: Train Machine Learning Models\n",
    "\n",
    "We'll train multiple scikit-learn classifiers and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-training-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for ML models\n",
    "trained_models = {}\n",
    "\n",
    "if len(train_features) > 0:\n",
    "    # Prepare feature matrix and labels\n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X = train_features[feature_columns]\n",
    "    y = train_features['label']\n",
    "    \n",
    "    print(f\"Training data prepared:\")\n",
    "    print(f\"  Features: {X.shape}\")\n",
    "    print(f\"  Labels: {y.shape}\")\n",
    "    print(f\"  Positive class: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Negative class: {len(y) - sum(y)} ({(len(y) - sum(y))/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Split into train/validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Train/validation split: {len(X_train)}/{len(X_val)}\")\n",
    "else:\n",
    "    print(\"No training features available - cannot train models\")\n",
    "    X_train = X_val = y_train = y_val = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple ML models\n",
    "if len(X_train) > 0:\n",
    "    # Define models to train\n",
    "    models_to_train = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'DecisionTree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "        'SVM': SVC(random_state=42, probability=True)  # probability=True for predict_proba\n",
    "    }\n",
    "    \n",
    "    print(\"Training ML models...\\n\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for model_name, model in models_to_train.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Validation predictions\n",
    "            val_predictions = model.predict(X_val)\n",
    "            val_probabilities = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else val_predictions\n",
    "            \n",
    "            # Calculate metrics\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "            \n",
    "            accuracy = accuracy_score(y_val, val_predictions)\n",
    "            precision = precision_score(y_val, val_predictions, zero_division=0)\n",
    "            recall = recall_score(y_val, val_predictions, zero_division=0)\n",
    "            f1 = f1_score(y_val, val_predictions, zero_division=0)\n",
    "            \n",
    "            # Store results\n",
    "            model_results.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1': f1\n",
    "            })\n",
    "            \n",
    "            # Store trained model\n",
    "            trained_models[model_name] = model\n",
    "            \n",
    "            print(f\"  ✓ {model_name}: F1={f1:.3f}, Precision={precision:.3f}, Recall={recall:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {model_name} training failed: {e}\")\n",
    "    \n",
    "    # Display results table\n",
    "    if model_results:\n",
    "        results_df = pd.DataFrame(model_results).round(3)\n",
    "        print(f\"\\n=== Model Comparison ===\\n\")\n",
    "        display(results_df)\n",
    "        \n",
    "        # Find best model\n",
    "        best_model_idx = results_df['F1'].idxmax()\n",
    "        best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "        best_f1 = results_df.loc[best_model_idx, 'F1']\n",
    "        \n",
    "        print(f\"\\n🏆 Best model: {best_model_name} (F1: {best_f1:.3f})\")\n",
    "        \n",
    "    print(f\"\\n✓ {len(trained_models)} models trained successfully\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot train models - no training data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-5-md",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for entity matching decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for models that support it\n",
    "if trained_models and len(X_train) > 0:\n",
    "    print(\"=== Feature Importance Analysis ===\\n\")\n",
    "    \n",
    "    for model_name, model in trained_models.items():\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(f\"\\n{model_name} Feature Importance:\")\n",
    "            \n",
    "            try:\n",
    "                # Create MLBasedMatcher to get feature importance\n",
    "                matcher = MLBasedMatcher(traditional_extractor)\n",
    "                importance_df = matcher.get_feature_importance(model, feature_columns)\n",
    "                \n",
    "                # Display top features\n",
    "                display(importance_df.head(8))\n",
    "                \n",
    "                # Create visualization\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                top_features = importance_df.head(8)\n",
    "                plt.barh(range(len(top_features)), top_features['importance'])\n",
    "                plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "                plt.xlabel('Feature Importance')\n",
    "                plt.title(f'{model_name} - Top Feature Importances')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing feature importance: {e}\")\n",
    "        \n",
    "        elif hasattr(model, 'coef_'):\n",
    "            print(f\"\\n{model_name} Coefficients:\")\n",
    "            # For linear models, show coefficients\n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'coefficient': model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "            }).reindex(model.coef_[0].argsort()[::-1] if len(model.coef_.shape) > 1 else model.coef_.argsort()[::-1])\n",
    "            \n",
    "            display(coef_df.head(8))\n",
    "        \n",
    "        else:\n",
    "            print(f\"{model_name}: Feature importance not available for this model type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-6-md",
   "metadata": {},
   "source": [
    "## Step 6: MLBasedMatcher Usage\n",
    "\n",
    "Now we'll use our trained models with the MLBasedMatcher to find entity correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-test-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate pairs for testing MLBasedMatcher\n",
    "def create_sample_candidates(df_left, df_right, max_pairs=200, strategy=\"random\"):\n",
    "    \"\"\"Create candidate pairs for entity matching.\"\"\"\n",
    "    left_ids = df_left['_id'].tolist()\n",
    "    right_ids = df_right['_id'].tolist()\n",
    "    \n",
    "    if strategy == \"random\":\n",
    "        # Random sampling\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        candidates = []\n",
    "        for _ in range(min(max_pairs, len(left_ids) * len(right_ids))):\n",
    "            left_id = np.random.choice(left_ids)\n",
    "            right_id = np.random.choice(right_ids)\n",
    "            candidates.append((left_id, right_id))\n",
    "    elif strategy == \"title_similarity\":\n",
    "        # Simple title-based blocking (first character match)\n",
    "        candidates = []\n",
    "        \n",
    "        # Group by first character of title\n",
    "        left_groups = df_left.groupby(df_left['title'].str[0].fillna(''))['_id'].apply(list).to_dict()\n",
    "        right_groups = df_right.groupby(df_right['title'].str[0].fillna(''))['_id'].apply(list).to_dict()\n",
    "        \n",
    "        for key in left_groups:\n",
    "            if key in right_groups:\n",
    "                for left_id in left_groups[key]:\n",
    "                    for right_id in right_groups[key]:\n",
    "                        candidates.append((left_id, right_id))\n",
    "                        if len(candidates) >= max_pairs:\n",
    "                            break\n",
    "                    if len(candidates) >= max_pairs:\n",
    "                        break\n",
    "                if len(candidates) >= max_pairs:\n",
    "                    break\n",
    "    \n",
    "    # Convert to DataFrame and remove duplicates\n",
    "    candidate_df = pd.DataFrame(candidates, columns=['id1', 'id2']).drop_duplicates()\n",
    "    return candidate_df\n",
    "\n",
    "# Create test candidates\n",
    "test_candidates = create_sample_candidates(academy_df, actors_df, max_pairs=150, strategy=\"title_similarity\")\n",
    "print(f\"Created {len(test_candidates)} test candidate pairs\")\n",
    "display(test_candidates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlbased-matching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MLBasedMatcher with trained models\n",
    "if trained_models and len(test_candidates) > 0:\n",
    "    print(\"=== MLBasedMatcher Results ===\\n\")\n",
    "    \n",
    "    # Create MLBasedMatcher\n",
    "    ml_matcher = MLBasedMatcher(traditional_extractor)\n",
    "    \n",
    "    # Test each trained model\n",
    "    ml_results = {}\n",
    "    \n",
    "    for model_name, trained_model in trained_models.items():\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Find matches using trained model\n",
    "            matches = ml_matcher.match(\n",
    "                df_left=academy_df,\n",
    "                df_right=actors_df,\n",
    "                candidates=[test_candidates],\n",
    "                trained_classifier=trained_model,\n",
    "                threshold=0.5,  # 50% confidence threshold\n",
    "                use_probabilities=True\n",
    "            )\n",
    "            \n",
    "            print(f\"  ✓ Found {len(matches)} matches above threshold 0.5\")\n",
    "            \n",
    "            # Store results\n",
    "            ml_results[model_name] = matches\n",
    "            \n",
    "            # Show top matches\n",
    "            if len(matches) > 0:\n",
    "                print(f\"  Top matches:\")\n",
    "                top_matches = matches.sort_values('score', ascending=False).head(3)\n",
    "                \n",
    "                for _, match in top_matches.iterrows():\n",
    "                    id1, id2, score = match['id1'], match['id2'], match['score']\n",
    "                    \n",
    "                    # Get titles for display\n",
    "                    title1 = academy_df[academy_df['_id'] == id1]['title'].iloc[0]\n",
    "                    title2 = actors_df[actors_df['_id'] == id2]['title'].iloc[0]\n",
    "                    \n",
    "                    print(f\"    {score:.3f}: '{title1}' <-> '{title2}'\")\n",
    "            \n",
    "            print()  # New line\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error with {model_name}: {e}\\n\")\n",
    "    \n",
    "    print(f\"✓ MLBasedMatcher testing complete\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot test MLBasedMatcher - no trained models or test candidates available\")\n",
    "    ml_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-7-md",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation on Test Set\n",
    "\n",
    "Let's evaluate our models on the test correspondences to get proper performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set if available\n",
    "if len(test_corr) > 0 and trained_models:\n",
    "    print(\"=== Test Set Evaluation ===\\n\")\n",
    "    \n",
    "    # Prepare test pairs (filter valid ones)\n",
    "    test_pairs = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for idx, row in test_corr.iterrows():\n",
    "        id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "        \n",
    "        # Check if both records exist\n",
    "        if (id1 in academy_df['_id'].values and \n",
    "            id2 in actors_df['_id'].values):\n",
    "            test_pairs.append({'id1': id1, 'id2': id2})\n",
    "            test_labels.append(label)\n",
    "    \n",
    "    test_pairs_df = pd.DataFrame(test_pairs)\n",
    "    test_labels = pd.Series(test_labels)\n",
    "    \n",
    "    print(f\"Valid test pairs: {len(test_pairs_df)} out of {len(test_corr)}\")\n",
    "    print(f\"Test positive class: {sum(test_labels)} ({sum(test_labels)/len(test_labels)*100:.1f}%)\")\n",
    "    \n",
    "    if len(test_pairs_df) > 0:\n",
    "        # Create MLBasedMatcher\n",
    "        ml_matcher = MLBasedMatcher(traditional_extractor)\n",
    "        \n",
    "        test_evaluation_results = []\n",
    "        \n",
    "        for model_name, trained_model in trained_models.items():\n",
    "            print(f\"\\nEvaluating {model_name} on test set...\")\n",
    "            \n",
    "            try:\n",
    "                # Get predictions for test pairs\n",
    "                predictions_df = ml_matcher.predict_pairs(\n",
    "                    academy_df, actors_df, test_pairs_df, trained_model, use_probabilities=True\n",
    "                )\n",
    "                \n",
    "                if len(predictions_df) > 0:\n",
    "                    # Convert to binary predictions with different thresholds\n",
    "                    thresholds = [0.3, 0.5, 0.7]\n",
    "                    \n",
    "                    for threshold in thresholds:\n",
    "                        binary_predictions = (predictions_df['prediction'] >= threshold).astype(int)\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        accuracy = accuracy_score(test_labels, binary_predictions)\n",
    "                        precision = precision_score(test_labels, binary_predictions, zero_division=0)\n",
    "                        recall = recall_score(test_labels, binary_predictions, zero_division=0)\n",
    "                        f1 = f1_score(test_labels, binary_predictions, zero_division=0)\n",
    "                        \n",
    "                        test_evaluation_results.append({\n",
    "                            'Model': model_name,\n",
    "                            'Threshold': threshold,\n",
    "                            'Accuracy': accuracy,\n",
    "                            'Precision': precision,\n",
    "                            'Recall': recall,\n",
    "                            'F1': f1\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  Threshold {threshold}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error evaluating {model_name}: {e}\")\n",
    "        \n",
    "        # Display test results\n",
    "        if test_evaluation_results:\n",
    "            test_results_df = pd.DataFrame(test_evaluation_results).round(3)\n",
    "            print(f\"\\n=== Test Set Results ===\\n\")\n",
    "            display(test_results_df)\n",
    "            \n",
    "            # Find best configuration\n",
    "            best_test_idx = test_results_df['F1'].idxmax()\n",
    "            best_test = test_results_df.loc[best_test_idx]\n",
    "            print(f\"\\n🏆 Best test performance: {best_test['Model']} @ threshold {best_test['Threshold']} (F1: {best_test['F1']:.3f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"Test evaluation not available - no test correspondences or trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-8-md",
   "metadata": {},
   "source": [
    "## Step 8: Advanced MLBasedMatcher Features\n",
    "\n",
    "Let's explore additional features of the MLBasedMatcher including threshold analysis and prediction utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis for optimal decision boundary\n",
    "if trained_models and len(test_pairs_df) > 0:\n",
    "    print(\"=== Threshold Analysis ===\\n\")\n",
    "    \n",
    "    # Use best performing model\n",
    "    if 'best_model_name' in globals() and best_model_name in trained_models:\n",
    "        best_model = trained_models[best_model_name]\n",
    "        print(f\"Using best model: {best_model_name}\")\n",
    "    else:\n",
    "        # Use first available model\n",
    "        best_model_name = list(trained_models.keys())[0]\n",
    "        best_model = trained_models[best_model_name]\n",
    "        print(f\"Using model: {best_model_name}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    ml_matcher = MLBasedMatcher(traditional_extractor)\n",
    "    predictions_df = ml_matcher.predict_pairs(\n",
    "        academy_df, actors_df, test_pairs_df, best_model, use_probabilities=True\n",
    "    )\n",
    "    \n",
    "    if len(predictions_df) > 0:\n",
    "        # Test different thresholds\n",
    "        threshold_range = np.arange(0.1, 1.0, 0.1)\n",
    "        threshold_analysis = []\n",
    "        \n",
    "        for threshold in threshold_range:\n",
    "            binary_predictions = (predictions_df['prediction'] >= threshold).astype(int)\n",
    "            \n",
    "            precision = precision_score(test_labels, binary_predictions, zero_division=0)\n",
    "            recall = recall_score(test_labels, binary_predictions, zero_division=0)\n",
    "            f1 = f1_score(test_labels, binary_predictions, zero_division=0)\n",
    "            \n",
    "            threshold_analysis.append({\n",
    "                'Threshold': threshold,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1': f1,\n",
    "                'Predictions': sum(binary_predictions)\n",
    "            })\n",
    "        \n",
    "        threshold_df = pd.DataFrame(threshold_analysis).round(3)\n",
    "        display(threshold_df)\n",
    "        \n",
    "        # Plot precision-recall curve\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'o-', label='Precision')\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Recall'], 's-', label='Recall')\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['F1'], '^-', label='F1')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'{best_model_name} - Precision/Recall vs Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(threshold_df['Recall'], threshold_df['Precision'], 'o-')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'{best_model_name} - Precision-Recall Curve')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find optimal threshold (best F1)\n",
    "        optimal_idx = threshold_df['F1'].idxmax()\n",
    "        optimal_threshold = threshold_df.loc[optimal_idx, 'Threshold']\n",
    "        optimal_f1 = threshold_df.loc[optimal_idx, 'F1']\n",
    "        \n",
    "        print(f\"\\n🎯 Optimal threshold: {optimal_threshold:.1f} (F1: {optimal_f1:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-9-md",
   "metadata": {},
   "source": [
    "## Step 9: Complete ML Pipeline\n",
    "\n",
    "Let's put everything together in a complete machine learning entity matching pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ml_entity_matching_pipeline(\n",
    "    df_left, df_right, \n",
    "    train_correspondences=None, \n",
    "    test_correspondences=None,\n",
    "    output_dir=None\n",
    "):\n",
    "    \"\"\"Complete ML entity matching pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=== Complete ML Entity Matching Pipeline ===\")\n",
    "    print(f\"Left dataset: {df_left.attrs.get('dataset_name', 'unknown')} ({len(df_left)} records)\")\n",
    "    print(f\"Right dataset: {df_right.attrs.get('dataset_name', 'unknown')} ({len(df_right)} records)\")\n",
    "    \n",
    "    pipeline_results = {}\n",
    "    \n",
    "    # Step 1: Feature Engineering\n",
    "    print(\"\\n1. Feature Engineering...\")\n",
    "    \n",
    "    # Create comprehensive feature extractor\n",
    "    comparators = [\n",
    "        StringComparator(\"title\", \"jaro_winkler\", str.lower),\n",
    "        StringComparator(\"title\", \"cosine\", str.lower),\n",
    "        DateComparator(\"date\", max_days_difference=365),\n",
    "        StringComparator(\"actor_name\", \"jaro_winkler\", str.lower),\n",
    "    ]\n",
    "    feature_extractor = FeatureExtractor(comparators)\n",
    "    \n",
    "    print(f\"   ✓ Created feature extractor with {len(comparators)} features\")\n",
    "    \n",
    "    # Step 2: Training Data Preparation\n",
    "    if train_correspondences is not None and len(train_correspondences) > 0:\n",
    "        print(\"\\n2. Preparing training data...\")\n",
    "        \n",
    "        # Filter valid pairs\n",
    "        train_pairs = []\n",
    "        train_labels = []\n",
    "        for _, row in train_correspondences.iterrows():\n",
    "            id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "            if (id1 in df_left['_id'].values and id2 in df_right['_id'].values):\n",
    "                train_pairs.append({'id1': id1, 'id2': id2})\n",
    "                train_labels.append(label)\n",
    "        \n",
    "        if len(train_pairs) > 0:\n",
    "            train_pairs_df = pd.DataFrame(train_pairs)\n",
    "            train_labels = pd.Series(train_labels)\n",
    "            \n",
    "            # Extract features\n",
    "            train_features = feature_extractor.create_features(\n",
    "                df_left, df_right, train_pairs_df, train_labels\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✓ Training features: {train_features.shape}\")\n",
    "            print(f\"   ✓ Positive examples: {sum(train_labels)} ({sum(train_labels)/len(train_labels)*100:.1f}%)\")\n",
    "            \n",
    "            # Step 3: Model Training\n",
    "            print(\"\\n3. Training ML models...\")\n",
    "            \n",
    "            feature_cols = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "            X = train_features[feature_cols]\n",
    "            y = train_features['label']\n",
    "            \n",
    "            # Train best performing model (Random Forest)\n",
    "            best_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            best_model.fit(X, y)\n",
    "            \n",
    "            print(f\"   ✓ Trained Random Forest classifier\")\n",
    "            pipeline_results['trained_model'] = best_model\n",
    "            pipeline_results['feature_extractor'] = feature_extractor\n",
    "            \n",
    "            # Step 4: Model Application\n",
    "            print(\"\\n4. Applying ML model for matching...\")\n",
    "            \n",
    "            # Create candidate pairs\n",
    "            candidates = create_sample_candidates(df_left, df_right, max_pairs=200, strategy=\"title_similarity\")\n",
    "            print(f\"   ✓ Generated {len(candidates)} candidate pairs\")\n",
    "            \n",
    "            # Use MLBasedMatcher\n",
    "            ml_matcher = MLBasedMatcher(feature_extractor)\n",
    "            matches = ml_matcher.match(\n",
    "                df_left, df_right, [candidates], best_model, threshold=0.5\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✓ Found {len(matches)} matches above threshold 0.5\")\n",
    "            pipeline_results['matches'] = matches\n",
    "            \n",
    "            # Step 5: Evaluation (if test data available)\n",
    "            if test_correspondences is not None and len(test_correspondences) > 0:\n",
    "                print(\"\\n5. Model evaluation...\")\n",
    "                \n",
    "                # Prepare test data\n",
    "                test_pairs = []\n",
    "                test_labels = []\n",
    "                for _, row in test_correspondences.iterrows():\n",
    "                    id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "                    if (id1 in df_left['_id'].values and id2 in df_right['_id'].values):\n",
    "                        test_pairs.append({'id1': id1, 'id2': id2})\n",
    "                        test_labels.append(label)\n",
    "                \n",
    "                if len(test_pairs) > 0:\n",
    "                    test_pairs_df = pd.DataFrame(test_pairs)\n",
    "                    test_labels = pd.Series(test_labels)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions_df = ml_matcher.predict_pairs(\n",
    "                        df_left, df_right, test_pairs_df, best_model\n",
    "                    )\n",
    "                    \n",
    "                    if len(predictions_df) > 0:\n",
    "                        binary_predictions = (predictions_df['prediction'] >= 0.5).astype(int)\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        eval_results = {\n",
    "                            'accuracy': accuracy_score(test_labels, binary_predictions),\n",
    "                            'precision': precision_score(test_labels, binary_predictions, zero_division=0),\n",
    "                            'recall': recall_score(test_labels, binary_predictions, zero_division=0),\n",
    "                            'f1': f1_score(test_labels, binary_predictions, zero_division=0)\n",
    "                        }\n",
    "                        \n",
    "                        pipeline_results['evaluation'] = eval_results\n",
    "                        print(f\"   ✓ Test F1: {eval_results['f1']:.3f}, Precision: {eval_results['precision']:.3f}, Recall: {eval_results['recall']:.3f}\")\n",
    "            \n",
    "            # Step 6: Output Generation\n",
    "            if output_dir:\n",
    "                print(f\"\\n6. Saving outputs to {output_dir}...\")\n",
    "                output_path = Path(output_dir)\n",
    "                output_path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Save matches\n",
    "                if len(matches) > 0:\n",
    "                    matches.to_csv(output_path / \"ml_entity_matches.csv\", index=False)\n",
    "                    print(f\"   ✓ Saved {len(matches)} matches\")\n",
    "                \n",
    "                # Save model info\n",
    "                model_info = {\n",
    "                    'model_type': 'RandomForestClassifier',\n",
    "                    'n_features': len(feature_cols),\n",
    "                    'feature_names': feature_cols,\n",
    "                    'training_samples': len(train_features),\n",
    "                    'threshold': 0.5\n",
    "                }\n",
    "                \n",
    "                if 'evaluation' in pipeline_results:\n",
    "                    model_info.update(pipeline_results['evaluation'])\n",
    "                \n",
    "                with open(output_path / \"model_info.json\", 'w') as f:\n",
    "                    json.dump(model_info, f, indent=2)\n",
    "                \n",
    "                print(f\"   ✓ Saved model information\")\n",
    "        \n",
    "        else:\n",
    "            print(\"   ✗ No valid training pairs found\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n2. No training data provided - cannot train ML models\")\n",
    "    \n",
    "    return pipeline_results\n",
    "\n",
    "# Run complete pipeline\n",
    "if len(train_corr) > 0:\n",
    "    output_dir = root / \"output\" / \"examples\" / \"ml_entitymatching\"\n",
    "    \n",
    "    pipeline_results = complete_ml_entity_matching_pipeline(\n",
    "        df_left=academy_df,\n",
    "        df_right=actors_df,\n",
    "        train_correspondences=train_corr,\n",
    "        test_correspondences=test_corr if len(test_corr) > 0 else None,\n",
    "        output_dir=str(output_dir)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Complete ML pipeline finished!\")\n",
    "    print(f\"📁 Check {output_dir} for outputs\")\n",
    "    \n",
    "    if 'evaluation' in pipeline_results:\n",
    "        eval_results = pipeline_results['evaluation']\n",
    "        print(f\"📊 Final Performance: F1={eval_results['f1']:.3f}, P={eval_results['precision']:.3f}, R={eval_results['recall']:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot run complete pipeline - no training correspondences available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated the complete machine learning entity matching workflow in PyDI using the MLBasedMatcher:\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "\n",
    "1. **Traditional Feature Extraction**: Convert entity pairs to similarity features using comparators\n",
    "   - String similarity (Jaro-Winkler, Levenshtein, Cosine)\n",
    "   - Date/numeric comparisons\n",
    "   - Custom feature functions\n",
    "   \n",
    "2. **Vector-Based Features** (optional): Deep feature representation using embeddings\n",
    "   - Sentence transformer models\n",
    "   - Multiple distance metrics (cosine, euclidean, manhattan)\n",
    "   - Flexible pooling strategies\n",
    "\n",
    "3. **ML Model Training**: Full scikit-learn integration\n",
    "   - Multiple classifier types (Random Forest, Logistic Regression, SVM, etc.)\n",
    "   - Proper train/validation splits\n",
    "   - Cross-validation and hyperparameter tuning ready\n",
    "\n",
    "4. **MLBasedMatcher Usage**: Production-ready matching with trained models\n",
    "   - Probabilistic and binary prediction modes\n",
    "   - Batch processing of candidates\n",
    "   - Flexible threshold configuration\n",
    "\n",
    "5. **Model Evaluation**: Comprehensive performance analysis\n",
    "   - Precision, recall, F1 score calculation\n",
    "   - Threshold optimization\n",
    "   - Feature importance analysis\n",
    "\n",
    "6. **Production Pipeline**: End-to-end workflow with proper outputs\n",
    "   - Structured result files\n",
    "   - Model metadata and configuration\n",
    "   - Reproducible execution\n",
    "\n",
    "### Best Practices for ML Entity Matching:\n",
    "\n",
    "1. **Feature Engineering is Crucial**: Combine multiple similarity measures for robust feature representation\n",
    "\n",
    "2. **Handle Class Imbalance**: Entity matching datasets typically have many more non-matches than matches\n",
    "\n",
    "3. **Threshold Tuning**: Different applications require different precision/recall trade-offs\n",
    "\n",
    "4. **Cross-Validation**: Use proper validation techniques to avoid overfitting\n",
    "\n",
    "5. **Feature Importance**: Understand which features drive matching decisions for interpretability\n",
    "\n",
    "6. **Scalability**: Consider blocking/candidate generation strategies for large datasets\n",
    "\n",
    "### Integration with Scikit-learn Ecosystem:\n",
    "\n",
    "The MLBasedMatcher provides minimal wrapping around scikit-learn, allowing you to:\n",
    "- Use any scikit-learn classifier\n",
    "- Apply hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\n",
    "- Use ensemble methods and pipelines\n",
    "- Leverage feature selection and preprocessing utilities\n",
    "- Apply cross-validation and model evaluation tools\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Advanced Blocking**: Implement sophisticated candidate generation strategies\n",
    "- **Deep Learning**: Experiment with neural network architectures for entity matching\n",
    "- **Active Learning**: Iteratively improve models with human feedback\n",
    "- **Ensemble Methods**: Combine multiple matching approaches for better performance\n",
    "- **Domain Adaptation**: Apply to your specific datasets with domain-specific comparators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}