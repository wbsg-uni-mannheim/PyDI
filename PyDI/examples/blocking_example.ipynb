{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching Blocking Demonstration\n",
    "\n",
    "This notebook demonstrates end-to-end candidate generation using multiple blocking strategies in PyDI. We'll work with movie datasets (academy awards and actors) to showcase various blocking techniques without performing actual matching.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Blocking** is a critical preprocessing step in entity matching that reduces the number of candidate pairs from the full Cartesian product to a manageable subset. This notebook demonstrates:\n",
    "\n",
    "- **NoBlocking**: Full Cartesian product with sampling (baseline)\n",
    "- **StandardBlocking**: Equality-based blocking on shared attributes\n",
    "- **SortedNeighbourhood**: Sequential similarity with sliding windows\n",
    "- **TokenBlocking**: Token-based blocking with deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /Users/aaronsteiner/Documents/GitHub/PyDI\n",
      "Output directory: /Users/aaronsteiner/Documents/GitHub/PyDI/output/examples/entitymatching/blocking_demo\n",
      "Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import json \n",
    "from datetime import datetime\n",
    "\n",
    "# PyDI imports\n",
    "from PyDI.io.loaders import load_xml, load_csv\n",
    "from PyDI.profiling import DataProfiler\n",
    "from PyDI.entitymatching.blocking import (\n",
    "    NoBlocking,\n",
    "    StandardBlocking, \n",
    "    SortedNeighbourhood,\n",
    "    TokenBlocking,\n",
    "    EmbeddingBlocking\n",
    ")\n",
    "from PyDI.entitymatching import BlockingEvaluator\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cross-platform path handling - works on both Windows and Mac\n",
    "root = Path.cwd().parents[1]  # repo root fallback for notebooks\n",
    "\n",
    "# Set up output directory using absolute paths from root\n",
    "OUTPUT_DIR = root / \"output\" / \"examples\" / \"entitymatching\" / \"blocking_demo\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize profiler\n",
    "profiler = DataProfiler()\n",
    "\n",
    "print(f\"Repository root: {root}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Profiling\n",
    "\n",
    "We'll load the movie datasets using PyDI's XML loader, which automatically adds unique identifiers and provenance metadata, then profile them using PyDI's DataProfiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:48,333 - PyDI.io.loaders - INFO - Loaded dataset 'academy_awards' via read_xml_flattened: shape=(4592, 7), source=/Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/entitymatching/data/academy_awards.xml\n",
      "2025-08-29 11:48:48,336 - PyDI.io.loaders - INFO - Loaded dataset 'actors' via read_xml_flattened: shape=(149, 7), source=/Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/entitymatching/data/actors.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academy Awards path: /Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/entitymatching/data/academy_awards.xml\n",
      "Actors path: /Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/entitymatching/data/actors.xml\n",
      "Academy Awards exists: True\n",
      "Actors exists: True\n",
      "\n",
      "Loading academy awards dataset...\n",
      "Loading actors dataset...\n",
      "Academy Awards dataset shape: (4592, 7)\n",
      "Actors dataset shape: (149, 7)\n",
      "\n",
      "=== Academy Awards Dataset Preview ===\n",
      "                   _id                id               title       actor_name  \\\n",
      "0  academy_awards-0000  academy_awards_1            Biutiful    Javier Bardem   \n",
      "1  academy_awards-0001  academy_awards_2           True Grit     Jeff Bridges   \n",
      "2  academy_awards-0002  academy_awards_2           True Grit     Jeff Bridges   \n",
      "3  academy_awards-0003  academy_awards_3  The Social Network  Jesse Eisenberg   \n",
      "4  academy_awards-0004  academy_awards_4   The King's Speech      Colin Firth   \n",
      "\n",
      "         date  director_name oscar  \n",
      "0  2010-01-01            NaN   NaN  \n",
      "1  2010-01-01      Joel Coen   NaN  \n",
      "2  2010-01-01     Ethan Coen   NaN  \n",
      "3  2010-01-01  David Fincher   yes  \n",
      "4  2010-01-01     Tom Hooper   yes  \n",
      "\n",
      "Columns: ['_id', 'id', 'title', 'actor_name', 'date', 'director_name', 'oscar']\n",
      "\n",
      "=== Actors Dataset Preview ===\n",
      "           _id        id                       title      actor_name  \\\n",
      "0  actors-0000  actors_1                  7th Heaven    Janet Gaynor   \n",
      "1  actors-0001  actors_2                    Coquette   Mary Pickford   \n",
      "2  actors-0002  actors_3                The Divorcee   Norma Shearer   \n",
      "3  actors-0003  actors_4                Min and Bill  Marie Dressler   \n",
      "4  actors-0004  actors_5  The Sin of Madelon Claudet     Helen Hayes   \n",
      "\n",
      "  actors_actor_birthday actors_actor_birthplace        date  \n",
      "0            1906-01-01            Pennsylvania  1929-01-01  \n",
      "1            1892-01-01                  Canada  1930-01-01  \n",
      "2            1902-01-01                  Canada  1931-01-01  \n",
      "3            1868-01-01                  Canada  1932-01-01  \n",
      "4            1900-01-01           Washington DC  1933-01-01  \n",
      "\n",
      "Columns: ['_id', 'id', 'title', 'actor_name', 'actors_actor_birthday', 'actors_actor_birthplace', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the movie datasets using cross-platform paths\n",
    "ACADEMY_AWARDS_PATH = root / \"input\" / \"movies\" / \"entitymatching\" / \"data\" / \"academy_awards.xml\"\n",
    "ACTORS_PATH = root / \"input\" / \"movies\" / \"entitymatching\" / \"data\" / \"actors.xml\"\n",
    "\n",
    "print(f\"Academy Awards path: {ACADEMY_AWARDS_PATH}\")\n",
    "print(f\"Actors path: {ACTORS_PATH}\")\n",
    "print(f\"Academy Awards exists: {ACADEMY_AWARDS_PATH.exists()}\")\n",
    "print(f\"Actors exists: {ACTORS_PATH.exists()}\")\n",
    "\n",
    "# Load the datasets\n",
    "print(\"\\nLoading academy awards dataset...\")\n",
    "df_awards = load_xml(\n",
    "    ACADEMY_AWARDS_PATH, \n",
    "    name=\"academy_awards\",\n",
    "    add_index=True,\n",
    "    index_column_name=\"_id\"\n",
    ")\n",
    "\n",
    "print(\"Loading actors dataset...\")\n",
    "df_actors = load_xml(\n",
    "    ACTORS_PATH, \n",
    "    name=\"actors\",\n",
    "    add_index=True,\n",
    "    index_column_name=\"_id\"\n",
    ")\n",
    "\n",
    "print(f\"Academy Awards dataset shape: {df_awards.shape}\")\n",
    "print(f\"Actors dataset shape: {df_actors.shape}\")\n",
    "\n",
    "# Preview the datasets\n",
    "print(\"\\n=== Academy Awards Dataset Preview ===\")\n",
    "print(df_awards.head())\n",
    "print(\"\\nColumns:\", list(df_awards.columns))\n",
    "\n",
    "print(\"\\n=== Actors Dataset Preview ===\")\n",
    "print(df_actors.head())\n",
    "print(\"\\nColumns:\", list(df_actors.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling & Analysis\n",
    "\n",
    "Let's use PyDI's DataProfiler to analyze our datasets comprehensively. The DataProfiler provides:\n",
    "\n",
    "- **Quick summaries**: Basic statistics without heavy report generation\n",
    "- **Detailed HTML reports**: Rich profiling using ydata-profiling (optional)\n",
    "- **Dataset comparison**: Side-by-side analysis using sweetviz (optional)\n",
    "\n",
    "This will help us understand data quality, column distributions, and identify the best columns for blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Profiling ===\n",
      "\n",
      "--- Academy Awards Summary ---\n",
      "rows: 4,592\n",
      "columns: 7\n",
      "nulls_total: 11,036\n",
      "nulls_per_column: 11036 total nulls\n",
      "  Columns with nulls:\n",
      "    title: 12 (0.3%)\n",
      "    actor_name: 3535 (77.0%)\n",
      "    director_name: 4172 (90.9%)\n",
      "    oscar: 3317 (72.2%)\n",
      "Column types: 2 unique types\n",
      "  string: 1 columns\n",
      "  object: 6 columns\n",
      "\n",
      "--- Actors Summary ---\n",
      "rows: 149\n",
      "columns: 7\n",
      "nulls_total: 0\n",
      "nulls_per_column: 0 total nulls\n",
      "Column types: 2 unique types\n",
      "  string: 1 columns\n",
      "  object: 6 columns\n",
      "\n",
      "--- Column Analysis ---\n",
      "Academy Awards columns: ['_id', 'actor_name', 'date', 'director_name', 'id', 'oscar', 'title']\n",
      "Actors columns: ['_id', 'actor_name', 'actors_actor_birthday', 'actors_actor_birthplace', 'date', 'id', 'title']\n",
      "Common columns for blocking: ['actor_name', 'date', 'id', 'title']\n",
      "\n",
      "--- Sample Values ---\n",
      "Academy Awards title: ['Eskimo', 'That Hamilton Woman', 'Task Force']\n",
      "Actors title: ['Erin Brockovich', 'To Each His Own', 'In the Heat of the Night']\n",
      "Academy Awards actor_name: ['Al Pacino', 'John Garfield', 'Laurence Olivier']\n",
      "Actors actor_name: ['Julia Roberts', 'Olivia de Havilland', 'Rod Steiger']\n",
      "Academy Awards director_name: ['Mike Nichols', 'Leo McCarey', 'George Lucas']\n",
      "\n",
      "--- Generating Detailed Reports ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 155.63it/s]0<00:00, 16.54it/s, Describe variable: oscar]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 33.84it/s, Completed]                \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.95it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 609.81it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Academy Awards profile: /Users/aaronsteiner/Documents/GitHub/PyDI/output/examples/entitymatching/blocking_demo/profiling/academy_awards_profile.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 601.57it/s]0<00:00, 24.14it/s, Describe variable: date]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 74.01it/s, Completed]                 \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.38it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 722.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Actors profile: /Users/aaronsteiner/Documents/GitHub/PyDI/output/examples/entitymatching/blocking_demo/profiling/actors_profile.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate profiling reports and summaries using PyDI's DataProfiler\n",
    "print(\"=== Dataset Profiling ===\")\n",
    "\n",
    "# Create profiling output directory\n",
    "profiling_dir = OUTPUT_DIR / \"profiling\"\n",
    "\n",
    "# Generate quick summaries for both datasets\n",
    "print(\"\\n--- Academy Awards Summary ---\")\n",
    "awards_summary = profiler.summary(df_awards)\n",
    "for key, value in awards_summary.items():\n",
    "    if key == 'nulls_per_column':\n",
    "        print(f\"{key}: {sum(v for v in value.values())} total nulls\")\n",
    "        null_cols = {k: v for k, v in value.items() if v > 0}\n",
    "        if null_cols:\n",
    "            print(\"  Columns with nulls:\")\n",
    "            for col, count in null_cols.items():\n",
    "                print(f\"    {col}: {count} ({count/awards_summary['rows']*100:.1f}%)\")\n",
    "    elif key == 'dtypes':\n",
    "        print(f\"Column types: {len(set(value.values()))} unique types\")\n",
    "        type_counts = {}\n",
    "        for dtype in value.values():\n",
    "            type_counts[dtype] = type_counts.get(dtype, 0) + 1\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  {dtype}: {count} columns\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:,}\" if isinstance(value, int) else f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n--- Actors Summary ---\")\n",
    "actors_summary = profiler.summary(df_actors)\n",
    "for key, value in actors_summary.items():\n",
    "    if key == 'nulls_per_column':\n",
    "        print(f\"{key}: {sum(v for v in value.values())} total nulls\")\n",
    "        null_cols = {k: v for k, v in value.items() if v > 0}\n",
    "        if null_cols:\n",
    "            print(\"  Columns with nulls:\")\n",
    "            for col, count in null_cols.items():\n",
    "                print(f\"    {col}: {count} ({count/actors_summary['rows']*100:.1f}%)\")\n",
    "    elif key == 'dtypes':\n",
    "        print(f\"Column types: {len(set(value.values()))} unique types\")\n",
    "        type_counts = {}\n",
    "        for dtype in value.values():\n",
    "            type_counts[dtype] = type_counts.get(dtype, 0) + 1\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  {dtype}: {count} columns\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:,}\" if isinstance(value, int) else f\"{key}: {value}\")\n",
    "\n",
    "# Find common columns for blocking\n",
    "awards_cols = set(df_awards.columns)\n",
    "actors_cols = set(df_actors.columns)\n",
    "common_cols = awards_cols.intersection(actors_cols)\n",
    "common_cols.discard('_id')  # Remove the ID column\n",
    "\n",
    "print(f\"\\n--- Column Analysis ---\")\n",
    "print(f\"Academy Awards columns: {sorted(awards_cols)}\")\n",
    "print(f\"Actors columns: {sorted(actors_cols)}\")\n",
    "print(f\"Common columns for blocking: {sorted(common_cols)}\")\n",
    "\n",
    "# Sample values from key columns for insight\n",
    "print(f\"\\n--- Sample Values ---\")\n",
    "key_columns = ['title', 'actor_name', 'director_name'] if 'title' in common_cols else list(common_cols)[:3]\n",
    "for col in key_columns:\n",
    "    if col in df_awards.columns:\n",
    "        awards_sample = df_awards[col].dropna().sample(min(3, df_awards[col].nunique()), random_state=42).tolist()\n",
    "        print(f\"Academy Awards {col}: {awards_sample}\")\n",
    "    if col in df_actors.columns:\n",
    "        actors_sample = df_actors[col].dropna().sample(min(3, df_actors[col].nunique()), random_state=42).tolist()\n",
    "        print(f\"Actors {col}: {actors_sample}\")\n",
    "\n",
    "# Generate detailed HTML profiling reports (optional - requires ydata-profiling)\n",
    "print(f\"\\n--- Generating Detailed Reports ---\")\n",
    "try:\n",
    "    awards_profile_path = profiler.profile(df_awards, str(profiling_dir))\n",
    "    print(f\"‚úì Academy Awards profile: {awards_profile_path}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  ydata-profiling not installed - skipping detailed HTML reports\")\n",
    "    print(\"   Install with: pip install ydata-profiling\")\n",
    "\n",
    "try:\n",
    "    actors_profile_path = profiler.profile(df_actors, str(profiling_dir))\n",
    "    print(f\"‚úì Actors profile: {actors_profile_path}\")\n",
    "except ImportError:\n",
    "    pass  # Already warned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation Statistics\n",
    "\n",
    "Before diving into blocking strategies, let's understand the scale of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Cartesian product: 684,208 pairs\n",
      "Memory estimate (assuming 16 bytes per pair): 10.4 MB\n",
      "‚ö†Ô∏è  WARNING: Large dataset - blocking is essential!\n"
     ]
    }
   ],
   "source": [
    "# Calculate theoretical maximum pairs\n",
    "max_pairs = len(df_awards) * len(df_actors)\n",
    "print(f\"Full Cartesian product: {max_pairs:,} pairs\")\n",
    "print(f\"Memory estimate (assuming 16 bytes per pair): {max_pairs * 16 / 1024**2:.1f} MB\")\n",
    "\n",
    "if max_pairs > 100_000:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Large dataset - blocking is essential!\")\n",
    "elif max_pairs > 10_000:\n",
    "    print(\"‚ö†Ô∏è  CAUTION: Medium dataset - blocking recommended\")\n",
    "else:\n",
    "    print(\"‚úì Small dataset - blocking optional but educational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_stats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking Strategies Overview\n",
    "\n",
    "Each blocker follows PyDI's unified API pattern but uses different parameters for specifying columns:\n",
    "\n",
    "- **NoBlocking**: No column parameters (generates full Cartesian product)\n",
    "- **StandardBlocking**: `on=[column_list]` - accepts list of columns for exact matching\n",
    "- **SortedNeighbourhood**: `key=column, window=size` - single column for sorting with window size\n",
    "- **TokenBlocking**: `column=column, min_token_len=N` - single column for tokenization\n",
    "\n",
    "All blockers share: `batch_size` parameter and return `CandidateBatch` DataFrames with `id1, id2` columns.\n",
    "\n",
    "## 1. NoBlocking (Baseline)\n",
    "\n",
    "NoBlocking generates the full Cartesian product in manageable batches. This serves as our baseline and is only practical for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NoBlocking Strategy ===\n",
      "Estimated pairs: 684,208\n",
      "\n",
      "Sampling candidate pairs...\n",
      "Stopping early after 10 batches...\n",
      "Processed 10 batches\n",
      "Total pairs processed: 8,940\n",
      "Sample pairs collected: 100\n",
      "\n",
      "Sample candidate pairs:\n",
      "                id1         id2  batch\n",
      "academy_awards-0004 actors-0115      1\n",
      "academy_awards-0002 actors-0142      1\n",
      "academy_awards-0003 actors-0078      1\n",
      "academy_awards-0004 actors-0126      1\n",
      "academy_awards-0000 actors-0039      1\n",
      "academy_awards-0001 actors-0141      1\n",
      "academy_awards-0002 actors-0002      1\n",
      "academy_awards-0002 actors-0035      1\n",
      "academy_awards-0001 actors-0059      1\n",
      "academy_awards-0000 actors-0136      1\n",
      "\n",
      "üìä NoBlocking: 8,940 pairs processed in 10 batches\n"
     ]
    }
   ],
   "source": [
    "# Initialize NoBlocking\n",
    "no_blocker = NoBlocking(df_awards, df_actors, batch_size=1000)\n",
    "\n",
    "print(\"=== NoBlocking Strategy ===\")\n",
    "print(f\"Estimated pairs: {no_blocker.estimate_pairs():,}\")\n",
    "\n",
    "# Sample some candidate pairs to avoid memory issues\n",
    "print(\"\\nSampling candidate pairs...\")\n",
    "# Process all batches\n",
    "all_pairs = []\n",
    "batch_count = 0\n",
    "\n",
    "for batch in no_blocker:\n",
    "    batch_count += 1\n",
    "sample_pairs = []\n",
    "batch_count = 0\n",
    "pair_count = 0\n",
    "\n",
    "for batch in no_blocker:\n",
    "    batch_count += 1\n",
    "    pair_count += len(batch)\n",
    "    \n",
    "    # Sample from this batch\n",
    "    if len(sample_pairs) < 100:  # Keep only first 100 for display\n",
    "        sample_size = min(10, len(batch))\n",
    "        sample_indices = np.random.choice(len(batch), sample_size, replace=False)\n",
    "        for idx in sample_indices:\n",
    "            sample_pairs.append({\n",
    "                'id1': batch.iloc[idx]['id1'],\n",
    "                'id2': batch.iloc[idx]['id2'],\n",
    "                'batch': batch_count\n",
    "            })\n",
    "    \n",
    "    # Stop early if too many pairs\n",
    "    if batch_count >= 10:\n",
    "        print(f\"Stopping early after {batch_count} batches...\")\n",
    "        break\n",
    "\n",
    "print(f\"Processed {batch_count} batches\")\n",
    "print(f\"Total pairs processed: {pair_count:,}\")\n",
    "print(f\"Sample pairs collected: {len(sample_pairs)}\")\n",
    "\n",
    "# Display sample pairs\n",
    "if sample_pairs:\n",
    "    sample_df = pd.DataFrame(sample_pairs[:10])\n",
    "    print(\"\\nSample candidate pairs:\")\n",
    "    print(sample_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìä NoBlocking: {pair_count:,} pairs processed in {batch_count} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StandardBlocking\n",
    "\n",
    "StandardBlocking groups records by exact matches on a specified attribute. Records are only compared if they have identical values for the blocking key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== StandardBlocking on 'title' ===\n",
      "Estimated pairs: 138\n",
      "Generated 138 candidate pairs in 1 batches\n",
      "Reduction ratio: 0.0002 (100.0% reduction)\n",
      "\\nSample candidate pairs:\n",
      "                id1         id2\n",
      "academy_awards-0001 actors-0119\n",
      "academy_awards-0002 actors-0119\n",
      "academy_awards-2194 actors-0119\n",
      "academy_awards-0347 actors-0077\n",
      "academy_awards-0348 actors-0148\n",
      "academy_awards-0405 actors-0147\n",
      "academy_awards-0412 actors-0076\n",
      "academy_awards-0452 actors-0146\n",
      "academy_awards-0457 actors-0075\n",
      "academy_awards-0507 actors-0145\n",
      "\\nBlocking statistics:\n",
      "Number of blocks: 128\n",
      "Average block size (sample): 1.2\n",
      "Max block size (sample): 3\n",
      "\\nüìä StandardBlocking: 138 pairs (0.0002 ratio)\n"
     ]
    }
   ],
   "source": [
    "# Find the best blocking column (most common between datasets)\n",
    "blocking_candidates = ['title'] if 'title' in common_cols else list(common_cols)[:1]\n",
    "\n",
    "if not blocking_candidates:\n",
    "    print(\"‚ö†Ô∏è  No common columns found for StandardBlocking\")\n",
    "    standard_blocking_stats = None\n",
    "else:\n",
    "    blocking_column = blocking_candidates[0]\n",
    "    print(f\"=== StandardBlocking on '{blocking_column}' ===\")\n",
    "    \n",
    "    # Initialize StandardBlocking \n",
    "    standard_blocker = StandardBlocking(\n",
    "        df_awards, \n",
    "        df_actors, \n",
    "        on=[blocking_column],  \n",
    "        batch_size=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"Estimated pairs: {standard_blocker.estimate_pairs() or 'Unknown'}\")\n",
    "    \n",
    "    # Process all batches\n",
    "    all_pairs = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in standard_blocker:\n",
    "        batch_count += 1\n",
    "        all_pairs.extend(batch.to_dict('records'))\n",
    "        \n",
    "        if batch_count >= 50:  # Limit batches for performance\n",
    "            print(f\"Stopping after {batch_count} batches...\")\n",
    "            break\n",
    "    \n",
    "    pair_count = len(all_pairs)\n",
    "    reduction_ratio = pair_count / max_pairs if max_pairs > 0 else 0\n",
    "    \n",
    "    print(f\"Generated {pair_count:,} candidate pairs in {batch_count} batches\")\n",
    "    print(f\"Reduction ratio: {reduction_ratio:.4f} ({100 * (1-reduction_ratio):.1f}% reduction)\")\n",
    "    \n",
    "    # Sample pairs for display\n",
    "    if all_pairs:\n",
    "        sample_pairs = pd.DataFrame(all_pairs[:10])\n",
    "        print(\"\\\\nSample candidate pairs:\")\n",
    "        print(sample_pairs.to_string(index=False))\n",
    "    \n",
    "    # Analyze block sizes if available\n",
    "    if hasattr(standard_blocker, '_common_keys'):\n",
    "        print(f\"\\\\nBlocking statistics:\")\n",
    "        print(f\"Number of blocks: {len(standard_blocker._common_keys)}\")\n",
    "        if standard_blocker._common_keys:\n",
    "            block_sizes = [len(standard_blocker._left_blocks[k]) * len(standard_blocker._right_blocks[k]) \n",
    "                          for k in standard_blocker._common_keys[:10]]  # Sample first 10\n",
    "            print(f\"Average block size (sample): {np.mean(block_sizes):.1f}\")\n",
    "            print(f\"Max block size (sample): {max(block_sizes)}\")\n",
    "    \n",
    "    print(f\"\\\\nüìä StandardBlocking: {pair_count:,} pairs ({reduction_ratio:.4f} ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:51,696 - PyDI.io.loaders - INFO - Loaded dataset 'gs_academy_awards_2_actors_test' via read_csv: shape=(3347, 3), source=/Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/entitymatching/splits/gs_academy_awards_2_actors_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy_awards_4529</td>\n",
       "      <td>actors_2</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academy_awards_4500</td>\n",
       "      <td>actors_3</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academy_awards_4475</td>\n",
       "      <td>actors_4</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>academy_awards_4446</td>\n",
       "      <td>actors_5</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academy_awards_4399</td>\n",
       "      <td>actors_6</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>academy_awards_3765</td>\n",
       "      <td>actors_15</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>academy_awards_1049</td>\n",
       "      <td>actors_65</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>academy_awards_1115</td>\n",
       "      <td>actors_101</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>academy_awards_3244</td>\n",
       "      <td>actors_101</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>academy_awards_2</td>\n",
       "      <td>actors_120</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3347 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id1         id2  label\n",
       "0     academy_awards_4529    actors_2   TRUE\n",
       "1     academy_awards_4500    actors_3   TRUE\n",
       "2     academy_awards_4475    actors_4   TRUE\n",
       "3     academy_awards_4446    actors_5   TRUE\n",
       "4     academy_awards_4399    actors_6   TRUE\n",
       "...                   ...         ...    ...\n",
       "3342  academy_awards_3765   actors_15  FALSE\n",
       "3343  academy_awards_1049   actors_65  FALSE\n",
       "3344  academy_awards_1115  actors_101  FALSE\n",
       "3345  academy_awards_3244  actors_101   TRUE\n",
       "3346     academy_awards_2  actors_120  FALSE\n",
       "\n",
       "[3347 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare teh gold standard\n",
    "gold = load_csv(\n",
    "    root / \"input\" / \"movies\" / \"entitymatching\" / \"splits\" / \"gs_academy_awards_2_actors_test.csv\",\n",
    "    name=\"gs_academy_awards_2_actors_test\",\n",
    "    header=None,\n",
    "    names=[\"id1\", \"id2\", \"label\"],\n",
    "    add_index=False,\n",
    "    index_col=False,\n",
    "    dtype=str,\n",
    ")\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:51,727 - root - INFO - Blocking evaluation: candidates=138 unique=137 duplicates=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardBlocking eval: {'unique_candidates': 137, 'candidate_recall': 0.723404255319149, 'pair_reduction': 0.9997997684914529}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    std_cands = standard_blocker.materialize()\n",
    "\n",
    "    # Map internal _id to original id values \n",
    "    # Careful: the _id is an id we created, not the id in the gold standard\n",
    "    left_map = df_awards.set_index('_id')['id']\n",
    "    right_map = df_actors.set_index('_id')['id']\n",
    "    std_cands['id1'] = std_cands['id1'].map(left_map)\n",
    "    std_cands['id2'] = std_cands['id2'].map(right_map)\n",
    "\n",
    "    std_eval = BlockingEvaluator.evaluate(\n",
    "        std_cands,\n",
    "        gold_pairs=gold,\n",
    "        gold_label_col=\"label\",\n",
    "        total_possible_pairs=max_pairs,\n",
    "        out_dir=str(OUTPUT_DIR / \"standardblocking\"),\n",
    "    )\n",
    "\n",
    "    blocking_stats.append({\n",
    "        'strategy': f'StandardBlocking(on=[{blocking_column}])' if 'blocking_column' in locals() else 'StandardBlocking(on=[title])',\n",
    "        'estimated_pairs': standard_blocker.estimate_pairs(),\n",
    "        'actual_pairs': len(std_cands),\n",
    "        'batches_processed': None,  \n",
    "        'candidate_recall': std_eval.get('candidate_recall'),\n",
    "        'reduction_ratio': len(std_cands) / max_pairs if max_pairs > 0 else 0,\n",
    "        'processing_time_seconds': None  \n",
    "    })\n",
    "    \n",
    "    print(\"StandardBlocking eval:\", {k: std_eval.get(k) for k in [\"unique_candidates\", \"candidate_recall\", \"pair_reduction\"]})\n",
    "except Exception as e:\n",
    "    print(\"StandardBlocking evaluation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SortedNeighbourhood Blocking\n",
    "\n",
    "SortedNeighbourhood sorting records by a key attribute and compares each record with its neighbors within a sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SortedNeighbourhood Blocking on 'title' ===\n",
      "Window size: 5\n",
      "Estimated pairs: 11852\n",
      "Generated 1,456 candidate pairs in 2 batches\n",
      "Reduction ratio: 0.0021 (99.8% reduction)\n",
      "\\nSample candidate pairs:\n",
      "                id1         id2\n",
      "academy_awards-4416 actors-0000\n",
      "academy_awards-2548 actors-0000\n",
      "academy_awards-2506 actors-0000\n",
      "academy_awards-0393 actors-0000\n",
      "academy_awards-4567 actors-0000\n",
      "academy_awards-0487 actors-0000\n",
      "academy_awards-0931 actors-0000\n",
      "academy_awards-0333 actors-0000\n",
      "academy_awards-0504 actors-0000\n",
      "academy_awards-2177 actors-0000\n",
      "\\nSample pair details:\n",
      "  1. academy_awards-4416 (42nd Street) <-> actors-0000 (7th Heaven)\n",
      "  2. academy_awards-2548 (55 Days at Peking) <-> actors-0000 (7th Heaven)\n",
      "  3. academy_awards-2506 (7 Faces of Dr. Lao) <-> actors-0000 (7th Heaven)\n",
      "\\nüìä SortedNeighbourhood: 1,456 pairs (0.0021 ratio)\n"
     ]
    }
   ],
   "source": [
    "# Use title for sorted neighbourhood if available\n",
    "if 'title' in common_cols:\n",
    "    sort_key = 'title'\n",
    "else:\n",
    "    sort_key = list(common_cols)[0] if common_cols else None\n",
    "\n",
    "if not sort_key:\n",
    "    print(\"‚ö†Ô∏è  No suitable column found for SortedNeighbourhood blocking\")\n",
    "    sn_blocking_stats = None\n",
    "else:\n",
    "    print(f\"=== SortedNeighbourhood Blocking on '{sort_key}' ===\")\n",
    "    \n",
    "    # Initialize SortedNeighbourhood with unified 'key' parameter (single column) and 'window'\n",
    "    sn_blocker = SortedNeighbourhood(\n",
    "        df_awards,\n",
    "        df_actors,\n",
    "        key=sort_key,  \n",
    "        window=5,      \n",
    "        batch_size=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"Window size: 5\")\n",
    "    print(f\"Estimated pairs: {sn_blocker.estimate_pairs() or 'Unknown'}\")\n",
    "    \n",
    "    \n",
    "    all_pairs = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in sn_blocker:  \n",
    "        batch_count += 1\n",
    "        all_pairs.extend(batch.to_dict('records'))\n",
    "        \n",
    "        if batch_count >= 50:  # Limit batches\n",
    "            print(f\"Stopping after {batch_count} batches...\")\n",
    "            break\n",
    "    \n",
    "    pair_count = len(all_pairs)\n",
    "    reduction_ratio = pair_count / max_pairs if max_pairs > 0 else 0\n",
    "    \n",
    "    print(f\"Generated {pair_count:,} candidate pairs in {batch_count} batches\")\n",
    "    print(f\"Reduction ratio: {reduction_ratio:.4f} ({100 * (1-reduction_ratio):.1f}% reduction)\")\n",
    "    \n",
    "    # Sample pairs for display\n",
    "    if all_pairs:\n",
    "        sample_pairs = pd.DataFrame(all_pairs[:10])\n",
    "        print(\"\\\\nSample candidate pairs:\")\n",
    "        print(sample_pairs.to_string(index=False))\n",
    "        \n",
    "        # Show actual values for sample pairs\n",
    "        print(\"\\\\nSample pair details:\")\n",
    "        for i, pair in enumerate(sample_pairs[:3].to_dict('records')):\n",
    "            awards_record = df_awards[df_awards['_id'] == pair['id1']]\n",
    "            actors_record = df_actors[df_actors['_id'] == pair['id2']]\n",
    "            if not awards_record.empty and not actors_record.empty:\n",
    "                awards_val = awards_record[sort_key].iloc[0] if sort_key in awards_record.columns else 'N/A'\n",
    "                actors_val = actors_record[sort_key].iloc[0] if sort_key in actors_record.columns else 'N/A'\n",
    "                print(f\"  {i+1}. {pair['id1']} ({awards_val}) <-> {pair['id2']} ({actors_val})\")\n",
    "    print(f\"\\\\nüìä SortedNeighbourhood: {pair_count:,} pairs ({reduction_ratio:.4f} ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:51,756 - root - INFO - Blocking evaluation: candidates=1456 unique=1453 duplicates=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SortedNeighbourhood eval: {'unique_candidates': 1453, 'candidate_recall': 0.9787234042553191, 'pair_reduction': 0.9978763767743143}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sn_cands = sn_blocker.materialize()\n",
    "\n",
    "    # Map internal _id to original id values \n",
    "    # Careful: the _id is an id we created, not the id in the gold standard\n",
    "    left_map = df_awards.set_index('_id')['id']\n",
    "    right_map = df_actors.set_index('_id')['id']\n",
    "    sn_cands['id1'] = sn_cands['id1'].map(left_map)\n",
    "    sn_cands['id2'] = sn_cands['id2'].map(right_map)\n",
    "\n",
    "    sn_eval = BlockingEvaluator.evaluate(\n",
    "        sn_cands,\n",
    "        gold_pairs=gold,\n",
    "        gold_label_col=\"label\",\n",
    "        total_possible_pairs=max_pairs,\n",
    "        out_dir=str(OUTPUT_DIR / \"\"),\n",
    "    )\n",
    "    blocking_stats.append({\n",
    "        'strategy': f'SortedNeighbourhood(key={sort_key}, window=5)',\n",
    "        'estimated_pairs': sn_blocker.estimate_pairs(),\n",
    "        'actual_pairs': len(sn_cands),\n",
    "        'batches_processed': batch_count,  \n",
    "        'candidate_recall': sn_eval.get('candidate_recall'),\n",
    "        'reduction_ratio': len(sn_cands) / max_pairs if max_pairs > 0 else 0,\n",
    "        'processing_time_seconds': None\n",
    "    })\n",
    "    print(\"SortedNeighbourhood eval:\", {k: sn_eval.get(k) for k in [\"unique_candidates\", \"candidate_recall\", \"pair_reduction\"]})\n",
    "except Exception as e:\n",
    "    print(\"SortedNeighbourhood evaluation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TokenBlocking\n",
    "\n",
    "TokenBlocking splits text attributes into tokens and creates blocks for each token. Records sharing any token are considered candidate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TokenBlocking on 'title' ===\n",
      "Min token length: 2\n",
      "Estimated pairs: 80850\n",
      "Stopping after 50 batches...\n",
      "Generated 50,000 unique candidate pairs in 50 batches\n",
      "Reduction ratio: 0.0731 (92.7% reduction)\n",
      "\\nüìä TokenBlocking: 50,000 pairs (0.0731 ratio)\n"
     ]
    }
   ],
   "source": [
    "# Use title for token blocking if available\n",
    "if 'title' in common_cols:\n",
    "    token_key = 'title'\n",
    "else:\n",
    "    # Find a text column for token blocking\n",
    "    text_cols = [col for col in common_cols \n",
    "                 if df_awards[col].dtype == 'object' or df_actors[col].dtype == 'object']\n",
    "    token_key = text_cols[0] if text_cols else None\n",
    "\n",
    "if not token_key:\n",
    "    print(\"‚ö†Ô∏è  No suitable text column found for TokenBlocking\")\n",
    "    token_blocking_stats = None\n",
    "else:\n",
    "    print(f\"=== TokenBlocking on '{token_key}' ===\")\n",
    "    \n",
    "   \n",
    "    token_blocker = TokenBlocking(\n",
    "        df_awards,\n",
    "        df_actors,\n",
    "        column=token_key,     \n",
    "        min_token_len=2,      \n",
    "        batch_size=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"Min token length: 2\")\n",
    "    print(f\"Estimated pairs: {token_blocker.estimate_pairs() or 'Unknown'}\")\n",
    "    \n",
    "    # Process all batches\n",
    "    all_pairs = []\n",
    "    batch_count = 0\n",
    "    unique_pairs = set()\n",
    "    \n",
    "    for batch in token_blocker:\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Deduplicate pairs (TokenBlocking can generate duplicates)\n",
    "        for _, row in batch.iterrows():\n",
    "            pair_key = (row['id1'], row['id2'])\n",
    "            if pair_key not in unique_pairs:\n",
    "                unique_pairs.add(pair_key)\n",
    "                all_pairs.append(row.to_dict())\n",
    "        \n",
    "        if batch_count >= 50:  # Limit batches\n",
    "            print(f\"Stopping after {batch_count} batches...\")\n",
    "            break\n",
    "    \n",
    "    pair_count = len(all_pairs)\n",
    "    reduction_ratio = pair_count / max_pairs if max_pairs > 0 else 0\n",
    "    \n",
    "    print(f\"Generated {pair_count:,} unique candidate pairs in {batch_count} batches\")\n",
    "    print(f\"Reduction ratio: {reduction_ratio:.4f} ({100 * (1-reduction_ratio):.1f}% reduction)\")\n",
    "    \n",
    "    print(f\"\\\\nüìä TokenBlocking: {pair_count:,} pairs ({reduction_ratio:.4f} ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:52,490 - root - INFO - Blocking evaluation: candidates=75242 unique=75126 duplicates=116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenBlocking eval: {'unique_candidates': 75126, 'candidate_recall': 1.0, 'pair_reduction': 0.8902000561232841}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TokenBlocking\n",
    "try:\n",
    "    token_cands = token_blocker.materialize()\n",
    "    \n",
    "    token_cands['id1'] = token_cands['id1'].map(left_map)\n",
    "    token_cands['id2'] = token_cands['id2'].map(right_map)\n",
    "\n",
    "    token_eval = BlockingEvaluator.evaluate(\n",
    "        token_cands,\n",
    "        gold_pairs=gold,\n",
    "        gold_label_col=\"label\",\n",
    "        total_possible_pairs=max_pairs,\n",
    "        out_dir=str(OUTPUT_DIR / \"tokenblocking\"),\n",
    "    )\n",
    "    blocking_stats.append({\n",
    "        'strategy': f\"TokenBlocking(column={token_key})\",\n",
    "        'estimated_pairs': token_blocker.estimate_pairs(),\n",
    "        'actual_pairs': len(token_cands),\n",
    "        'batches_processed': batch_count,\n",
    "        'candidate_recall': token_eval.get('candidate_recall'),\n",
    "        'reduction_ratio': len(token_cands) / max_pairs if max_pairs > 0 else 0,\n",
    "        'processing_time_seconds': None\n",
    "    })\n",
    "    print(\"TokenBlocking eval:\", {k: token_eval.get(k) for k in [\"unique_candidates\", \"candidate_recall\", \"pair_reduction\"]})\n",
    "except Exception as e:\n",
    "    print(\"TokenBlocking evaluation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:52,502 - PyDI.entitymatching.blocking.embedding - INFO - Initialized EmbeddingBlocking with sklearn backend, top_k=20, threshold=0.5\n",
      "2025-08-29 11:48:52,507 - PyDI.entitymatching.blocking.embedding - INFO - Computing embeddings for datasets...\n",
      "2025-08-29 11:48:52,508 - PyDI.entitymatching.blocking.embedding - INFO - Computing embeddings for left dataset (4592 records)\n",
      "2025-08-29 11:48:52,508 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EmbeddingBlocking on ['title', 'actor_name'] ===\n",
      "Text columns: ['title', 'actor_name']\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Index backend: sklearn\n",
      "Top-k neighbors: 20\n",
      "Similarity threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:54,057 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n",
      "2025-08-29 11:48:54,368 - PyDI.entitymatching.blocking.embedding - INFO - Loaded sentence transformer model: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-08-29 11:48:55,742 - PyDI.entitymatching.blocking.embedding - INFO - Computing embeddings for right dataset (149 records)\n",
      "2025-08-29 11:48:55,784 - PyDI.entitymatching.blocking.embedding - INFO - Built sklearn index with 149 vectors, metric=cosine\n",
      "2025-08-29 11:48:55,839 - PyDI.entitymatching.blocking.embedding - INFO - Estimated 743 candidate pairs from 1000 samples\n",
      "2025-08-29 11:48:55,854 - PyDI.entitymatching.blocking.embedding - INFO - Starting embedding-based blocking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated pairs: 743...\n",
      "\n",
      "Generating candidate pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:57,432 - PyDI.entitymatching.blocking.embedding - INFO - Completed embedding-based blocking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 762 candidate pairs in 1 batches\n",
      "Processing time: 1.60 seconds\n",
      "Reduction ratio: 0.0011 (99.9% reduction)\n",
      "\n",
      "Sample candidate pairs:\n",
      "                id1         id2\n",
      "academy_awards-0001 actors-0119\n",
      "academy_awards-0002 actors-0119\n",
      "academy_awards-0010 actors-0075\n",
      "academy_awards-0011 actors-0028\n",
      "academy_awards-0011 actors-0077\n",
      "academy_awards-0012 actors-0067\n",
      "academy_awards-0017 actors-0047\n",
      "academy_awards-0044 actors-0095\n",
      "academy_awards-0071 actors-0055\n",
      "academy_awards-0082 actors-0124\n",
      "\n",
      "Sample pair details (showing combined text):\n",
      "  1. Awards: 'True Grit Jeff Bridges'\n",
      "     Actors:  'True Grit John Wayne'\n",
      "  2. Awards: 'True Grit Jeff Bridges'\n",
      "     Actors:  'True Grit John Wayne'\n",
      "  3. Awards: 'Rabbit Hole Nicole Kidman'\n",
      "     Actors:  'The Hours Nicole Kidman'\n",
      "\n",
      "üìä EmbeddingBlocking: 762 pairs (0.0011 ratio) in 1.60s\n"
     ]
    }
   ],
   "source": [
    "# Use title and actor_name for embedding-based blocking\n",
    "text_columns = ['title', 'actor_name']\n",
    "\n",
    "print(f\"=== EmbeddingBlocking on {text_columns} ===\")\n",
    "\n",
    "# Initialize EmbeddingBlocking with smaller parameters for demo\n",
    "embedding_blocker = EmbeddingBlocking(\n",
    "    df_awards,\n",
    "    df_actors,\n",
    "    text_cols=text_columns,\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    index_backend=\"sklearn\",\n",
    "    metric=\"cosine\",\n",
    "    top_k=20,           \n",
    "    threshold=0.5,      \n",
    "    normalize=True,\n",
    "    batch_size=1000,\n",
    "    query_batch_size=100  \n",
    ")\n",
    "\n",
    "print(f\"Text columns: {text_columns}\")\n",
    "print(f\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Index backend: sklearn\")\n",
    "print(f\"Top-k neighbors: 20\")\n",
    "print(f\"Similarity threshold: {embedding_blocker.threshold}\")\n",
    "print(f\"Estimated pairs: {embedding_blocker.estimate_pairs() or 'Computing...'}...\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_pairs = []\n",
    "batch_count = 0\n",
    "\n",
    "print(\"\\nGenerating candidate pairs...\")\n",
    "for batch in embedding_blocker:\n",
    "    batch_count += 1\n",
    "    all_pairs.extend(batch.to_dict('records'))\n",
    "    \n",
    "    if batch_count >= 20:  # Limit batches for demo\n",
    "        print(f\"Stopping after {batch_count} batches for demo...\")\n",
    "        break\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "pair_count = len(all_pairs)\n",
    "reduction_ratio = pair_count / max_pairs if max_pairs > 0 else 0\n",
    "\n",
    "print(f\"\\nGenerated {pair_count:,} candidate pairs in {batch_count} batches\")\n",
    "print(f\"Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"Reduction ratio: {reduction_ratio:.4f} ({100 * (1-reduction_ratio):.1f}% reduction)\")\n",
    "\n",
    "# Sample pairs for display\n",
    "if all_pairs:\n",
    "    sample_pairs = pd.DataFrame(all_pairs[:10])\n",
    "    print(\"\\nSample candidate pairs:\")\n",
    "    print(sample_pairs.to_string(index=False))\n",
    "    \n",
    "    # Show actual text values for sample pairs\n",
    "    print(\"\\nSample pair details (showing combined text):\")\n",
    "    for i, pair in enumerate(sample_pairs[:3].to_dict('records')):\n",
    "        awards_record = df_awards[df_awards['_id'] == pair['id1']]\n",
    "        actors_record = df_actors[df_actors['_id'] == pair['id2']]\n",
    "        if not awards_record.empty and not actors_record.empty:\n",
    "            # Combine text columns\n",
    "            awards_text = ' '.join([\n",
    "                str(awards_record[col].iloc[0]) if col in awards_record.columns and pd.notna(awards_record[col].iloc[0]) \n",
    "                else '' for col in text_columns\n",
    "            ]).strip()\n",
    "            actors_text = ' '.join([\n",
    "                str(actors_record[col].iloc[0]) if col in actors_record.columns and pd.notna(actors_record[col].iloc[0]) \n",
    "                else '' for col in text_columns\n",
    "            ]).strip()\n",
    "            print(f\"  {i+1}. Awards: '{awards_text}'\")\n",
    "            print(f\"     Actors:  '{actors_text}'\")\n",
    "\n",
    "print(f\"\\nüìä EmbeddingBlocking: {pair_count:,} pairs ({reduction_ratio:.4f} ratio) in {processing_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:48:57,459 - PyDI.entitymatching.blocking.embedding - INFO - Starting embedding-based blocking...\n",
      "2025-08-29 11:48:59,084 - PyDI.entitymatching.blocking.embedding - INFO - Completed embedding-based blocking\n",
      "2025-08-29 11:48:59,096 - root - INFO - Blocking evaluation: candidates=762 unique=761 duplicates=1\n",
      "2025-08-29 11:48:59,135 - PyDI.entitymatching.blocking.embedding - INFO - Estimated 794 candidate pairs from 1000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingBlocking eval: {'unique_candidates': 761, 'candidate_recall': 1.0, 'pair_reduction': 0.9988877651240559}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate EmbeddingBlocking\n",
    "try:\n",
    "    embedding_cands = embedding_blocker.materialize()\n",
    "    \n",
    "    # Map internal _id to original id values \n",
    "    embedding_cands['id1'] = embedding_cands['id1'].map(left_map)\n",
    "    embedding_cands['id2'] = embedding_cands['id2'].map(right_map)\n",
    "\n",
    "    embedding_eval = BlockingEvaluator.evaluate(\n",
    "        embedding_cands,\n",
    "        gold_pairs=gold,\n",
    "        gold_label_col=\"label\",\n",
    "        total_possible_pairs=max_pairs,\n",
    "        out_dir=str(OUTPUT_DIR / \"embeddingblocking\"),\n",
    "    )\n",
    "    \n",
    "    # Store embedding blocking stats for comparison\n",
    "    embedding_blocking_stats = {\n",
    "        'strategy': f\"EmbeddingBlocking(text_cols={text_columns})\",\n",
    "        'estimated_pairs': embedding_blocker.estimate_pairs() if hasattr(embedding_blocker, \"estimate_pairs\") else None,\n",
    "        'actual_pairs': len(embedding_cands),\n",
    "        'batches_processed': batch_count,\n",
    "        'candidate_recall': embedding_eval.get('candidate_recall'),\n",
    "        'reduction_ratio': len(embedding_cands) / max_pairs if max_pairs > 0 else 0,\n",
    "        'processing_time_seconds': processing_time if 'processing_time' in locals() else None,\n",
    "    }\n",
    "    blocking_stats.append(embedding_blocking_stats)\n",
    "    print(\"EmbeddingBlocking eval:\", {k: embedding_eval.get(k) for k in [\"unique_candidates\", \"candidate_recall\", \"pair_reduction\"]})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"EmbeddingBlocking evaluation failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Comparison with EmbeddingBlocking\n",
    "\n",
    "Let's update our comparison to include the new EmbeddingBlocking results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>estimated_pairs</th>\n",
       "      <th>actual_pairs</th>\n",
       "      <th>batches_processed</th>\n",
       "      <th>candidate_recall</th>\n",
       "      <th>reduction_ratio</th>\n",
       "      <th>processing_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TokenBlocking(column=title)</td>\n",
       "      <td>80850</td>\n",
       "      <td>75242</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109969</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EmbeddingBlocking(text_cols=['title', 'actor_n...</td>\n",
       "      <td>794</td>\n",
       "      <td>762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>1.599513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SortedNeighbourhood(key=title, window=5)</td>\n",
       "      <td>11852</td>\n",
       "      <td>1456</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StandardBlocking(on=[title])</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            strategy  estimated_pairs  \\\n",
       "2                        TokenBlocking(column=title)            80850   \n",
       "3  EmbeddingBlocking(text_cols=['title', 'actor_n...              794   \n",
       "1           SortedNeighbourhood(key=title, window=5)            11852   \n",
       "0                       StandardBlocking(on=[title])              138   \n",
       "\n",
       "   actual_pairs  batches_processed  candidate_recall  reduction_ratio  \\\n",
       "2         75242               50.0          1.000000         0.109969   \n",
       "3           762                1.0          1.000000         0.001114   \n",
       "1          1456                2.0          0.978723         0.002128   \n",
       "0           138                NaN          0.723404         0.000202   \n",
       "\n",
       "   processing_time_seconds  \n",
       "2                      NaN  \n",
       "3                 1.599513  \n",
       "1                      NaN  \n",
       "0                      NaN  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(blocking_stats).sort_values(\"candidate_recall\", ascending=False)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Artifact Generation\n",
    "\n",
    "Save all results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Final detailed results saved to: /Users/aaronsteiner/Documents/GitHub/PyDI/output/examples/entitymatching/blocking_demo/blocking_comparison_final.json\n",
      "\n",
      "‚úÖ All final artifacts saved to: /Users/aaronsteiner/Documents/GitHub/PyDI/output/examples/entitymatching/blocking_demo\n",
      "\n",
      "=== Final Summary ===\n",
      "Datasets: Academy Awards (4592 records) √ó Actors (149 records)\n",
      "Blocking strategies tested: 4\n",
      "Maximum possible pairs: 684,208\n",
      "Best reduction achieved: 100.0%\n",
      "Best recall achieved: 100.0%\n",
      "\n",
      "üéØ Next steps: Use these candidate pairs for entity matching with similarity functions!\n",
      "üß† EmbeddingBlocking provides excellent semantic matching capabilities for text-heavy datasets!\n"
     ]
    }
   ],
   "source": [
    "results_updated = {\n",
    "    'metadata': {\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'datasets': {\n",
    "            'academy_awards': {\n",
    "                'path': str(ACADEMY_AWARDS_PATH),\n",
    "                'shape': df_awards.shape,\n",
    "                'columns': list(df_awards.columns)\n",
    "            },\n",
    "            'actors': {\n",
    "                'path': str(ACTORS_PATH),\n",
    "                'shape': df_actors.shape,\n",
    "                'columns': list(df_actors.columns)\n",
    "            }\n",
    "        },\n",
    "        'max_possible_pairs': max_pairs,\n",
    "        'common_columns': list(common_cols)\n",
    "    },\n",
    "    'blocking_results': blocking_stats \n",
    "}\n",
    "\n",
    "# Save final results as JSON\n",
    "results_path_final = OUTPUT_DIR / 'blocking_comparison_final.json'\n",
    "with open(results_path_final, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_updated, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "print(f\"\\nüìÅ Final detailed results saved to: {results_path_final}\")\n",
    "\n",
    "# Save final comparison DataFrame as CSV\n",
    "if 'comparison_df_updated' in locals():\n",
    "    csv_path_final = OUTPUT_DIR / 'blocking_comparison_final.csv'\n",
    "    comparison_df.to_csv(csv_path_final, index=False, encoding='utf-8')\n",
    "    print(f\"üìä Final comparison table saved to: {csv_path_final}\")\n",
    "\n",
    "# Updated summary with EmbeddingBlocking\n",
    "print(f\"\\n‚úÖ All final artifacts saved to: {OUTPUT_DIR.absolute()}\")\n",
    "\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "print(f\"Datasets: Academy Awards ({len(df_awards)} records) √ó Actors ({len(df_actors)} records)\")\n",
    "\n",
    "print(f\"Blocking strategies tested: {len(blocking_stats)}\")\n",
    "print(f\"Maximum possible pairs: {max_pairs:,}\")\n",
    "\n",
    "# Find best performing strategies\n",
    "if blocking_stats and len(blocking_stats) > 1:\n",
    "    blocking_final_stats = [s for s in blocking_stats if s['strategy'] != 'NoBlocking' and s.get('reduction_ratio')]\n",
    "    if blocking_final_stats:\n",
    "        best_reduction = max((1-s['reduction_ratio']) for s in blocking_final_stats if s.get('reduction_ratio'))\n",
    "        print(f\"Best reduction achieved: {best_reduction*100:.1f}%\")\n",
    "        \n",
    "        # Find best recall\n",
    "        recall_final_stats = [s for s in blocking_stats if s.get('candidate_recall') is not None]\n",
    "        if recall_final_stats:\n",
    "            best_recall = max(s['candidate_recall'] for s in recall_final_stats)\n",
    "            print(f\"Best recall achieved: {best_recall*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüéØ Next steps: Use these candidate pairs for entity matching with similarity functions!\")\n",
    "print(\"üß† EmbeddingBlocking provides excellent semantic matching capabilities for text-heavy datasets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
