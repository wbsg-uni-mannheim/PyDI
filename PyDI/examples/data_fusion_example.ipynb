{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDI Data Fusion Framework Showcase\n",
    "\n",
    "This notebook demonstrates the data fusion capabilities of PyDI. We'll show:\n",
    "\n",
    "1. **Loading and preparing Winter movie datasets**\n",
    "2. **Creating sophisticated fusion strategies**\n",
    "3. **Running the fusion engine with connected components grouping**\n",
    "4. **Evaluating fusion quality**\n",
    "5. **Generating reports**\n",
    "6. **Custom conflict resolution rules**\n",
    "7. **Provenance tracking**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyDI fusion components imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import PyDI fusion components\n",
    "from PyDI.fusion import (\n",
    "    # Core engine and strategy\n",
    "    DataFusionEngine,\n",
    "    DataFusionStrategy,\n",
    "    DataFusionEvaluator,\n",
    "    \n",
    "    # Conflict resolution rules\n",
    "    LongestString,\n",
    "    ShortestString,\n",
    "    Average,\n",
    "    Median,\n",
    "    Maximum,\n",
    "    MostRecent,\n",
    "    Earliest,\n",
    "    Union,\n",
    "    Intersection,\n",
    "    Voting,\n",
    "    FavourSources,\n",
    "    \n",
    "    # Reporting and evaluation\n",
    "    FusionReport,\n",
    "    FusionQualityMetrics,\n",
    "    ProvenanceTracker,\n",
    "    \n",
    "    # Base classes for custom rules\n",
    "    ConflictResolutionFunction,\n",
    "    AttributeValueFuser,\n",
    "    FusionResult,\n",
    "    \n",
    "    # Convenience functions\n",
    "    create_simple_strategy,\n",
    "    build_record_groups_from_correspondences,\n",
    ")\n",
    "\n",
    "# Import entity matching components\n",
    "from PyDI.entitymatching.base import ensure_record_ids\n",
    "\n",
    "print(\"‚úÖ PyDI fusion components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Winter Movie Datasets\n",
    "\n",
    "We'll load the movie datasets from Winter's XML files and convert them to pandas DataFrames suitable for fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_winter_xml(xml_path, dataset_name):\n",
    "    \"\"\"Parse Winter XML format and convert to pandas DataFrame.\"\"\"\n",
    "    \n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    movies = []\n",
    "    \n",
    "    for movie in root.findall('.//movie'):\n",
    "        movie_data = {}\n",
    "        \n",
    "        # Basic movie information\n",
    "        movie_data['_id'] = movie.find('id').text if movie.find('id') is not None else None\n",
    "        movie_data['title'] = movie.find('title').text if movie.find('title') is not None else None\n",
    "        movie_data['date'] = movie.find('date').text if movie.find('date') is not None else None\n",
    "        \n",
    "        # Handle directors\n",
    "        directors = movie.find('directors')\n",
    "        if directors is not None:\n",
    "            director_names = [d.find('name').text for d in directors.findall('director') \n",
    "                            if d.find('name') is not None]\n",
    "            movie_data['director'] = ', '.join(director_names) if director_names else None\n",
    "        \n",
    "        # Handle actors\n",
    "        actors = movie.find('actors')\n",
    "        if actors is not None:\n",
    "            actor_names = [a.find('name').text for a in actors.findall('actor') \n",
    "                          if a.find('name') is not None]\n",
    "            movie_data['actors'] = ', '.join(actor_names) if actor_names else None\n",
    "        \n",
    "        # Handle studios\n",
    "        studios = movie.find('studios')\n",
    "        if studios is not None:\n",
    "            studio_names = [s.find('name').text for s in studios.findall('studio') \n",
    "                           if s.find('name') is not None]\n",
    "            movie_data['studio'] = ', '.join(studio_names) if studio_names else None\n",
    "        \n",
    "        movies.append(movie_data)\n",
    "    \n",
    "    df = pd.DataFrame(movies)\n",
    "    \n",
    "    # Add dataset metadata\n",
    "    df.attrs['dataset_name'] = dataset_name\n",
    "    df.attrs['source_format'] = 'winter_xml'\n",
    "    df.attrs['loaded_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define paths to Winter datasets\n",
    "data_dir = Path('/Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/fusion/data')\n",
    "correspondences_dir = Path('/Users/aaronsteiner/Documents/GitHub/PyDI/input/movies/fusion/correspondences')\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Correspondences directory: {correspondences_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three movie datasets\n",
    "academy_awards_df = parse_winter_xml(data_dir / 'academy_awards.xml', 'academy_awards')\n",
    "actors_df = parse_winter_xml(data_dir / 'actors.xml', 'actors')\n",
    "golden_globes_df = parse_winter_xml(data_dir / 'golden_globes.xml', 'golden_globes')\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Academy Awards: {len(academy_awards_df)} movies\")\n",
    "print(f\"Actors: {len(actors_df)} movies\")\n",
    "print(f\"Golden Globes: {len(golden_globes_df)} movies\")\n",
    "print(f\"\\nTotal records: {len(academy_awards_df) + len(actors_df) + len(golden_globes_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data from each dataset\n",
    "print(\"üé¨ Academy Awards Sample:\")\n",
    "display(academy_awards_df[['_id', 'title', 'director', 'date']].head())\n",
    "\n",
    "print(\"\\nüé≠ Actors Sample:\")\n",
    "display(actors_df[['_id', 'title', 'actors', 'date']].head())\n",
    "\n",
    "print(\"\\nüèÜ Golden Globes Sample:\")\n",
    "display(golden_globes_df[['_id', 'title', 'director', 'date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load correspondences (matches between datasets)\n",
    "def load_correspondences(corr_path):\n",
    "    \"\"\"Load correspondences from Winter CSV format.\"\"\"\n",
    "    df = pd.read_csv(corr_path, header=None, names=['id1', 'id2', 'score'])\n",
    "    return df\n",
    "\n",
    "# Load all correspondence files\n",
    "corr_aa_actors = load_correspondences(correspondences_dir / 'academy_awards_2_actors_correspondences.csv')\n",
    "corr_actors_gg = load_correspondences(correspondences_dir / 'actors_2_golden_globes_correspondences.csv')\n",
    "\n",
    "print(\"üîó Correspondences Overview:\")\n",
    "print(f\"Academy Awards ‚Üî Actors: {len(corr_aa_actors)} matches\")\n",
    "print(f\"Actors ‚Üî Golden Globes: {len(corr_actors_gg)} matches\")\n",
    "\n",
    "# Combine correspondences for fusion\n",
    "all_correspondences = pd.concat([corr_aa_actors, corr_actors_gg], ignore_index=True)\n",
    "print(f\"\\nTotal correspondences: {len(all_correspondences)}\")\n",
    "\n",
    "display(all_correspondences.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Data Quality and Overlap\n",
    "\n",
    "Before fusion, let's analyze the data quality and understand what conflicts we might encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attribute coverage across datasets\n",
    "datasets = [academy_awards_df, actors_df, golden_globes_df]\n",
    "dataset_names = ['Academy Awards', 'Actors', 'Golden Globes']\n",
    "\n",
    "print(\"üìä Attribute Coverage Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_attributes = set()\n",
    "for df in datasets:\n",
    "    all_attributes.update(df.columns)\n",
    "\n",
    "coverage_df = pd.DataFrame(index=sorted(all_attributes), columns=dataset_names)\n",
    "\n",
    "for i, df in enumerate(datasets):\n",
    "    for attr in all_attributes:\n",
    "        if attr in df.columns:\n",
    "            non_null_count = df[attr].count()\n",
    "            total_count = len(df)\n",
    "            coverage = f\"{non_null_count}/{total_count} ({non_null_count/total_count:.1%})\"\n",
    "        else:\n",
    "            coverage = \"0/0 (0%)\"\n",
    "        coverage_df.iloc[coverage_df.index.get_loc(attr), i] = coverage\n",
    "\n",
    "display(coverage_df.fillna(\"0/0 (0%)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze potential conflicts by examining matched records\n",
    "def analyze_conflicts_preview(datasets, correspondences, sample_size=5):\n",
    "    \"\"\"Preview potential conflicts in matched records.\"\"\"\n",
    "    \n",
    "    # Build lookup tables\n",
    "    id_to_record = {}\n",
    "    id_to_dataset = {}\n",
    "    \n",
    "    for df in datasets:\n",
    "        dataset_name = df.attrs['dataset_name']\n",
    "        for _, record in df.iterrows():\n",
    "            record_id = record['_id']\n",
    "            id_to_record[record_id] = record\n",
    "            id_to_dataset[record_id] = dataset_name\n",
    "    \n",
    "    # Sample some correspondences to show conflicts\n",
    "    sample_corr = correspondences.head(sample_size)\n",
    "    \n",
    "    print(f\"üîç Conflict Analysis Preview (First {sample_size} matches):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (_, corr) in enumerate(sample_corr.iterrows(), 1):\n",
    "        id1, id2, score = corr['id1'], corr['id2'], corr['score']\n",
    "        \n",
    "        record1 = id_to_record.get(id1)\n",
    "        record2 = id_to_record.get(id2)\n",
    "        \n",
    "        if record1 is None or record2 is None:\n",
    "            continue\n",
    "            \n",
    "        dataset1 = id_to_dataset[id1]\n",
    "        dataset2 = id_to_dataset[id2]\n",
    "        \n",
    "        print(f\"\\nMatch {i}: {dataset1} ‚Üî {dataset2} (score: {score})\")\n",
    "        print(f\"  {dataset1}: {record1.get('title', 'N/A')} | {record1.get('director', 'N/A')} | {record1.get('date', 'N/A')}\")\n",
    "        print(f\"  {dataset2}: {record2.get('title', 'N/A')} | {record2.get('director', 'N/A')} | {record2.get('date', 'N/A')}\")\n",
    "        \n",
    "        # Check for conflicts\n",
    "        conflicts = []\n",
    "        common_attrs = set(record1.index) & set(record2.index)\n",
    "        for attr in ['title', 'director', 'date']:\n",
    "            if attr in common_attrs:\n",
    "                val1, val2 = record1.get(attr), record2.get(attr)\n",
    "                if pd.notna(val1) and pd.notna(val2) and str(val1) != str(val2):\n",
    "                    conflicts.append(f\"{attr}: '{val1}' vs '{val2}'\")\n",
    "        \n",
    "        if conflicts:\n",
    "            print(f\"  ‚ö†Ô∏è  Conflicts: {'; '.join(conflicts)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ No obvious conflicts\")\n",
    "\n",
    "analyze_conflicts_preview(datasets, all_correspondences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Sophisticated Fusion Strategy\n",
    "\n",
    "Now we'll create a comprehensive fusion strategy that handles different types of conflicts intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom conflict resolution function for movie titles\n",
    "class SmartMovieTitle(ConflictResolutionFunction):\n",
    "    \"\"\"Intelligent movie title fusion that handles common variations.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"smart_movie_title\"\n",
    "    \n",
    "    def resolve(self, values, context):\n",
    "        if not values:\n",
    "            return FusionResult(value=None, confidence=0.0, rule_used=self.name)\n",
    "        \n",
    "        # Remove duplicates and None values\n",
    "        clean_values = [str(v).strip() for v in values if pd.notna(v) and str(v).strip()]\n",
    "        \n",
    "        if not clean_values:\n",
    "            return FusionResult(value=None, confidence=0.0, rule_used=self.name)\n",
    "        \n",
    "        # If all values are the same, high confidence\n",
    "        if len(set(clean_values)) == 1:\n",
    "            return FusionResult(\n",
    "                value=clean_values[0],\n",
    "                confidence=1.0,\n",
    "                rule_used=self.name,\n",
    "                metadata={\"reason\": \"unanimous\"}\n",
    "            )\n",
    "        \n",
    "        # Choose the longest title (often more complete)\n",
    "        longest_title = max(clean_values, key=len)\n",
    "        \n",
    "        # Check if shorter titles are substrings of longer ones\n",
    "        is_superset = all(short_title.lower() in longest_title.lower() \n",
    "                         for short_title in clean_values)\n",
    "        \n",
    "        confidence = 0.9 if is_superset else 0.7\n",
    "        \n",
    "        return FusionResult(\n",
    "            value=longest_title,\n",
    "            confidence=confidence,\n",
    "            rule_used=self.name,\n",
    "            metadata={\n",
    "                \"candidates\": clean_values,\n",
    "                \"is_superset\": is_superset\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create a custom date fusion rule\n",
    "class SmartDateFusion(ConflictResolutionFunction):\n",
    "    \"\"\"Smart date fusion that handles different date formats and chooses most reliable.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"smart_date\"\n",
    "    \n",
    "    def resolve(self, values, context):\n",
    "        if not values:\n",
    "            return FusionResult(value=None, confidence=0.0, rule_used=self.name)\n",
    "        \n",
    "        clean_values = [v for v in values if pd.notna(v)]\n",
    "        if not clean_values:\n",
    "            return FusionResult(value=None, confidence=0.0, rule_used=self.name)\n",
    "        \n",
    "        # Parse dates and score them by precision\n",
    "        parsed_dates = []\n",
    "        for val in clean_values:\n",
    "            date_str = str(val)\n",
    "            precision_score = 0\n",
    "            \n",
    "            # Score based on precision (more specific dates get higher scores)\n",
    "            if len(date_str) >= 10:  # Full date YYYY-MM-DD\n",
    "                precision_score = 3\n",
    "            elif len(date_str) >= 7:   # Year-Month YYYY-MM\n",
    "                precision_score = 2\n",
    "            elif len(date_str) >= 4:   # Just year YYYY\n",
    "                precision_score = 1\n",
    "            \n",
    "            parsed_dates.append((val, precision_score))\n",
    "        \n",
    "        # Choose date with highest precision\n",
    "        best_date = max(parsed_dates, key=lambda x: x[1])\n",
    "        \n",
    "        # Calculate confidence\n",
    "        max_score = best_date[1]\n",
    "        confidence = min(1.0, 0.5 + max_score * 0.15)\n",
    "        \n",
    "        return FusionResult(\n",
    "            value=best_date[0],\n",
    "            confidence=confidence,\n",
    "            rule_used=self.name,\n",
    "            metadata={\n",
    "                \"precision_score\": max_score,\n",
    "                \"candidates\": clean_values\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Custom conflict resolution functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive fusion strategy\n",
    "movie_strategy = DataFusionStrategy(\"comprehensive_movie_fusion\")\n",
    "\n",
    "# Configure fusion rules for each attribute\n",
    "movie_strategy.add_attribute_fuser_from_resolver(\"title\", SmartMovieTitle())\n",
    "movie_strategy.add_attribute_fuser_from_resolver(\"director\", LongestString())  # Longer names often more complete\n",
    "movie_strategy.add_attribute_fuser_from_resolver(\"actors\", Union())           # Combine all actors\n",
    "movie_strategy.add_attribute_fuser_from_resolver(\"studio\", Union())           # Combine all studios\n",
    "movie_strategy.add_attribute_fuser_from_resolver(\"date\", SmartDateFusion())   # Smart date handling\n",
    "\n",
    "print(\"üéØ Fusion Strategy Configuration:\")\n",
    "print(f\"Strategy name: {movie_strategy.name}\")\n",
    "print(f\"Registered attributes: {list(movie_strategy.get_registered_attributes())}\")\n",
    "\n",
    "# Show the rules for each attribute\n",
    "for attr in movie_strategy.get_registered_attributes():\n",
    "    fuser = movie_strategy.get_attribute_fuser(attr)\n",
    "    print(f\"  {attr}: {fuser.resolver.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Fusion Engine\n",
    "\n",
    "Now we'll execute the fusion process using the DataFusionEngine with connected components grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the fusion engine\n",
    "fusion_engine = DataFusionEngine(movie_strategy)\n",
    "\n",
    "print(\"üöÄ Starting Data Fusion Process...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Run fusion with all datasets and correspondences\n",
    "fused_movies = fusion_engine.run(\n",
    "    datasets=[academy_awards_df, actors_df, golden_globes_df],\n",
    "    correspondences=all_correspondences\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Fusion Complete!\")\n",
    "print(f\"Input records: {len(academy_awards_df) + len(actors_df) + len(golden_globes_df)}\")\n",
    "print(f\"Output records: {len(fused_movies)}\")\n",
    "print(f\"Compression ratio: {len(fused_movies) / (len(academy_awards_df) + len(actors_df) + len(golden_globes_df)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the fusion results\n",
    "print(\"üé¨ Fusion Results Overview:\")\n",
    "print(f\"Fused dataset shape: {fused_movies.shape}\")\n",
    "print(f\"Columns: {list(fused_movies.columns)}\")\n",
    "\n",
    "# Show sample fused records\n",
    "print(\"\\nüìã Sample Fused Records:\")\n",
    "display_cols = ['_id', 'title', 'director', 'date', '_fusion_confidence', '_fusion_sources']\n",
    "available_cols = [col for col in display_cols if col in fused_movies.columns]\n",
    "display(fused_movies[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fusion confidence and sources\n",
    "print(\"üìä Fusion Quality Analysis:\")\n",
    "\n",
    "if '_fusion_confidence' in fused_movies.columns:\n",
    "    confidence_stats = fused_movies['_fusion_confidence'].describe()\n",
    "    print(\"\\nConfidence Statistics:\")\n",
    "    print(confidence_stats)\n",
    "    \n",
    "    # Show confidence distribution\n",
    "    print(\"\\nConfidence Distribution:\")\n",
    "    confidence_bins = pd.cut(fused_movies['_fusion_confidence'], \n",
    "                           bins=[0, 0.5, 0.7, 0.9, 1.0], \n",
    "                           labels=['Low (0-0.5)', 'Medium (0.5-0.7)', 'High (0.7-0.9)', 'Very High (0.9-1.0)'])\n",
    "    print(confidence_bins.value_counts())\n",
    "\n",
    "if '_fusion_sources' in fused_movies.columns:\n",
    "    # Analyze source combinations\n",
    "    source_counts = fused_movies['_fusion_sources'].apply(len)\n",
    "    print(\"\\nSource Count Distribution:\")\n",
    "    print(source_counts.value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nMulti-source records: {(source_counts > 1).sum()}\")\n",
    "    print(f\"Single-source records: {(source_counts == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Analysis of High-Quality Matches\n",
    "\n",
    "Let's examine some of the best fusion results to see how our strategy performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and display high-confidence multi-source fusions\n",
    "if '_fusion_confidence' in fused_movies.columns and '_fusion_sources' in fused_movies.columns:\n",
    "    high_quality_fusions = fused_movies[\n",
    "        (fused_movies['_fusion_confidence'] > 0.8) & \n",
    "        (fused_movies['_fusion_sources'].apply(len) > 1)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"üåü High-Quality Multi-Source Fusions ({len(high_quality_fusions)} records):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (_, record) in enumerate(high_quality_fusions.head(5).iterrows(), 1):\n",
    "        print(f\"\\n{i}. {record.get('title', 'N/A')}\")\n",
    "        print(f\"   Director: {record.get('director', 'N/A')}\")\n",
    "        print(f\"   Date: {record.get('date', 'N/A')}\")\n",
    "        print(f\"   Confidence: {record.get('_fusion_confidence', 0):.3f}\")\n",
    "        print(f\"   Sources: {', '.join(record.get('_fusion_sources', []))}\")\n",
    "        \n",
    "        # Show actors and studios if available\n",
    "        if pd.notna(record.get('actors')):\n",
    "            actors = str(record.get('actors'))\n",
    "            if len(actors) > 100:\n",
    "                actors = actors[:100] + \"...\"\n",
    "            print(f\"   Actors: {actors}\")\n",
    "        \n",
    "        if pd.notna(record.get('studio')):\n",
    "            print(f\"   Studio: {record.get('studio')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Fusion metadata not available for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Comprehensive Fusion Report\n",
    "\n",
    "PyDI's reporting framework provides detailed analytics and diagnostics for fusion results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive fusion report\n",
    "fusion_report = FusionReport(\n",
    "    fused_df=fused_movies,\n",
    "    input_datasets=[academy_awards_df, actors_df, golden_globes_df],\n",
    "    strategy_name=movie_strategy.name,\n",
    "    correspondences=all_correspondences,\n",
    "    # evaluation_results will be added later\n",
    ")\n",
    "\n",
    "# Display the comprehensive report\n",
    "fusion_report.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed quality metrics\n",
    "quality_metrics = FusionQualityMetrics.calculate_consistency_metrics(fused_movies)\n",
    "coverage_metrics = FusionQualityMetrics.calculate_coverage_metrics(\n",
    "    [academy_awards_df, actors_df, golden_globes_df], fused_movies\n",
    ")\n",
    "\n",
    "print(\"üìà Detailed Quality Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüéØ Quality Metrics:\")\n",
    "for key, value in quality_metrics.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"    {sub_key}: {sub_value}\")\n",
    "    else:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìä Coverage Metrics:\")\n",
    "for key, value in coverage_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploring Connected Components Grouping\n",
    "\n",
    "Let's examine how PyDI's connected components algorithm grouped records for fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually examine the record grouping process\n",
    "record_groups = build_record_groups_from_correspondences(\n",
    "    [academy_awards_df, actors_df, golden_globes_df],\n",
    "    all_correspondences\n",
    ")\n",
    "\n",
    "print(f\"üîó Connected Components Analysis:\")\n",
    "print(f\"Total groups: {len(record_groups)}\")\n",
    "\n",
    "# Analyze group sizes\n",
    "group_sizes = [len(group.records) for group in record_groups]\n",
    "group_size_counts = pd.Series(group_sizes).value_counts().sort_index()\n",
    "\n",
    "print(\"\\nGroup Size Distribution:\")\n",
    "for size, count in group_size_counts.items():\n",
    "    print(f\"  {size} records: {count} groups\")\n",
    "\n",
    "# Show examples of multi-record groups\n",
    "multi_record_groups = [g for g in record_groups if len(g.records) > 1]\n",
    "print(f\"\\nüé≠ Multi-Record Groups: {len(multi_record_groups)}\")\n",
    "\n",
    "for i, group in enumerate(multi_record_groups[:3], 1):\n",
    "    print(f\"\\nGroup {i} ({group.group_id}): {len(group.records)} records\")\n",
    "    for record in group.records:\n",
    "        dataset = group.source_datasets.get(record.get('_id', ''), 'unknown')\n",
    "        title = record.get('title', 'N/A')\n",
    "        print(f\"  - [{dataset}] {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced: Custom Evaluation Rules\n",
    "\n",
    "Let's create some custom evaluation rules to assess fusion quality against domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gold standard for a few movies (manually curated)\n",
    "gold_standard_data = {\n",
    "    '_id': ['manual_1', 'manual_2', 'manual_3'],\n",
    "    'title': ['7th Heaven', 'Coquette', 'The Broadway Melody'],\n",
    "    'date': ['1927-01-01', '1929-01-01', '1929-01-01'],  # Known correct dates\n",
    "    'director': ['Frank Borzage', 'Sam Taylor', 'Harry Beaumont']\n",
    "}\n",
    "\n",
    "gold_standard_df = pd.DataFrame(gold_standard_data)\n",
    "\n",
    "print(\"üèÜ Gold Standard Dataset:\")\n",
    "display(gold_standard_df)\n",
    "\n",
    "# For this demo, we'll evaluate a subset of our results\n",
    "# In practice, you'd have a comprehensive gold standard\n",
    "print(\"\\nüìù Note: This is a simplified evaluation for demonstration.\")\n",
    "print(\"In practice, you would have a comprehensive gold standard dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Provenance Tracking Demonstration\n",
    "\n",
    "PyDI's provenance tracking system keeps detailed lineage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate provenance tracking\n",
    "provenance_tracker = ProvenanceTracker()\n",
    "\n",
    "# Register datasets with trust scores\n",
    "provenance_tracker.register_dataset_source('academy_awards', trust_score=0.9)\n",
    "provenance_tracker.register_dataset_source('actors', trust_score=0.85)\n",
    "provenance_tracker.register_dataset_source('golden_globes', trust_score=0.88)\n",
    "\n",
    "# Track input data\n",
    "for df in [academy_awards_df, actors_df, golden_globes_df]:\n",
    "    provenance_tracker.track_input_data(df, df.attrs['dataset_name'])\n",
    "\n",
    "print(\"üîç Provenance Tracking Summary:\")\n",
    "source_stats = provenance_tracker.get_source_statistics()\n",
    "\n",
    "for source, stats in source_stats.items():\n",
    "    print(f\"\\nüìä {source}:\")\n",
    "    print(f\"  Records: {stats['record_count']}\")\n",
    "    print(f\"  Trust Score: {stats['trust_score']:.2f}\")\n",
    "    print(f\"  Avg Confidence: {stats['average_confidence']:.3f}\")\n",
    "    print(f\"  Contribution: {stats['contribution_ratio']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Legacy API\n",
    "\n",
    "PyDI maintains backward compatibility with the original DataFuser API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate backward compatibility with legacy API\n",
    "from PyDI.fusion import DataFuser, FusionRule\n",
    "\n",
    "print(\"üîÑ Legacy API Demonstration:\")\n",
    "print(\"(Maintaining backward compatibility)\")\n",
    "\n",
    "# Create legacy fusion rules\n",
    "legacy_rules = {\n",
    "    'title': FusionRule('longest'),\n",
    "    'director': FusionRule('longest'),\n",
    "    'date': FusionRule('most_recent')\n",
    "}\n",
    "\n",
    "# Use legacy DataFuser\n",
    "legacy_fuser = DataFuser()\n",
    "\n",
    "# For demo, use a small subset of data\n",
    "small_correspondences = all_correspondences.head(5)\n",
    "\n",
    "print(f\"\\nRunning legacy fusion with {len(small_correspondences)} correspondences...\")\n",
    "\n",
    "try:\n",
    "    legacy_result = legacy_fuser.fuse(\n",
    "        datasets=[academy_awards_df, actors_df, golden_globes_df],\n",
    "        correspondences=small_correspondences,\n",
    "        rules=legacy_rules\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Legacy fusion successful: {len(legacy_result)} records\")\n",
    "    if len(legacy_result) > 0:\n",
    "        display_cols = ['_id', 'title', 'director', 'date']\n",
    "        available_cols = [col for col in display_cols if col in legacy_result.columns]\n",
    "        display(legacy_result[available_cols].head(3))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Legacy fusion encountered an issue: {e}\")\n",
    "    print(\"This may be due to data format differences - the new API is more robust!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance and Scalability Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of the fusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance analysis\n",
    "print(\"‚ö° Performance Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Measure fusion time for different data sizes\n",
    "def measure_fusion_time(datasets, correspondences, strategy):\n",
    "    start_time = time.time()\n",
    "    engine = DataFusionEngine(strategy)\n",
    "    result = engine.run(datasets, correspondences)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, len(result)\n",
    "\n",
    "# Test with full dataset\n",
    "fusion_time, result_count = measure_fusion_time(\n",
    "    [academy_awards_df, actors_df, golden_globes_df],\n",
    "    all_correspondences,\n",
    "    movie_strategy\n",
    ")\n",
    "\n",
    "total_input_records = len(academy_awards_df) + len(actors_df) + len(golden_globes_df)\n",
    "\n",
    "print(f\"üìä Full Dataset Performance:\")\n",
    "print(f\"  Input records: {total_input_records:,}\")\n",
    "print(f\"  Output records: {result_count:,}\")\n",
    "print(f\"  Correspondences: {len(all_correspondences):,}\")\n",
    "print(f\"  Fusion time: {fusion_time:.3f} seconds\")\n",
    "print(f\"  Records/second: {total_input_records/fusion_time:.0f}\")\n",
    "print(f\"  Compression ratio: {result_count/total_input_records:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export and Reporting\n",
    "\n",
    "Finally, let's export our results and generate comprehensive reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fusion results to different formats\n",
    "print(\"üíæ Export Options:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show JSON report structure (without actually saving)\n",
    "report_json = fusion_report.to_json()\n",
    "print(f\"üìÑ JSON Report Size: {len(report_json):,} characters\")\n",
    "\n",
    "# Show data export formats available\n",
    "print(f\"\\nüìä Available Export Formats:\")\n",
    "print(f\"  ‚Ä¢ CSV: {len(fused_movies)} records ready for export\")\n",
    "print(f\"  ‚Ä¢ JSON: Full report with metadata\")\n",
    "print(f\"  ‚Ä¢ HTML: Interactive report with visualizations\")\n",
    "print(f\"  ‚Ä¢ Parquet: Efficient binary format with metadata\")\n",
    "\n",
    "# Sample export commands (commented to avoid file creation)\n",
    "print(f\"\\nüí° Sample Export Commands:\")\n",
    "print(f\"```python\")\n",
    "print(f\"# Export fused data\")\n",
    "print(f\"fused_movies.to_csv('fused_movies.csv', index=False)\")\n",
    "print(f\"\")\n",
    "print(f\"# Export comprehensive report\")\n",
    "print(f\"fusion_report.export_detailed_results('output/movie_fusion/')\")\n",
    "print(f\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "üéâ **Congratulations!** You've successfully explored PyDI's comprehensive data fusion framework.\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **‚úÖ Loaded Winter movie datasets** - Parsed XML data and correspondences\n",
    "2. **‚úÖ Created sophisticated fusion strategies** - Custom conflict resolution rules\n",
    "3. **‚úÖ Executed connected components fusion** - Advanced record grouping\n",
    "4. **‚úÖ Generated quality reports** - Comprehensive analytics and diagnostics\n",
    "5. **‚úÖ Demonstrated provenance tracking** - Full lineage and trust management\n",
    "6. **‚úÖ Showed backward compatibility** - Legacy API still supported\n",
    "\n",
    "### Key Features of PyDI Fusion:\n",
    "\n",
    "- üîß **Modular Architecture**: Pluggable rules and configurable strategies\n",
    "- üêº **Pandas-First**: Native DataFrame operations with metadata support\n",
    "- üìä **Rich Analytics**: Confidence scores, quality metrics, and detailed reporting\n",
    "- üîç **Full Provenance**: Track data lineage and source contributions\n",
    "- ‚ö° **High Performance**: Efficient algorithms for large-scale fusion\n",
    "- üîÑ **Backward Compatible**: Maintains existing API while adding new capabilities\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore custom conflict resolution functions for your domain\n",
    "- Set up evaluation pipelines with gold standard data\n",
    "- Integrate with PyDI's entity matching and schema matching components\n",
    "- Scale to larger datasets and implement distributed processing\n",
    "\n",
    "The PyDI data fusion framework provides production-ready capabilities for complex data integration scenarios while maintaining the flexibility and ease-of-use that Python developers expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"üèÅ Final Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìö Datasets processed: 3 (Academy Awards, Actors, Golden Globes)\")\n",
    "print(f\"üìä Total input records: {total_input_records:,}\")\n",
    "print(f\"üéØ Fused output records: {len(fused_movies):,}\")\n",
    "print(f\"üîó Correspondences used: {len(all_correspondences):,}\")\n",
    "print(f\"‚öôÔ∏è  Fusion rules applied: {len(movie_strategy.get_registered_attributes())}\")\n",
    "print(f\"‚è±Ô∏è  Processing time: {fusion_time:.3f} seconds\")\n",
    "print(f\"‚ú® Framework: PyDI Data Fusion v2.0\")\n",
    "print(\"\\nüéâ Notebook execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
