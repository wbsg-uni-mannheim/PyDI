{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDI Data Integration Tutorial\n",
    "\n",
    "This tutorial demonstrates comprehensive data integration using PyDI. We'll work with movie datasets to showcase the complete data integration pipeline.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Data Loading & Profiling**: Load and analyze movie datasets with provenance tracking\n",
    "2. **Identity Resolution**: \n",
    "   - Advanced blocking strategies (Standard, Sorted Neighbourhood, Token-based, Embedding-based)\n",
    "   - Multi-attribute similarity matching with custom comparators\n",
    "   - Machine learning-based entity matching\n",
    "3. **Data Fusion**: \n",
    "   - Conflict resolution with custom fusion rules\n",
    "   - Quality assessment against gold standards\n",
    "   - Provenance tracking and trust management\n",
    "4. **Advanced Techniques**: \n",
    "   - Semantic similarity with embeddings\n",
    "   - Performance optimization and scalability\n",
    "   - End-to-end pipeline integration\n",
    "\n",
    "### Datasets\n",
    "\n",
    "We'll use three movie datasets:\n",
    "- **Academy Awards**: Movies with Oscar information (4,592 records)\n",
    "- **Actors**: Movies with actor details (149 records) \n",
    "- **Golden Globes**: Movies with Golden Globe awards (2,286 records)\n",
    "\n",
    "These datasets contain overlapping movie information but with different attributes, data quality issues, and conflicting values - perfect for demonstrating real-world data integration challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the PyDI package if not already installed\n",
    "# First navigate to the root directory of the repository in your terminal, then run:\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Embedding models available\n",
      "PyDI Tutorial\n",
      "Repository root: c:\\Users\\Ralph\\dev\\pydi\n",
      "Output directory: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\n",
      "All systems ready! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PyDI imports for data loading and profiling\n",
    "from PyDI.io import load_xml, load_csv\n",
    "from PyDI.profiling import DataProfiler\n",
    "\n",
    "# PyDI imports for entity matching\n",
    "from PyDI.entitymatching import (\n",
    "    # Blocking strategies\n",
    "    NoBlocking, StandardBlocking, SortedNeighbourhood, \n",
    "    TokenBlocking, EmbeddingBlocking,\n",
    "    # Matchers\n",
    "    RuleBasedMatcher, MLBasedMatcher,\n",
    "    # Feature extraction for ML\n",
    "    FeatureExtractor,\n",
    "    # Comparators\n",
    "    StringComparator, DateComparator, NumericComparator,\n",
    "    # Evaluation - NEW: Separate methods for blocking and matching evaluation\n",
    "    EntityMatchingEvaluator,\n",
    "    # Utilities\n",
    "    ensure_record_ids\n",
    ")\n",
    "\n",
    "# PyDI imports for data fusion\n",
    "from PyDI.fusion import (\n",
    "    DataFusionEngine, DataFusionStrategy, DataFusionEvaluator,\n",
    "    # Fusion rules\n",
    "    longest_string, shortest_string, most_recent, earliest,\n",
    "    average, median, maximum, minimum, most_complete,\n",
    "    union, intersection, voting,\n",
    "    # Convenient aliases\n",
    "    LONGEST, SHORTEST, LATEST, EARLIEST, AVG, MAX, MIN, VOTE, UNION,\n",
    "    # Analysis and reporting\n",
    "    FusionReport, FusionQualityMetrics, ProvenanceTracker,\n",
    "    build_record_groups_from_correspondences,\n",
    ")\n",
    "\n",
    "# Setup paths\n",
    "def get_repo_root():\n",
    "    \"\"\"Get repository root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'pyproject.toml').exists():\n",
    "            return current\n",
    "        current = current.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = get_repo_root()\n",
    "OUTPUT_DIR = ROOT / \"output\" / \"tutorial\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if embeddings are available\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    use_embeddings = True\n",
    "    print(\"üß† Embedding models available\")\n",
    "except ImportError:\n",
    "    use_embeddings = False\n",
    "    print(\"‚ö†Ô∏è  Embedding models not available (install sentence-transformers)\")\n",
    "\n",
    "print(f\"PyDI Tutorial\")\n",
    "print(f\"Repository root: {ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"All systems ready! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Profiling\n",
    "\n",
    "PyDI provides provenance-aware data loading that automatically tracks dataset metadata and adds unique identifiers. Let's load our movie datasets and understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Movie Datasets ===\n",
      "PyDI provides provenance-aware loading with automatic ID generation.\n",
      "\n",
      "Academy Awards:\n",
      "  Records: 4,592\n",
      "  Attributes: 7\n",
      "  Columns: ['_id', 'id', 'title', 'actor_name', 'date', 'director_name', 'oscar']\n",
      "  Dataset name: academy_awards\n",
      "\n",
      "Actors:\n",
      "  Records: 149\n",
      "  Attributes: 7\n",
      "  Columns: ['_id', 'id', 'title', 'actor_name', 'actors_actor_birthday', 'actors_actor_birthplace', 'date']\n",
      "  Dataset name: actors\n",
      "\n",
      "Golden Globes:\n",
      "  Records: 2,286\n",
      "  Attributes: 7\n",
      "  Columns: ['_id', 'id', 'title', 'actor_name', 'date', 'director_name', 'globe']\n",
      "  Dataset name: golden_globes\n",
      "\n",
      "Total records across all datasets: 7,027\n"
     ]
    }
   ],
   "source": [
    "# Define dataset paths\n",
    "DATA_DIR = ROOT / \"input\" / \"movies\"\n",
    "\n",
    "print(\"=== Loading Movie Datasets ===\")\n",
    "print(\"PyDI provides provenance-aware loading with automatic ID generation.\\n\")\n",
    "\n",
    "# Load Academy Awards dataset\n",
    "academy_awards = load_xml(\n",
    "    DATA_DIR / \"entitymatching\" / \"data\" / \"academy_awards.xml\",\n",
    "    name=\"academy_awards\",\n",
    "    record_tag=\"movie\",\n",
    "    add_index=True,\n",
    "    index_column_name=\"_id\"\n",
    ")\n",
    "\n",
    "# Load Actors dataset  \n",
    "actors = load_xml(\n",
    "    DATA_DIR / \"entitymatching\" / \"data\" / \"actors.xml\",\n",
    "    name=\"actors\", \n",
    "    record_tag=\"movie\",\n",
    "    add_index=True,\n",
    "    index_column_name=\"_id\"\n",
    ")\n",
    "\n",
    "# Load Golden Globes dataset\n",
    "golden_globes = load_xml(\n",
    "    DATA_DIR / \"fusion\" / \"data\" / \"golden_globes.xml\",\n",
    "    name=\"golden_globes\",\n",
    "    record_tag=\"movie\", \n",
    "    add_index=True,\n",
    "    index_column_name=\"_id\"\n",
    ")\n",
    "\n",
    "# Display basic information\n",
    "datasets = [academy_awards, actors, golden_globes]\n",
    "names = [\"Academy Awards\", \"Actors\", \"Golden Globes\"]\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Records: {len(df):,}\")\n",
    "    print(f\"  Attributes: {len(df.columns)}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Dataset name: {df.attrs.get('dataset_name', 'unknown')}\")\n",
    "    print()\n",
    "\n",
    "total_records = sum(len(df) for df in datasets)\n",
    "print(f\"Total records across all datasets: {total_records:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Previews ===\n",
      "\n",
      "üìΩÔ∏è Academy Awards Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>date</th>\n",
       "      <th>director_name</th>\n",
       "      <th>oscar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy_awards-0000</td>\n",
       "      <td>academy_awards_1</td>\n",
       "      <td>Biutiful</td>\n",
       "      <td>Javier Bardem</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academy_awards-0001</td>\n",
       "      <td>academy_awards_2</td>\n",
       "      <td>True Grit</td>\n",
       "      <td>Jeff Bridges</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Joel Coen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academy_awards-0002</td>\n",
       "      <td>academy_awards_2</td>\n",
       "      <td>True Grit</td>\n",
       "      <td>Jeff Bridges</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Ethan Coen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   _id                id      title     actor_name  \\\n",
       "0  academy_awards-0000  academy_awards_1   Biutiful  Javier Bardem   \n",
       "1  academy_awards-0001  academy_awards_2  True Grit   Jeff Bridges   \n",
       "2  academy_awards-0002  academy_awards_2  True Grit   Jeff Bridges   \n",
       "\n",
       "         date director_name oscar  \n",
       "0  2010-01-01           NaN   NaN  \n",
       "1  2010-01-01     Joel Coen   NaN  \n",
       "2  2010-01-01    Ethan Coen   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé≠ Actors Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>actors_actor_birthday</th>\n",
       "      <th>actors_actor_birthplace</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actors-0000</td>\n",
       "      <td>actors_1</td>\n",
       "      <td>7th Heaven</td>\n",
       "      <td>Janet Gaynor</td>\n",
       "      <td>1906-01-01</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1929-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actors-0001</td>\n",
       "      <td>actors_2</td>\n",
       "      <td>Coquette</td>\n",
       "      <td>Mary Pickford</td>\n",
       "      <td>1892-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1930-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actors-0002</td>\n",
       "      <td>actors_3</td>\n",
       "      <td>The Divorcee</td>\n",
       "      <td>Norma Shearer</td>\n",
       "      <td>1902-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1931-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           _id        id         title     actor_name actors_actor_birthday  \\\n",
       "0  actors-0000  actors_1    7th Heaven   Janet Gaynor            1906-01-01   \n",
       "1  actors-0001  actors_2      Coquette  Mary Pickford            1892-01-01   \n",
       "2  actors-0002  actors_3  The Divorcee  Norma Shearer            1902-01-01   \n",
       "\n",
       "  actors_actor_birthplace        date  \n",
       "0            Pennsylvania  1929-01-01  \n",
       "1                  Canada  1930-01-01  \n",
       "2                  Canada  1931-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Golden Globes Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>date</th>\n",
       "      <th>director_name</th>\n",
       "      <th>globe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>golden_globes-0000</td>\n",
       "      <td>golden_globes_1</td>\n",
       "      <td>Frankie and Alice</td>\n",
       "      <td>Halle Berry</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>golden_globes-0001</td>\n",
       "      <td>golden_globes_2</td>\n",
       "      <td>Rabbit Hole</td>\n",
       "      <td>Nicole Kidman</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>golden_globes-0002</td>\n",
       "      <td>golden_globes_3</td>\n",
       "      <td>Winter's Bone</td>\n",
       "      <td>Jennifer Lawrence</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  _id               id              title         actor_name  \\\n",
       "0  golden_globes-0000  golden_globes_1  Frankie and Alice        Halle Berry   \n",
       "1  golden_globes-0001  golden_globes_2        Rabbit Hole      Nicole Kidman   \n",
       "2  golden_globes-0002  golden_globes_3      Winter's Bone  Jennifer Lawrence   \n",
       "\n",
       "         date director_name globe  \n",
       "0  2011-01-01           NaN   NaN  \n",
       "1  2011-01-01           NaN   NaN  \n",
       "2  2011-01-01           NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview the data structure\n",
    "print(\"=== Dataset Previews ===\")\n",
    "\n",
    "print(\"\\nüìΩÔ∏è Academy Awards Dataset:\")\n",
    "display(academy_awards.head(3))\n",
    "\n",
    "print(\"\\nüé≠ Actors Dataset:\")\n",
    "display(actors.head(3))\n",
    "\n",
    "print(\"\\nüèÜ Golden Globes Dataset:\")\n",
    "display(golden_globes.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Analysis\n",
    "\n",
    "Let's use PyDI's profiling capabilities to understand our data quality and identify the best attributes for matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Dataset Summary\n",
    "\n",
    "First, let's use the DataProfiler's `summary()` method to get basic statistics for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Summary Statistics ===\n",
      "\n",
      "academy_awards:\n",
      "  Rows: 4,592\n",
      "  Columns: 7\n",
      "  Total nulls: 11,036\n",
      "  Null percentage: 34.3%\n",
      "  Null counts per column:\n",
      "    title: 12 (0.3%)\n",
      "    actor_name: 3,535 (77.0%)\n",
      "    director_name: 4,172 (90.9%)\n",
      "    oscar: 3,317 (72.2%)\n",
      "\n",
      "actors:\n",
      "  Rows: 149\n",
      "  Columns: 7\n",
      "  Total nulls: 0\n",
      "  Null percentage: 0.0%\n",
      "\n",
      "golden_globes:\n",
      "  Rows: 2,286\n",
      "  Columns: 7\n",
      "  Total nulls: 3,681\n",
      "  Null percentage: 23.0%\n",
      "  Null counts per column:\n",
      "    actor_name: 54 (2.4%)\n",
      "    director_name: 1,966 (86.0%)\n",
      "    globe: 1,661 (72.7%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rows': 2286,\n",
       " 'columns': 7,\n",
       " 'nulls_total': 3681,\n",
       " 'nulls_per_column': {'_id': 0,\n",
       "  'id': 0,\n",
       "  'title': 0,\n",
       "  'actor_name': 54,\n",
       "  'date': 0,\n",
       "  'director_name': 1966,\n",
       "  'globe': 1661},\n",
       " 'dtypes': {'_id': 'string',\n",
       "  'id': 'object',\n",
       "  'title': 'object',\n",
       "  'actor_name': 'object',\n",
       "  'date': 'object',\n",
       "  'director_name': 'object',\n",
       "  'globe': 'object'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the DataProfiler\n",
    "profiler = DataProfiler()\n",
    "\n",
    "print(\"=== Dataset Summary Statistics ===\\n\")\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    profile = profiler.summary(df) # automatically prints some statistics and returns object containing stats\n",
    "\n",
    "display(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Coverage Analysis\n",
    "\n",
    "Next, let's use the `analyze_coverage()` method to understand how attributes overlap across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Attribute Coverage Analysis ===\n",
      "\n",
      "üìä Attribute coverage across datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute</th>\n",
       "      <th>academy_awards_count</th>\n",
       "      <th>academy_awards_pct</th>\n",
       "      <th>academy_awards_coverage</th>\n",
       "      <th>academy_awards_samples</th>\n",
       "      <th>actors_count</th>\n",
       "      <th>actors_pct</th>\n",
       "      <th>actors_coverage</th>\n",
       "      <th>actors_samples</th>\n",
       "      <th>golden_globes_count</th>\n",
       "      <th>golden_globes_pct</th>\n",
       "      <th>golden_globes_coverage</th>\n",
       "      <th>golden_globes_samples</th>\n",
       "      <th>avg_coverage</th>\n",
       "      <th>max_coverage</th>\n",
       "      <th>datasets_with_attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_id</td>\n",
       "      <td>4592/4592</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['academy_awards-0000', 'academy_awards-0001',...</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['actors-0000', 'actors-0001', 'actors-0002']</td>\n",
       "      <td>2286/2286</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['golden_globes-0000', 'golden_globes-0001', '...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor_name</td>\n",
       "      <td>1057/4592</td>\n",
       "      <td>23.0%</td>\n",
       "      <td>0.230183</td>\n",
       "      <td>['Javier Bardem', 'Jeff Bridges', 'Jeff Bridges']</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Janet Gaynor', 'Mary Pickford', 'Norma Shear...</td>\n",
       "      <td>2232/2286</td>\n",
       "      <td>97.6%</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>['Halle Berry', 'Nicole Kidman', 'Jennifer Law...</td>\n",
       "      <td>0.735520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actors_actor_birthday</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['1906-01-01', '1892-01-01', '1902-01-01']</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>actors_actor_birthplace</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Pennsylvania', 'Canada', 'Canada']</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date</td>\n",
       "      <td>4592/4592</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['2010-01-01', '2010-01-01', '2010-01-01']</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['1929-01-01', '1930-01-01', '1931-01-01']</td>\n",
       "      <td>2286/2286</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['2011-01-01', '2011-01-01', '2011-01-01']</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>director_name</td>\n",
       "      <td>420/4592</td>\n",
       "      <td>9.1%</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>['Joel Coen', 'Ethan Coen', 'David Fincher']</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>320/2286</td>\n",
       "      <td>14.0%</td>\n",
       "      <td>0.139983</td>\n",
       "      <td>['Darren Aronofsky', 'David Fincher', 'Tom Hoo...</td>\n",
       "      <td>0.077149</td>\n",
       "      <td>0.139983</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>globe</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>625/2286</td>\n",
       "      <td>27.3%</td>\n",
       "      <td>0.273403</td>\n",
       "      <td>['yes', 'yes', 'yes']</td>\n",
       "      <td>0.091134</td>\n",
       "      <td>0.273403</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id</td>\n",
       "      <td>4592/4592</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['academy_awards_1', 'academy_awards_2', 'acad...</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['actors_1', 'actors_2', 'actors_3']</td>\n",
       "      <td>2286/2286</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['golden_globes_1', 'golden_globes_2', 'golden...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oscar</td>\n",
       "      <td>1275/4592</td>\n",
       "      <td>27.8%</td>\n",
       "      <td>0.277657</td>\n",
       "      <td>['yes', 'yes', 'yes']</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.092552</td>\n",
       "      <td>0.277657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>title</td>\n",
       "      <td>4580/4592</td>\n",
       "      <td>99.7%</td>\n",
       "      <td>0.997387</td>\n",
       "      <td>['Biutiful', 'True Grit', 'True Grit']</td>\n",
       "      <td>149/149</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['7th Heaven', 'Coquette', 'The Divorcee']</td>\n",
       "      <td>2286/2286</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['Frankie and Alice', 'Rabbit Hole', \"Winter's...</td>\n",
       "      <td>0.999129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 attribute academy_awards_count academy_awards_pct  \\\n",
       "0                      _id            4592/4592             100.0%   \n",
       "1               actor_name            1057/4592              23.0%   \n",
       "2    actors_actor_birthday                  0/0                 0%   \n",
       "3  actors_actor_birthplace                  0/0                 0%   \n",
       "4                     date            4592/4592             100.0%   \n",
       "5            director_name             420/4592               9.1%   \n",
       "6                    globe                  0/0                 0%   \n",
       "7                       id            4592/4592             100.0%   \n",
       "8                    oscar            1275/4592              27.8%   \n",
       "9                    title            4580/4592              99.7%   \n",
       "\n",
       "   academy_awards_coverage                             academy_awards_samples  \\\n",
       "0                 1.000000  ['academy_awards-0000', 'academy_awards-0001',...   \n",
       "1                 0.230183  ['Javier Bardem', 'Jeff Bridges', 'Jeff Bridges']   \n",
       "2                 0.000000                                                N/A   \n",
       "3                 0.000000                                                N/A   \n",
       "4                 1.000000         ['2010-01-01', '2010-01-01', '2010-01-01']   \n",
       "5                 0.091463       ['Joel Coen', 'Ethan Coen', 'David Fincher']   \n",
       "6                 0.000000                                                N/A   \n",
       "7                 1.000000  ['academy_awards_1', 'academy_awards_2', 'acad...   \n",
       "8                 0.277657                              ['yes', 'yes', 'yes']   \n",
       "9                 0.997387             ['Biutiful', 'True Grit', 'True Grit']   \n",
       "\n",
       "  actors_count actors_pct  actors_coverage  \\\n",
       "0      149/149     100.0%              1.0   \n",
       "1      149/149     100.0%              1.0   \n",
       "2      149/149     100.0%              1.0   \n",
       "3      149/149     100.0%              1.0   \n",
       "4      149/149     100.0%              1.0   \n",
       "5          0/0         0%              0.0   \n",
       "6          0/0         0%              0.0   \n",
       "7      149/149     100.0%              1.0   \n",
       "8          0/0         0%              0.0   \n",
       "9      149/149     100.0%              1.0   \n",
       "\n",
       "                                      actors_samples golden_globes_count  \\\n",
       "0      ['actors-0000', 'actors-0001', 'actors-0002']           2286/2286   \n",
       "1  ['Janet Gaynor', 'Mary Pickford', 'Norma Shear...           2232/2286   \n",
       "2         ['1906-01-01', '1892-01-01', '1902-01-01']                 0/0   \n",
       "3               ['Pennsylvania', 'Canada', 'Canada']                 0/0   \n",
       "4         ['1929-01-01', '1930-01-01', '1931-01-01']           2286/2286   \n",
       "5                                                N/A            320/2286   \n",
       "6                                                N/A            625/2286   \n",
       "7               ['actors_1', 'actors_2', 'actors_3']           2286/2286   \n",
       "8                                                N/A                 0/0   \n",
       "9         ['7th Heaven', 'Coquette', 'The Divorcee']           2286/2286   \n",
       "\n",
       "  golden_globes_pct  golden_globes_coverage  \\\n",
       "0            100.0%                1.000000   \n",
       "1             97.6%                0.976378   \n",
       "2                0%                0.000000   \n",
       "3                0%                0.000000   \n",
       "4            100.0%                1.000000   \n",
       "5             14.0%                0.139983   \n",
       "6             27.3%                0.273403   \n",
       "7            100.0%                1.000000   \n",
       "8                0%                0.000000   \n",
       "9            100.0%                1.000000   \n",
       "\n",
       "                               golden_globes_samples  avg_coverage  \\\n",
       "0  ['golden_globes-0000', 'golden_globes-0001', '...      1.000000   \n",
       "1  ['Halle Berry', 'Nicole Kidman', 'Jennifer Law...      0.735520   \n",
       "2                                                N/A      0.333333   \n",
       "3                                                N/A      0.333333   \n",
       "4         ['2011-01-01', '2011-01-01', '2011-01-01']      1.000000   \n",
       "5  ['Darren Aronofsky', 'David Fincher', 'Tom Hoo...      0.077149   \n",
       "6                              ['yes', 'yes', 'yes']      0.091134   \n",
       "7  ['golden_globes_1', 'golden_globes_2', 'golden...      1.000000   \n",
       "8                                                N/A      0.092552   \n",
       "9  ['Frankie and Alice', 'Rabbit Hole', \"Winter's...      0.999129   \n",
       "\n",
       "   max_coverage  datasets_with_attribute  \n",
       "0      1.000000                        3  \n",
       "1      1.000000                        3  \n",
       "2      1.000000                        1  \n",
       "3      1.000000                        1  \n",
       "4      1.000000                        3  \n",
       "5      0.139983                        2  \n",
       "6      0.273403                        1  \n",
       "7      1.000000                        3  \n",
       "8      0.277657                        1  \n",
       "9      1.000000                        3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Attributes suitable for entity matching:\n",
      "Available in 2+ datasets: ['_id', 'actor_name', 'date', 'director_name', 'id', 'title']\n"
     ]
    }
   ],
   "source": [
    "# Analyze attribute coverage across all three datasets\n",
    "print(\"=== Attribute Coverage Analysis ===\\n\")\n",
    "\n",
    "coverage = profiler.analyze_coverage(\n",
    "    datasets=datasets,\n",
    "    include_samples=True,\n",
    "    sample_count=3  # Show 3 sample values per attribute\n",
    ")\n",
    "\n",
    "print(\"üìä Attribute coverage across datasets:\")\n",
    "display(coverage)\n",
    "\n",
    "# Identify attributes suitable for entity matching\n",
    "print(\"\\nüîó Attributes suitable for entity matching:\")\n",
    "matching_attrs = coverage[coverage['datasets_with_attribute'] >= 2]['attribute'].tolist()\n",
    "print(f\"Available in 2+ datasets: {matching_attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Data Profiling\n",
    "\n",
    "Now let's generate comprehensive HTML profiles for each dataset using the `profile()` method. These reports provide in-depth statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating Detailed Dataset Profiles ===\n",
      "\n",
      "üìä Profiling Academy Awards...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a444bf82c79b4450abd86220de2d9395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 94.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b08d30cbda04133a6a84a44a7992e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c688750ec3af434c9fe05a8f2f362978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e81900ae674137a894ce18d4bbfa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Profile saved: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\data_profiles\\academy_awards_profile.html\n",
      "üìä Profiling Actors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763e85127595427bbf027df0d070fd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 241.38it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447e509436094f93a2410057826f4639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790a5a98ebf40d9b44e1b6e4576a6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3de18e372e492786f46396ae7e96dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Profile saved: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\data_profiles\\actors_profile.html\n",
      "üìä Profiling Golden Globes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7e8250e9d741568ce9a0ae4f109b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 97.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bc237a072e4e94a50cb6bc4b279e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04be67bfb798449eb72451c20c698e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0c1470b91841d3a4666efdc49a9fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Profile saved: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\data_profiles\\golden_globes_profile.html\n",
      "\n",
      "üéØ Generated 3 detailed HTML reports\n",
      "üìÅ Location: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\data_profiles\n",
      "\n",
      "üí° Open these HTML files in your browser for interactive exploration:\n",
      "  ‚Ä¢ academy_awards_profile.html\n",
      "  ‚Ä¢ actors_profile.html\n",
      "  ‚Ä¢ golden_globes_profile.html\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed HTML profiles for each dataset\n",
    "print(\"=== Generating Detailed Dataset Profiles ===\\n\")\n",
    "\n",
    "profile_dir = OUTPUT_DIR / \"data_profiles\"\n",
    "profile_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "profile_paths = []\n",
    "\n",
    "for df, name in zip(datasets, names):\n",
    "    print(f\"üìä Profiling {name}...\")\n",
    "    \n",
    "    profile_path = profiler.profile(df, str(profile_dir))\n",
    "    profile_paths.append(profile_path)\n",
    "    print(f\"  ‚úÖ Profile saved: {profile_path}\")\n",
    "\n",
    "print(f\"\\nüéØ Generated {len(profile_paths)} detailed HTML reports\")\n",
    "print(f\"üìÅ Location: {profile_dir}\")\n",
    "print(\"\\nüí° Open these HTML files in your browser for interactive exploration:\")\n",
    "for path in profile_paths:\n",
    "    print(f\"  ‚Ä¢ {Path(path).name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Comparison\n",
    "\n",
    "Finally, let's use the `compare()` method to create a comparison report between two datasets, highlighting differences and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Comparison Analysis ===\n",
      "\n",
      "üîç Comparing Academy Awards vs Golden Globes datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e7ae26859644cabbee2d5de881cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |          | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\comparisons\\academy_awards_vs_golden_globes_compare.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n",
      "‚úÖ Comparison report saved: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\comparisons\\academy_awards_vs_golden_globes_compare.html\n",
      "\n",
      "üéØ Interactive comparison report generated\n",
      "üìÅ Location: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\comparisons\\academy_awards_vs_golden_globes_compare.html\n",
      "üí° Open in browser to explore:\n",
      "  ‚Ä¢ Attribute distributions\n",
      "  ‚Ä¢ Value frequency comparisons\n",
      "  ‚Ä¢ Missing data patterns\n",
      "  ‚Ä¢ Statistical differences\n"
     ]
    }
   ],
   "source": [
    "# Compare Academy Awards vs Golden Globes datasets\n",
    "print(\"=== Dataset Comparison Analysis ===\\n\")\n",
    "\n",
    "compare_dir = OUTPUT_DIR / \"comparisons\"\n",
    "compare_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîç Comparing Academy Awards vs Golden Globes datasets...\")\n",
    "\n",
    "# Fix the comparison call by using Sweetviz directly with correct format\n",
    "import sweetviz as sv\n",
    "report = sv.compare((academy_awards, \"Academy Awards\"), (golden_globes, \"Golden Globes\"))\n",
    "comparison_path = str(compare_dir / \"academy_awards_vs_golden_globes_compare.html\")\n",
    "report.show_html(comparison_path)\n",
    "print(f\"‚úÖ Comparison report saved: {comparison_path}\")\n",
    "\n",
    "print(f\"\\nüéØ Interactive comparison report generated\")\n",
    "print(f\"üìÅ Location: {comparison_path}\")\n",
    "print(\"üí° Open in browser to explore:\")\n",
    "print(\"  ‚Ä¢ Attribute distributions\")\n",
    "print(\"  ‚Ä¢ Value frequency comparisons\") \n",
    "print(\"  ‚Ä¢ Missing data patterns\")\n",
    "print(\"  ‚Ä¢ Statistical differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Identity Resolution (Entity Matching)\n",
    "\n",
    "Identity Resolution is the process of identifying records that refer to the same real-world entity. PyDI provides comprehensive blocking and matching capabilities.\n",
    "\n",
    "### Step 1: Blocking Strategies\n",
    "\n",
    "Blocking reduces the number of comparisons from O(n¬≤) to a manageable subset. Let's explore different blocking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Identity Resolution: Blocking Strategies ===\n",
      "Blocking reduces comparisons from full Cartesian product to manageable candidates.\n",
      "\n",
      "Without blocking: 684,208 comparisons required\n",
      "\n",
      "üéØ Goal: Reduce comparisons while maintaining high recall\n",
      "\n",
      "Testing different blocking strategies...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Identity Resolution: Blocking Strategies ===\")\n",
    "print(\"Blocking reduces comparisons from full Cartesian product to manageable candidates.\\n\")\n",
    "\n",
    "# We'll focus on Academy Awards vs Actors for entity matching\n",
    "left_df = academy_awards\n",
    "right_df = actors\n",
    "\n",
    "max_pairs = len(left_df) * len(right_df)\n",
    "print(f\"Without blocking: {max_pairs:,} comparisons required\")\n",
    "print(\"\\nüéØ Goal: Reduce comparisons while maintaining high recall\\n\")\n",
    "\n",
    "# Ensure datasets have proper IDs for matching\n",
    "left_df = ensure_record_ids(left_df)\n",
    "right_df = ensure_record_ids(right_df)\n",
    "\n",
    "blocking_results = []\n",
    "\n",
    "print(\"Testing different blocking strategies...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Standard Blocking (First 3 Characters of Title)\n",
      "  Generated: 34,457 candidates\n",
      "  Reduction: 95.0% (0.0504 ratio)\n",
      "  Time: 0.064 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. Standard Blocking - First 3 characters of title\n",
    "print(\"\\n1Ô∏è‚É£ Standard Blocking (First 3 Characters of Title)\")\n",
    "\n",
    "# Add title_prefix directly to the original dataframes\n",
    "academy_awards['title_prefix'] = academy_awards['title'].astype(str).str[:3]\n",
    "actors['title_prefix'] = actors['title'].astype(str).str[:3]\n",
    "\n",
    "standard_blocker = StandardBlocking(\n",
    "    academy_awards, actors,\n",
    "    on=['title_prefix'],  # Block on first 3 characters of title\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "standard_candidates = []\n",
    "for batch in standard_blocker:\n",
    "    standard_candidates.extend(batch.to_dict('records'))\n",
    "    \n",
    "standard_time = time.time() - start_time\n",
    "reduction_ratio = len(standard_candidates) / max_pairs\n",
    "\n",
    "print(f\"  Generated: {len(standard_candidates):,} candidates\")\n",
    "print(f\"  Reduction: {(1-reduction_ratio)*100:.1f}% ({reduction_ratio:.4f} ratio)\")\n",
    "print(f\"  Time: {standard_time:.3f} seconds\")\n",
    "\n",
    "blocking_results.append({\n",
    "    'strategy': 'StandardBlocking',\n",
    "    'candidates': len(standard_candidates),\n",
    "    'reduction_ratio': reduction_ratio,\n",
    "    'time_seconds': standard_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Sorted Neighbourhood Blocking (Title-based, Window=5)\n",
      "  Generated: 2,906 candidates\n",
      "  Reduction: 99.6% (0.0042 ratio)\n",
      "  Time: 0.007 seconds\n"
     ]
    }
   ],
   "source": [
    "# 2. Sorted Neighbourhood - Sequential similarity\n",
    "print(\"\\n2Ô∏è‚É£ Sorted Neighbourhood Blocking (Title-based, Window=5)\")\n",
    "\n",
    "sn_blocker = SortedNeighbourhood(\n",
    "    academy_awards, actors,\n",
    "    key='title',  # Sort by title\n",
    "    window=10,     # Compare with 5 neighbors\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "sn_candidates = []\n",
    "for batch in sn_blocker:\n",
    "    sn_candidates.extend(batch.to_dict('records'))\n",
    "    \n",
    "sn_time = time.time() - start_time\n",
    "reduction_ratio = len(sn_candidates) / max_pairs\n",
    "\n",
    "print(f\"  Generated: {len(sn_candidates):,} candidates\")\n",
    "print(f\"  Reduction: {(1-reduction_ratio)*100:.1f}% ({reduction_ratio:.4f} ratio)\")\n",
    "print(f\"  Time: {sn_time:.3f} seconds\")\n",
    "\n",
    "blocking_results.append({\n",
    "    'strategy': 'SortedNeighbourhood', \n",
    "    'candidates': len(sn_candidates),\n",
    "    'reduction_ratio': reduction_ratio,\n",
    "    'time_seconds': sn_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Token Blocking (Title Tokens, Min Length=2)\n",
      "  Generated: 75,242 candidates\n",
      "  Reduction: 89.0% (0.1100 ratio)\n",
      "  Time: 0.146 seconds\n"
     ]
    }
   ],
   "source": [
    "# 3. Token Blocking - Token-based similarity\n",
    "print(\"\\n3Ô∏è‚É£ Token Blocking (Title Tokens, Min Length=2)\")\n",
    "\n",
    "token_blocker = TokenBlocking(\n",
    "    academy_awards, actors,\n",
    "    column='title',      # Tokenize titles\n",
    "    min_token_len=2,     # Ignore very short tokens\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "token_candidates = []\n",
    "batch_count = 0\n",
    "\n",
    "# Token blocking can generate many candidates, so we'll limit processing\n",
    "for batch in token_blocker:\n",
    "    batch_count += 1\n",
    "    token_candidates.extend(batch.to_dict('records'))\n",
    "        \n",
    "token_time = time.time() - start_time\n",
    "reduction_ratio = len(token_candidates) / max_pairs\n",
    "\n",
    "print(f\"  Generated: {len(token_candidates):,} candidates\")\n",
    "print(f\"  Reduction: {(1-reduction_ratio)*100:.1f}% ({reduction_ratio:.4f} ratio)\")\n",
    "print(f\"  Time: {token_time:.3f} seconds\")\n",
    "\n",
    "blocking_results.append({\n",
    "    'strategy': 'TokenBlocking',\n",
    "    'candidates': len(token_candidates),\n",
    "    'reduction_ratio': reduction_ratio, \n",
    "    'time_seconds': token_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ Embedding Blocking (Semantic Similarity)\n",
      "Using neural embeddings for semantic movie matching...\n",
      "  Generated: 1,030 candidates\n",
      "  Reduction: 99.8% (0.0015 ratio)\n",
      "  Time: 3.298 seconds\n",
      "  üß† Semantic matching can find similar movies with different titles!\n"
     ]
    }
   ],
   "source": [
    "# 4. Embedding Blocking - Semantic similarity (Advanced)\n",
    "print(\"\\n4Ô∏è‚É£ Embedding Blocking (Semantic Similarity)\")\n",
    "print(\"Using neural embeddings for semantic movie matching...\")\n",
    "\n",
    "embedding_blocker = EmbeddingBlocking(\n",
    "    academy_awards, actors,\n",
    "    text_cols=['title'],\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    index_backend=\"sklearn\",\n",
    "    top_k=10,          # Top 10 most similar\n",
    "    threshold=0.5,     # Similarity threshold\n",
    "    batch_size=500\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "embedding_candidates = []\n",
    "for batch in embedding_blocker:\n",
    "    embedding_candidates.extend(batch.to_dict('records'))\n",
    "    \n",
    "embedding_time = time.time() - start_time\n",
    "reduction_ratio = len(embedding_candidates) / max_pairs\n",
    "\n",
    "print(f\"  Generated: {len(embedding_candidates):,} candidates\")\n",
    "print(f\"  Reduction: {(1-reduction_ratio)*100:.1f}% ({reduction_ratio:.4f} ratio)\")\n",
    "print(f\"  Time: {embedding_time:.3f} seconds\")\n",
    "print(\"  üß† Semantic matching can find similar movies with different titles!\")\n",
    "\n",
    "blocking_results.append({\n",
    "    'strategy': 'EmbeddingBlocking',\n",
    "    'candidates': len(embedding_candidates),\n",
    "    'reduction_ratio': reduction_ratio,\n",
    "    'time_seconds': embedding_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pair Completeness: 0.979\n",
      "  Pair Quality:      0.001\n",
      "  Reduction Ratio:   0.950\n",
      "  True Matches Found: 46/47\n",
      "\n",
      "üí° Evaluating pair quality only makes sense if the test set contains all possible pairs, which is not the case in this example!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pair_completeness': 0.9787234042553191,\n",
       " 'pair_quality': 0.0013349972429404766,\n",
       " 'reduction_ratio': 0.9496395832846152,\n",
       " 'total_candidates': 34457,\n",
       " 'total_possible_pairs': 684208,\n",
       " 'true_positives_found': 46,\n",
       " 'total_true_pairs': 47,\n",
       " 'evaluation_timestamp': '2025-09-09T15:12:53.252897'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Showcase EntityMatchingEvaluator.evaluate_blocking utility\n",
    "\n",
    "# Load test set with proper _id format\n",
    "test_gt = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"splits\" / \"academy_awards_2_actors_test.csv\",\n",
    "    name=\"test_set\", header=None, names=['id1', 'id2', 'label'], add_index=False\n",
    ")\n",
    "\n",
    "# Use EntityMatchingEvaluator.evaluate_blocking on Standard Blocking\n",
    "candidates_df = pd.DataFrame(standard_candidates)\n",
    "total_pairs = len(academy_awards) * len(actors)\n",
    "\n",
    "results = EntityMatchingEvaluator.evaluate_blocking(\n",
    "    candidate_pairs=candidates_df[['id1', 'id2']],\n",
    "    test_pairs=test_gt,\n",
    "    total_possible_pairs=total_pairs\n",
    ")\n",
    "\n",
    "print(f\"\\nüí° Evaluating pair quality only makes sense if the test set contains all possible pairs, which is not the case in this example!\")\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Selecting Best Blocking Method ===\n",
      "Standard\n",
      "  Pair Completeness: 0.979\n",
      "  Pair Quality:      0.001\n",
      "  Reduction Ratio:   0.950\n",
      "  True Matches Found: 46/47\n",
      "SortedNeighbourhood\n",
      "  Pair Completeness: 0.979\n",
      "  Pair Quality:      0.016\n",
      "  Reduction Ratio:   0.996\n",
      "  True Matches Found: 46/47\n",
      "Token\n",
      "  Pair Completeness: 1.000\n",
      "  Pair Quality:      0.001\n",
      "  Reduction Ratio:   0.890\n",
      "  True Matches Found: 47/47\n",
      "Embedding\n",
      "  Pair Completeness: 1.000\n",
      "  Pair Quality:      0.046\n",
      "  Reduction Ratio:   0.998\n",
      "  True Matches Found: 47/47\n",
      "üìä Blocking Method Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Candidates</th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Reduction</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard</td>\n",
       "      <td>34457</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SortedNeighbourhood</td>\n",
       "      <td>2906</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Token</td>\n",
       "      <td>75242</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embedding</td>\n",
       "      <td>1030</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>3.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method  Candidates Completeness Reduction Time (s)\n",
       "0             Standard       34457        0.979     0.950    0.064\n",
       "1  SortedNeighbourhood        2906        0.979     0.996    0.007\n",
       "2                Token       75242        1.000     0.890    0.146\n",
       "3            Embedding        1030        1.000     0.998    3.298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Method: Embedding (Completeness: 1.000, Reduction: 0.998)\n",
      "‚úÖ Using 1,030 candidate pairs for matching\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all blocking methods and select the best one based on highest pair completeness, then highest reduction ratio (if tie)\n",
    "print(\"=== Selecting Best Blocking Method ===\")\n",
    "\n",
    "# Evaluate all blocking strategies\n",
    "blocking_methods = {\n",
    "    'Standard': (standard_candidates, standard_time),\n",
    "    'SortedNeighbourhood': (sn_candidates, sn_time), \n",
    "    'Token': (token_candidates, token_time),\n",
    "    'Embedding': (embedding_candidates, embedding_time)\n",
    "}\n",
    "\n",
    "best_method = None\n",
    "best_completeness = -1\n",
    "best_reduction = -1\n",
    "results_summary = []\n",
    "\n",
    "for method, (candidates, time_taken) in blocking_methods.items():\n",
    "    print(method)\n",
    "    candidates_df = pd.DataFrame(candidates)\n",
    "    eval_results = EntityMatchingEvaluator.evaluate_blocking(\n",
    "        candidate_pairs=candidates_df[['id1', 'id2']],\n",
    "        test_pairs=test_gt,\n",
    "        total_possible_pairs=total_pairs\n",
    "    )\n",
    "    \n",
    "    completeness = eval_results['pair_completeness']\n",
    "    reduction = eval_results['reduction_ratio']\n",
    "    \n",
    "    results_summary.append({\n",
    "        'Method': method,\n",
    "        'Candidates': len(candidates),\n",
    "        'Completeness': f\"{completeness:.3f}\",\n",
    "        'Reduction': f\"{reduction:.3f}\",\n",
    "        'Time (s)': f\"{time_taken:.3f}\"\n",
    "    })\n",
    "    \n",
    "    # Select best: highest completeness, then highest reduction ratio (if tie)\n",
    "    if (completeness > best_completeness or \n",
    "        (completeness == best_completeness and reduction > best_reduction)):\n",
    "        best_completeness = completeness\n",
    "        best_reduction = reduction\n",
    "        best_method = method\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Blocking Method Comparison:\")\n",
    "display(pd.DataFrame(results_summary))\n",
    "\n",
    "# Select best candidates\n",
    "best_candidates = blocking_methods[best_method][0]\n",
    "print(f\"\\nüèÜ Best Method: {best_method} (Completeness: {best_completeness:.3f}, Reduction: {best_reduction:.3f})\")\n",
    "print(f\"‚úÖ Using {len(best_candidates):,} candidate pairs for matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Blocking log functionality. What should this look like as each method does blocking very differently? Print samples of the \"blocks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Entity Matching with Comparators\n",
    "\n",
    "Now we'll use PyDI's matching capabilities to find duplicate movies using multiple attribute comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparators for different attributes\n",
    "comparators = [\n",
    "    # Title similarity - most important for movies\n",
    "    StringComparator(\n",
    "        column='title',\n",
    "        similarity_function='jaro_winkler',  # Good for movie titles\n",
    "        preprocess=str.lower  # Case normalization\n",
    "    ),\n",
    "    \n",
    "    # Date proximity - movies from same year likely same film\n",
    "    DateComparator(\n",
    "        column='date', \n",
    "        max_days_difference=365  # Allow 1 year difference\n",
    "    ),\n",
    "    \n",
    "    # Actor name similarity - supporting evidence\n",
    "    StringComparator(\n",
    "        column='actor_name',\n",
    "        similarity_function='cosine',  # Good for names\n",
    "        preprocess=str.lower\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define attribute weights\n",
    "weights = [0.6, 0.25, 0.15]  # Title most important, then date, then actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Performing Entity Matching ===\n",
      "Candidate pairs to evaluate: 1,030\n",
      "Applying multi-attribute matching rules with threshold 0.7...\n",
      "\n",
      "Found 114 matches in 0.489 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Rule-Based Matcher\n",
    "matcher = RuleBasedMatcher()\n",
    "\n",
    "print(\"\\n=== Performing Entity Matching ===\")\n",
    "print(f\"Candidate pairs to evaluate: {len(best_candidates):,}\")\n",
    "print(\"Applying multi-attribute matching rules with threshold 0.7...\\n\")\n",
    "\n",
    "candidates_df = pd.DataFrame(best_candidates)\n",
    "\n",
    "# Perform matching with threshold 0.7\n",
    "start_time = time.time()\n",
    "\n",
    "matches = matcher.match(\n",
    "    df_left=left_df,\n",
    "    df_right=right_df, \n",
    "    candidates=[candidates_df],\n",
    "    comparators=comparators,\n",
    "    weights=weights,\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "matching_time = time.time() - start_time\n",
    "\n",
    "print(f\"Found {len(matches):,} matches in {matching_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluation Against Ground Truth\n",
    "\n",
    "PyDI provides separate, focused evaluation methods for different aspects of entity matching:\n",
    "- **`evaluate_blocking()`**: Evaluates blocking strategies with pair completeness, pair quality, and reduction ratio\n",
    "- **`evaluate_matching()`**: Evaluates matching results with precision, recall, F1-score, and accuracy\n",
    "\n",
    "Let's evaluate our matching results against the provided ground truth correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Against Ground Truth ===\n",
      "Loading Winter framework's ground truth correspondences...\n",
      "\n",
      "Training ground truth: 335 pairs\n",
      "Test ground truth: 3,347 pairs\n",
      "Training set: 103 positive matches out of 335 pairs (30.7%)\n",
      "Test set: 47 positive matches out of 3,347 pairs (1.4%)\n",
      "\n",
      "üéØ We'll evaluate against the test set (3,347 pairs)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluation Against Ground Truth ===\")\n",
    "print(\"Loading Winter framework's ground truth correspondences...\\n\")\n",
    "\n",
    "# Load ground truth correspondences\n",
    "gt_train = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"splits\" / \"academy_awards_2_actors_training.csv\",\n",
    "    name=\"ground_truth_train\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'label'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "gt_test = load_csv(\n",
    "    DATA_DIR / \"entitymatching\" / \"splits\" / \"academy_awards_2_actors_test.csv\", \n",
    "    name=\"ground_truth_test\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'label'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "print(f\"Training ground truth: {len(gt_train):,} pairs\")\n",
    "print(f\"Test ground truth: {len(gt_test):,} pairs\")\n",
    "\n",
    "# Analyze label distribution\n",
    "for name, gt in [('Training', gt_train), ('Test', gt_test)]:\n",
    "    true_matches = (gt['label'] == 'TRUE').sum() if 'TRUE' in gt['label'].values else (gt['label'] == True).sum()\n",
    "    total = len(gt)\n",
    "    print(f\"{name} set: {true_matches:,} positive matches out of {total:,} pairs ({true_matches/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ We'll evaluate against the test set ({len(gt_test):,} pairs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entity Matching Evaluation Results ===\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.976\n",
      "  Precision: 0.342\n",
      "  Recall:    0.830\n",
      "  F1-Score:  0.484\n",
      "Confusion Matrix:\n",
      "  True Positives:  39\n",
      "  True Negatives:  3299\n",
      "  False Positives: 75\n",
      "  False Negatives: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.34210526315789475,\n",
       " 'recall': 0.8297872340425532,\n",
       " 'f1': 0.484472049689441,\n",
       " 'accuracy': 0.9757380882782812,\n",
       " 'true_positives': 39,\n",
       " 'false_positives': 75,\n",
       " 'false_negatives': 8,\n",
       " 'true_negatives': 3299,\n",
       " 'threshold_used': 0.0,\n",
       " 'total_correspondences': 114,\n",
       " 'filtered_correspondences': 114,\n",
       " 'evaluation_timestamp': '2025-09-09T15:12:55.279738',\n",
       " 'output_files': ['c:\\\\Users\\\\Ralph\\\\dev\\\\pydi\\\\output\\\\tutorial\\\\matching_evaluation_summary.json',\n",
       "  'c:\\\\Users\\\\Ralph\\\\dev\\\\pydi\\\\output\\\\tutorial\\\\matching_detailed_results.csv']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform evaluation using PyDI's EntityMatchingEvaluator\n",
    "print(\"\\n=== Entity Matching Evaluation Results ===\")\n",
    "\n",
    "# Use the new evaluate_matching method for cleaner evaluation\n",
    "eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "    correspondences=matches,\n",
    "    test_pairs=gt_test,\n",
    "    out_dir=str(OUTPUT_DIR)\n",
    ")\n",
    "\n",
    "display(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Re-running matcher with debug mode to capture detailed results:\n",
      "  Using 1030 actual candidate pairs from Embedding blocking\n",
      "  Found 114 matches in 0.514 seconds with debug enabled\n",
      "  ‚úÖ Full debug results: debugResultsMatchingRule.csv\n",
      "  ‚úÖ Short debug results: debugResultsMatchingRule.csv_short\n",
      "üìÅ Debug files saved to: c:\\Users\\Ralph\\dev\\pydi\\output\\tutorial\\debug_results\n"
     ]
    }
   ],
   "source": [
    "# Re-run the matcher with debug mode enabled to get detailed debug data\n",
    "print(\"üîç Re-running matcher with debug mode to capture detailed results:\")\n",
    "\n",
    "# Use the same candidates and settings from before\n",
    "candidates_df = pd.DataFrame(best_candidates)\n",
    "print(f\"  Using {len(candidates_df)} actual candidate pairs from {best_method} blocking\")\n",
    "\n",
    "# Re-run matching with debug enabled to capture detailed comparator results\n",
    "start_time = time.time()\n",
    "\n",
    "# Enable debug mode in the matcher to capture detailed results\n",
    "matches, debug_info = matcher.match(\n",
    "    df_left=left_df,\n",
    "    df_right=right_df, \n",
    "    candidates=[candidates_df],\n",
    "    comparators=comparators,\n",
    "    weights=weights,\n",
    "    threshold=0.7,\n",
    "    debug=True  # This enables debug output capture\n",
    ")\n",
    "\n",
    "matching_time = time.time() - start_time\n",
    "print(f\"  Found {len(matches)} matches in {matching_time:.3f} seconds with debug enabled\")\n",
    "\n",
    "debug_output_dir = OUTPUT_DIR / \"debug_results\"\n",
    "debug_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Call the write_debug_results function with actual results\n",
    "full_debug_path, short_debug_path = EntityMatchingEvaluator.write_debug_results(\n",
    "    correspondences=matches,\n",
    "    debug_results=debug_info,\n",
    "    out_dir=str(debug_output_dir),\n",
    "    matcher_instance=matcher\n",
    ")\n",
    "\n",
    "print(f\"  ‚úÖ Full debug results: {Path(full_debug_path).name}\")\n",
    "print(f\"  ‚úÖ Short debug results: {Path(short_debug_path).name}\")\n",
    "\n",
    "print(f\"üìÅ Debug files saved to: {debug_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demonstrating Cluster Size Distribution Analysis ===\n",
      "Analyzing cluster size distribution in our entity matching results...\n",
      "\n",
      "üìä Cluster Size Distribution Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>frequency</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>98.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.785714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_size  frequency  percentage\n",
       "0             2        110   98.214286\n",
       "1             3          2    1.785714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Demonstrating Cluster Size Distribution Analysis ===\")\n",
    "print(\"Analyzing cluster size distribution in our entity matching results...\")\n",
    "\n",
    "# Create cluster size distribution from our matches\n",
    "cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "    correspondences=matches,\n",
    "    out_dir=str(OUTPUT_DIR / \"cluster_analysis\")\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cluster Size Distribution Results:\")\n",
    "display(cluster_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out detailed cluster information with all entity records for debugging purposes\n",
    "\n",
    "# Use the matches we found earlier to demonstrate cluster details\n",
    "cluster_details_path = OUTPUT_DIR / \"cluster_analysis\" / \"detailed_cluster_info.json\"\n",
    "cluster_details_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Call write_cluster_details with our entity matches\n",
    "output_path = EntityMatchingEvaluator.write_cluster_details(\n",
    "    correspondences=matches,\n",
    "    out_path=str(cluster_details_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Machine Learning-based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ML-Based Matching with Similarity Features ===\n",
      "Demonstrating MLBasedMatcher with FeatureExtractor using GridSearchCV\n",
      "Training on gt_train and testing on gt_test\n",
      "\n",
      "\n",
      "üîß Creating Similarity-Based FeatureExtractor...\n",
      "‚úÖ Created FeatureExtractor with 7 similarity features\n",
      "Feature names: ['StringComparator(title, jaro_winkler)', 'StringComparator(title, levenshtein)', 'StringComparator(title, cosine)', 'StringComparator(title, jaccard)', 'DateComparator(date)', 'StringComparator(actor_name, jaro_winkler)', 'StringComparator(actor_name, cosine)']\n",
      "\n",
      "‚öôÔ∏è Extracting Features from Training Pairs...\n",
      "Valid training pairs: 335 out of 335\n",
      "‚úÖ Training features extracted: (335, 10)\n",
      "Feature columns: ['StringComparator(title, jaro_winkler)', 'StringComparator(title, levenshtein)', 'StringComparator(title, cosine)', 'StringComparator(title, jaccard)', 'DateComparator(date)', 'StringComparator(actor_name, jaro_winkler)', 'StringComparator(actor_name, cosine)']\n",
      "Training data: X=(335, 7), y=(335,)\n",
      "Class distribution: {0: 232, 1: 103}\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ML-Based Matching with Similarity Features ===\")\n",
    "print(\"Demonstrating MLBasedMatcher with FeatureExtractor using GridSearchCV\")\n",
    "print(\"Training on gt_train and testing on gt_test\\n\")\n",
    "\n",
    "# Convert string labels to numeric\n",
    "gt_train['label'] = gt_train['label'].map({'TRUE': 1, 'FALSE': 0, True: 1, False: 0})\n",
    "gt_test['label'] = gt_test['label'].map({'TRUE': 1, 'FALSE': 0, True: 1, False: 0})\n",
    "\n",
    "# Create similarity-based FeatureExtractor \n",
    "print(\"\\nüîß Creating Similarity-Based FeatureExtractor...\")\n",
    "\n",
    "similarity_comparators = [\n",
    "    # Title similarity features - most important for movie matching\n",
    "    StringComparator(\"title\", similarity_function=\"jaro_winkler\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"levenshtein\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"cosine\", preprocess=str.lower),\n",
    "    StringComparator(\"title\", similarity_function=\"jaccard\", preprocess=str.lower),\n",
    "    \n",
    "    # Date proximity features\n",
    "    DateComparator(\"date\", max_days_difference=730),  # 2 years tolerance\n",
    "    \n",
    "    # Actor name similarity\n",
    "    StringComparator(\"actor_name\", similarity_function=\"jaro_winkler\", preprocess=str.lower),\n",
    "    StringComparator(\"actor_name\", similarity_function=\"cosine\", preprocess=str.lower),\n",
    "]\n",
    "\n",
    "feature_extractor = FeatureExtractor(similarity_comparators)\n",
    "print(f\"‚úÖ Created FeatureExtractor with {len(similarity_comparators)} similarity features\")\n",
    "print(f\"Feature names: {feature_extractor.get_feature_names()}\")\n",
    "\n",
    "# Extract training features\n",
    "print(f\"\\n‚öôÔ∏è Extracting Features from Training Pairs...\")\n",
    "\n",
    "# Filter training pairs to ensure both records exist\n",
    "valid_train_pairs = []\n",
    "valid_train_labels = []\n",
    "\n",
    "for _, row in gt_train.iterrows():\n",
    "    id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "    if (id1 in left_df['_id'].values and id2 in right_df['_id'].values):\n",
    "        valid_train_pairs.append({'id1': id1, 'id2': id2})\n",
    "        valid_train_labels.append(label)\n",
    "\n",
    "train_pairs_df = pd.DataFrame(valid_train_pairs)\n",
    "train_labels_series = pd.Series(valid_train_labels)\n",
    "\n",
    "print(f\"Valid training pairs: {len(train_pairs_df)} out of {len(gt_train)}\")\n",
    "\n",
    "# Extract features using FeatureExtractor\n",
    "train_features = feature_extractor.create_features(\n",
    "    left_df, right_df, train_pairs_df, labels=train_labels_series\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training features extracted: {train_features.shape}\")\n",
    "print(f\"Feature columns: {[col for col in train_features.columns if col not in ['id1', 'id2', 'label']]}\")\n",
    "\n",
    "# Prepare data for ML training\n",
    "feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "\n",
    "X_train = train_features[feature_columns]\n",
    "y_train = train_features['label']\n",
    "\n",
    "print(f\"Training data: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Class distribution: {y_train.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Scikit-learn integration\n",
    "\n",
    "From here on out, the full scikit-learn library can be used with the features extracted from PyDIs feature extractor without any wrapping as everything in PyDI is based on pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Setting up GridSearchCV...\n",
      "GridSearch setup: 4 models, F1 scoring, 5-fold CV\n",
      "\n",
      "üöÄ Training Models with GridSearchCV...\n",
      "\n",
      "Training RandomForest...\n",
      "  ‚úÖ RandomForest: Best CV F1 = 0.9853\n",
      "     Best params: {'class_weight': 'balanced', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "\n",
      "Training LogisticRegression...\n",
      "  ‚úÖ LogisticRegression: Best CV F1 = 0.9953\n",
      "     Best params: {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "\n",
      "Training GradientBoosting...\n",
      "  ‚úÖ GradientBoosting: Best CV F1 = 0.9953\n",
      "     Best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Training SVM...\n",
      "  ‚úÖ SVM: Best CV F1 = 0.9953\n",
      "     Best params: {'C': 0.1, 'class_weight': 'balanced', 'kernel': 'rbf'}\n",
      "\n",
      "üèÜ Best Overall Model: LogisticRegression (CV F1: 0.9953)\n"
     ]
    }
   ],
   "source": [
    "# Set up GridSearchCV with multiple models and hyperparameters\n",
    "print(f\"\\nüîç Setting up GridSearchCV...\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define models and parameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l2'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.1, 0.2],\n",
    "            'max_depth': [3, 5],\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42, probability=True),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use F1 score as the scoring metric (good for imbalanced data)\n",
    "scorer = make_scorer(f1_score)\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"GridSearch setup: {len(param_grids)} models, F1 scoring, 5-fold CV\")\n",
    "\n",
    "# Train models using GridSearchCV\n",
    "print(f\"\\nüöÄ Training Models with GridSearchCV...\")\n",
    "\n",
    "grid_search_results = {}\n",
    "best_overall_score = -1\n",
    "best_overall_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for model_name, config in param_grids.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "\n",
    "    # Create GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        scoring=scorer,\n",
    "        cv=cv_folds,\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    grid_search_results[model_name] = {\n",
    "        'grid_search': grid_search,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_estimator': grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ {model_name}: Best CV F1 = {grid_search.best_score_:.4f}\")\n",
    "    print(f\"     Best params: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Track overall best model\n",
    "    if grid_search.best_score_ > best_overall_score:\n",
    "        best_overall_score = grid_search.best_score_\n",
    "        best_overall_model = grid_search.best_estimator_\n",
    "        best_model_name = model_name\n",
    "            \n",
    "print(f\"\\nüèÜ Best Overall Model: {best_model_name} (CV F1: {best_overall_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Testing Best Model on Test Set...\n",
      "Valid test pairs: 3347 out of 3347\n"
     ]
    }
   ],
   "source": [
    "# Apply best trained model using MLBasedMatcher on test data\n",
    "print(f\"\\nüéØ Testing Best Model on Test Set...\")\n",
    "\n",
    "\n",
    "# Prepare test pairs\n",
    "valid_test_pairs = []\n",
    "valid_test_labels = []\n",
    "\n",
    "for _, row in gt_test.iterrows():\n",
    "    id1, id2, label = row['id1'], row['id2'], row['label']\n",
    "    if (id1 in left_df['_id'].values and id2 in right_df['_id'].values):\n",
    "        valid_test_pairs.append({'id1': id1, 'id2': id2})\n",
    "        valid_test_labels.append(label)\n",
    "\n",
    "test_pairs_df = pd.DataFrame(valid_test_pairs)\n",
    "test_labels_series = pd.Series(valid_test_labels)\n",
    "\n",
    "print(f\"Valid test pairs: {len(test_pairs_df)} out of {len(gt_test)}\")\n",
    "\n",
    "\n",
    "# Create MLBasedMatcher and apply trained model\n",
    "ml_matcher = MLBasedMatcher(feature_extractor)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "matches = ml_matcher.match(\n",
    "    left_df, right_df, [test_pairs_df], best_overall_model\n",
    ")\n",
    "\n",
    "# Show feature importance if available\n",
    "if hasattr(best_overall_model, 'feature_importances_'):\n",
    "    print(f\"\\nüîç Top Feature Importances:\")\n",
    "    importance_df = ml_matcher.get_feature_importance(best_overall_model, feature_columns)\n",
    "    display(importance_df.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the ML-based matching with the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML-based Entity Matching Evaluation Results ===\n",
      "Performance Metrics:\n",
      "  Accuracy:  0.998\n",
      "  Precision: 0.870\n",
      "  Recall:    1.000\n",
      "  F1-Score:  0.931\n",
      "Confusion Matrix:\n",
      "  True Positives:  47\n",
      "  True Negatives:  3293\n",
      "  False Positives: 7\n",
      "  False Negatives: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8703703703703703,\n",
       " 'recall': 1.0,\n",
       " 'f1': 0.9306930693069307,\n",
       " 'accuracy': 0.9979085748431431,\n",
       " 'true_positives': 47,\n",
       " 'false_positives': 7,\n",
       " 'false_negatives': 0,\n",
       " 'true_negatives': 3293,\n",
       " 'threshold_used': 0.0,\n",
       " 'total_correspondences': 54,\n",
       " 'filtered_correspondences': 54,\n",
       " 'evaluation_timestamp': '2025-09-09T15:13:01.824437',\n",
       " 'output_files': ['c:\\\\Users\\\\Ralph\\\\dev\\\\pydi\\\\output\\\\tutorial\\\\matching_evaluation_summary.json',\n",
       "  'c:\\\\Users\\\\Ralph\\\\dev\\\\pydi\\\\output\\\\tutorial\\\\matching_detailed_results.csv']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cluster Size Distribution Analysis ===\n",
      "\n",
      "üìä Cluster Size Distribution Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>frequency</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>94.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.882353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_size  frequency  percentage\n",
       "0             2         48   94.117647\n",
       "1             3          3    5.882353"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform evaluation using PyDI's EntityMatchingEvaluator\n",
    "print(\"\\n=== ML-based Entity Matching Evaluation Results ===\")\n",
    "\n",
    "# Use the new evaluate_matching method for cleaner evaluation\n",
    "eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "    correspondences=matches,\n",
    "    test_pairs=gt_test,\n",
    "    out_dir=str(OUTPUT_DIR)\n",
    ")\n",
    "\n",
    "display(eval_results)\n",
    "\n",
    "print(\"=== Cluster Size Distribution Analysis ===\")\n",
    "\n",
    "# Create cluster size distribution from our matches\n",
    "cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "    correspondences=matches,\n",
    "    out_dir=str(OUTPUT_DIR / \"cluster_analysis\")\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cluster Size Distribution Results:\")\n",
    "display(cluster_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to similarity metrics for each attribute, PyDIs VectorFeatureExtractor can be used to create embeddings using SentenceTransformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer features shape: (1030, 3)\n"
     ]
    }
   ],
   "source": [
    "# VectorFeatureExtractor Examples\n",
    "\n",
    "from PyDI.entitymatching import VectorFeatureExtractor\n",
    "\n",
    "# SentenceTransformers embeddings using VectorFeatureExtractor\n",
    "st_extractor = VectorFeatureExtractor(\n",
    "    embedding_model='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    columns=['title', 'actor_name', 'date'],\n",
    "    distance_metrics=['cosine'],\n",
    "    pooling_strategy='concatenate'\n",
    ")\n",
    "\n",
    "st_features = st_extractor.create_features(\n",
    "    left_df, right_df, candidates_df\n",
    ")\n",
    "print(f\"SentenceTransformer features shape: {st_features.shape}\")\n",
    "\n",
    "# Extract features using FeatureExtractor\n",
    "train_features = feature_extractor.create_features(\n",
    "    left_df, right_df, train_pairs_df, labels=train_labels_series\n",
    ")\n",
    "\n",
    "# ready to train ML models with scikit-learn as before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fusion Input Datasets:\n",
      "  Academy Awards: 4,592 records\n",
      "  Actors: 149 records\n",
      "  Golden Globes: 2,286 records\n",
      "  Total: 7,027 records\n",
      "\n",
      "üéØ Goal: Create single authoritative movie record per entity\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Fusion Input Datasets:\")\n",
    "for df, name in zip(datasets, names):\n",
    "    print(f\"  {name}: {len(df):,} records\")\n",
    "\n",
    "total_input_records = sum(len(df) for df in datasets)\n",
    "print(f\"  Total: {total_input_records:,} records\")\n",
    "print(f\"\\nüéØ Goal: Create single authoritative movie record per entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading Correspondence Files\n",
    "\n",
    "Data fusion requires correspondence information to group records referring to the same entity. Let's load the pre-computed correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Correspondences for Data Fusion ===\n",
      "Academy Awards ‚Üî Actors correspondences: 150\n",
      "Actors ‚Üî Golden Globes correspondences: 107\n",
      "\n",
      "üìä Correspondence Structure:\n",
      "Academy Awards ‚Üî Actors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy_awards_4557</td>\n",
       "      <td>actors_1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academy_awards_4529</td>\n",
       "      <td>actors_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academy_awards_4500</td>\n",
       "      <td>actors_3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>academy_awards_4475</td>\n",
       "      <td>actors_4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academy_awards_4446</td>\n",
       "      <td>actors_5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id1       id2  score\n",
       "0  academy_awards_4557  actors_1    1.0\n",
       "1  academy_awards_4529  actors_2    1.0\n",
       "2  academy_awards_4500  actors_3    1.0\n",
       "3  academy_awards_4475  actors_4    1.0\n",
       "4  academy_awards_4446  actors_5    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actors ‚Üî Golden Globes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actors_16</td>\n",
       "      <td>golden_globes_2279</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actors_22</td>\n",
       "      <td>golden_globes_2263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actors_23</td>\n",
       "      <td>golden_globes_2252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>actors_24</td>\n",
       "      <td>golden_globes_2240</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>actors_25</td>\n",
       "      <td>golden_globes_2226</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id1                 id2  score\n",
       "0  actors_16  golden_globes_2279    1.0\n",
       "1  actors_22  golden_globes_2263    1.0\n",
       "2  actors_23  golden_globes_2252    1.0\n",
       "3  actors_24  golden_globes_2240    1.0\n",
       "4  actors_25  golden_globes_2226    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-computed correspondences from the Winter framework\n",
    "print(\"=== Loading Correspondences for Data Fusion ===\")\n",
    "\n",
    "CORR_DIR = ROOT / \"input\" / \"movies\" / \"fusion\" / \"correspondences\"\n",
    "\n",
    "# Load correspondence files\n",
    "academy_actors_corr = load_csv(\n",
    "    CORR_DIR / \"academy_awards_2_actors_correspondences.csv\",\n",
    "    name=\"academy_actors_correspondences\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'score'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "actors_globes_corr = load_csv(\n",
    "    CORR_DIR / \"actors_2_golden_globes_correspondences.csv\", \n",
    "    name=\"actors_globes_correspondences\",\n",
    "    header=None,\n",
    "    names=['id1', 'id2', 'score'],\n",
    "    add_index=False\n",
    ")\n",
    "\n",
    "print(f\"Academy Awards ‚Üî Actors correspondences: {len(academy_actors_corr):,}\")\n",
    "print(f\"Actors ‚Üî Golden Globes correspondences: {len(actors_globes_corr):,}\")\n",
    "\n",
    "# Preview correspondence structure\n",
    "print(\"\\nüìä Correspondence Structure:\")\n",
    "print(\"Academy Awards ‚Üî Actors:\")\n",
    "display(academy_actors_corr.head())\n",
    "\n",
    "print(\"Actors ‚Üî Golden Globes:\")\n",
    "display(actors_globes_corr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Running Fusion using correspondences to build record groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correspondences: 257\n"
     ]
    }
   ],
   "source": [
    "# Combine all correspondences into a single list\n",
    "all_correspondences = []\n",
    "\n",
    "# Add Academy Awards ‚Üî Actors correspondences\n",
    "for _, row in academy_actors_corr.iterrows():\n",
    "    all_correspondences.append((row['id1'], row['id2'], row['score']))\n",
    "    \n",
    "# Add Actors ‚Üî Golden Globes correspondences  \n",
    "for _, row in actors_globes_corr.iterrows():\n",
    "    all_correspondences.append((row['id1'], row['id2'], row['score']))\n",
    "\n",
    "all_correspondences = pd.DataFrame(all_correspondences, columns=['id1', 'id2', 'score'])\n",
    "\n",
    "print(f\"Total correspondences: {len(all_correspondences):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyDI Data Fusion Framework Demonstration ===\n",
      "\n",
      "‚úÖ Strategy 'movie_fusion' configured with 6 rules\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PyDI Data Fusion Framework Demonstration ===\")\n",
    "\n",
    "# Import additional fusion components needed\n",
    "from PyDI.fusion import AttributeValueFuser\n",
    "\n",
    "# Initialize the fusion strategy\n",
    "fusion_strategy = DataFusionStrategy(\"movie_fusion\")\n",
    "\n",
    "# Title: Use longest string (often more complete)\n",
    "fusion_strategy.add_attribute_fuser(\"title\", AttributeValueFuser(longest_string))\n",
    "\n",
    "# Date: Use most recent (latest data often more accurate)\n",
    "fusion_strategy.add_attribute_fuser(\"date\", AttributeValueFuser(most_recent))\n",
    "\n",
    "# Actor name: Use most complete (non-null, longest)\n",
    "fusion_strategy.add_attribute_fuser(\"actor_name\", AttributeValueFuser(most_complete))\n",
    "\n",
    "# Director name: Use longest string\n",
    "fusion_strategy.add_attribute_fuser(\"director_name\", AttributeValueFuser(longest_string))\n",
    "\n",
    "# Awards: Union all award information\n",
    "fusion_strategy.add_attribute_fuser(\"oscar\", AttributeValueFuser(union))\n",
    "fusion_strategy.add_attribute_fuser(\"globe\", AttributeValueFuser(union))\n",
    "\n",
    "print(f\"\\n‚úÖ Strategy '{fusion_strategy.name}' configured with {len(fusion_strategy.get_registered_attributes())} rules\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input datasets: 3\n",
      "Input records: 7,027\n",
      "Correspondences: 257\n",
      "\n",
      "‚úÖ Fusion Complete!\n",
      "  Total time: 0.287 seconds\n",
      "  Output records: 6,755\n",
      "  Compression ratio: 96.1%\n"
     ]
    }
   ],
   "source": [
    "# Create fusion engine with our strategy\n",
    "fusion_engine = DataFusionEngine(fusion_strategy)\n",
    "\n",
    "print(f\"Input datasets: {len(datasets)}\")\n",
    "print(f\"Input records: {total_input_records:,}\")\n",
    "print(f\"Correspondences: {len(all_correspondences):,}\")\n",
    "\n",
    "# Execute fusion with timing\n",
    "start_time = time.time()\n",
    "\n",
    "fused_dataset, execution_time = fusion_engine.run(\n",
    "    datasets=datasets,\n",
    "    correspondences=all_correspondences, \n",
    "    id_column='id',  # Use original 'id' column for matching\n",
    "    include_singletons=True  # Include unmatched records\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Fusion Complete!\")\n",
    "print(f\"  Total time: {total_time:.3f} seconds\") \n",
    "print(f\"  Output records: {len(fused_dataset):,}\")\n",
    "print(f\"  Compression ratio: {len(fused_dataset)/total_input_records:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
